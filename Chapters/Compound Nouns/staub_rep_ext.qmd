```{r echo = F, warning = F, message = F}
options (contrasts = c("contr.sum","cont.sum"))
library(tidyverse)
library(brms)
library(RColorBrewer)
library(ggpubr)
library(knitr)
library(kableExtra)


data_n1 = read_csv('Data/data_analysis_n1.csv')



model_n1_binary = brm(FIRST_FIXATION_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n1, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       control = list(adapt_delta = 0.95),
       file = './Data/model_n1_binary')

model_n1_binary_gaze = brm(GAZE_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n1, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_binary_gaze')

model_n1_binary_first_pass = brm(FIRST_PASS_REGRESSION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_n1, 
       family = bernoulli(),
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_binary_regression')

model_n1_binary_go_past = brm(REGRESSION_PATH_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_path_n1, 
       chains = 4, 
       cores = 4, 
       warmup = 6000, 
       iter = 12000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_binary_regression_path')


model_n2 = brm (FIRST_FIXATION_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + plausibility|item), 
                data = data_analysis_first_fixation_n2, 
                chains = 4, 
                cores = 4, 
                warmup = 4000, 
                iter = 8000, 
                thin = 4, 
                control = list(adapt_delta = 0.99),
                file = './Data/model_n2'
)

model_n2_binary_gaze = brm (GAZE_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + plausibility|item), 
                data = data_analysis_first_fixation_n2, 
                chains = 4, 
                cores = 4, 
                warmup = 4000, 
                iter = 8000, 
                thin = 4, 
                prior = priors,
                control = list(adapt_delta = 0.99),
                file = './Data/model_n2_gaze'
)

model_n2_binary_go_past = brm (REGRESSION_PATH_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + plausibility|item), 
                data = data_analysis_regression_path_n2, 
                chains = 4, 
                cores = 4, 
                warmup = 4000, 
                iter = 8000, 
                thin = 4, 
                prior = priors,
                control = list(adapt_delta = 0.99),
                file = './Data/model_n2_regression_path'
)

model_n2_binary_first_pass = brm(FIRST_PASS_REGRESSION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + plausibility|item), 
                data = data_analysis_regression_n2, 
                family = bernoulli(),
                chains = 4, 
                cores = 4, 
                warmup = 4000, 
                iter = 8000, 
                thin = 4, 
                prior = priors,
                control = list(adapt_delta = 0.99),
                file = './Data/model_n2_regression'
)


first_fixation_filler =
  brm(FIRST_FIXATION_DURATION ~ filler_higher_freq + (1 + filler_higher_freq||subject) + (1 + filler_higher_freq||filler_sentence) + (1|filler_word), 
       data = data_filler_first_fixation_filler, 
       chains = 4, 
       cores = 4, 
       warmup = 6000,  
       init = 0,
       iter = 12000, 
       thin = 4,
       #control = list(adapt_delta = 0.95),
       file = './Data/model_filler_first_fixation')


gaze_filler =
  brm(GAZE_DURATION ~ filler_higher_freq + (1 + filler_higher_freq||subject) + (1 + filler_higher_freq||filler_sentence) + (1|filler_word), 
       data = data_filler_gaze, 
       chains = 4, 
       cores = 4, 
       warmup = 6000, 
       init = 0,
       iter = 12000, 
       thin = 4,
       #control = list(adapt_delta = 0.95),
       file = './Data/model_filler_gaze')


go_past_filler =
  brm(REGRESSION_PATH_DURATION ~ filler_higher_freq + (1 + filler_higher_freq||subject) + (1 + filler_higher_freq||filler_sentence) + (1|filler_word), 
       data = data_filler_go_past, 
       chains = 4, 
       cores = 4, 
       warmup = 6000, 
       init = 0,
       iter = 12000, 
       thin = 4,
       #control = list(adapt_delta = 0.95),
       file = './Data/model_filler_go_past')

first_pass_filler =
  brm(FIRST_PASS_REGRESSION ~ filler_higher_freq + (1 + filler_higher_freq||subject) + (1 + filler_higher_freq||filler_sentence) + (1|filler_word), 
       data = data_filler_first_pass, 
       family = bernoulli(),
       chains = 4, 
       cores = 4, 
       warmup = 6000, 
       init = 0,
       iter = 12000, 
       thin = 4,
       #control = list(adapt_delta = 0.95),
       file = './Data/model_filler_first_pass')


model_n1_staub =
  brm(FIRST_FIXATION_DURATION ~ frequency*condition + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n1, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       control = list(adapt_delta = 0.95),
       file = './Data/model_n1_staub')


model_n1_gaze_staub =
  brm(GAZE_DURATION ~ frequency*condition + (1 + frequency*condition|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n1_staub, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_gaze_staub')



model_n1_regression_path_staub =
  brm(REGRESSION_PATH_DURATION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_path_n1_staub, 
       chains = 4, 
       cores = 4, 
       warmup = 10000, 
       iter = 20000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_regression_path_staub')


model_n1_regression_staub =
  brm(FIRST_PASS_REGRESSION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_n1_staub, 
       family = bernoulli(),
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_regression_staub')


model_n2_staub =
  brm(FIRST_FIXATION_DURATION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n2, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       control = list(adapt_delta = 0.95),
       file = './Data/model_n2_staub')

model_n2_gaze_staub =
  brm(GAZE_DURATION ~ condition*frequency + (1 + frequency*condition|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n2_staub, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n2_gaze_staub2')

model_n2_regression_path_staub =
  brm(REGRESSION_PATH_DURATION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_path_n2_staub, 
       chains = 4, 
       cores = 4, 
       warmup = 10000, 
       iter = 20000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n2_regression_path_staub')


model_n2_regression_staub =
  brm(FIRST_PASS_REGRESSION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_n2_staub, 
       family = bernoulli(),
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n2_regression_staub')

maze_staub_n1 = readRDS('./Data/model_n1_sum_coding.rds')
maze_staub_n2 = readRDS('./Data/model_n2_sum_coding.rds')

maze_pred_n1_v1 = readRDS('./Data/N1_simple1.rds')
maze_pred_n2_v1 = readRDS('./Data/N2_simple1.rds')
maze_pred_n1_v2 = readRDS('./Data/N1_simple2.rds')
maze_pred_n2_v2 = readRDS('./Data/N2_simple2.rds')

```

# Does Predictability Drive the Holistic Storage of Compound Nouns?

## Introduction

Learning a language is not a trivial task. In order to be successful, learners must accurately segment the continuous speech stream into smaller segments, including phrases, words, morphemes, and phonemes. One of the main questions that arises out of this task is what exactly the size of the units that learners are storing is. That is, are they storing individual words, entire sentences, phrases, or some combination of all of these? One possibility is that learners store very little outside of words and idioms. For example, traditional theories have argued that learners don't store any more than they need to: they store only what they can't form compositionally using a set of rules, and generate everything else [e.g., @chomsky1965]. According to these theories, inflected words, such as *walked* would be generated by accessing the stored root, *walk*, and then applying a past tense rule that generates *walked* from the root. Similarly, a phrase like *I don't know* would be generated by accessing each of the individually stored words *I*, *don't*, and *know*.

On the opposite side of this theoretical spectrum, it's also possible that learners store everything, including entire sentences. @ambridgeStoredAbstractionsRadical2020 argued for exactly this, specifically arguing that everything a learner hears "is stored with its meaning, as understood in that individual situation" and that unwitnessed novel-forms are produced using on-the-fly analogy across stored exemplars [@ambridgeStoredAbstractionsRadical2020]. For example, producing a novel plural form, like *wugs*, would consist of analogizing (on-the-fly) over multiple stored exemplars (e.g., *cats*, *chairs*, *dogs*, etc).

It is also possible that the size and number of units being stored lies somewhere in between these two extremes. For example, usage-based construction grammar approaches have posited that a lot more than just words are stored -- including high-frequency phrases -- but rather than storing everything, or storing only the most basic units, instead humans store units of varying size depending on usage-based factors such as frequency [@bybee2003; @bybee2001; @goldbergConstructionsNewTheoretical2003; @morgan2024; @morganAbstractKnowledgeDirect2016; @morgan2015; @arnonMoreWordsFrequency2010; @baayenAmorphousModelMorphological2011; @odonnellProductivityReuseLanguage2016; @tomaselloConstructingLanguageUsagebased2005]. That is to say, the size of the units stored is driven by the statistical distribution of the language that the learner is producing and perceiving. For example, @bybee2003 drew an analogy to learning to play a piece on the piano:

> An important result of learning to play several pieces is that new pieces are then easier to master. Why is this? I hypothesize that the player can access bits of old stored pieces and incorporate them into new pieces. The part of a new piece that uses parts of a major scale is much easier to master if the player has practiced scales than is a part with a new melody that does not hearken back to common sequences. This means that snatches of motor sequences can be reused in new contexts. The more motor sequences stored, the greater ease with which the player can master a new piece [@bybee2003, p. 14-15].

\noindent In this same line of thinking, @bybee2003 further argued against a strictly traditional view, stating that learning the English past tense *-ed* requires learning a series of words that contain that segment (e.g., *played*, *spilled*, *talked*) and that these are not necessarily flushed from memory after learning the English past tense marker.

There is no shortage of evidence for the holistic storage of multi-word phrases. For example, high-frequency phrases, such as *I don't know*, have been shown to undergo phonetic reduction that isn't seen in other low or mid-frequency phrases containing *don't* [@bybeeEffectUsageDegrees1999] suggesting that the representation of *I don't know* is separate from the representation of each of the individual words. In other words, the susceptibility of high-frequency phrases to phonological change that doesn't occur in their component words is strong evidence that they may come to have a mental representation for the whole expression (i.e., holistic storage). This example is not an outlier either, there are many examples of high-frequency phrases undergoing phonetic reduction: *going to*, *want to, have to*, etc [@bybee2003]. This is also not a feature of English specifically. For example, in Korean, @yiEumun2002 demonstrated that in multi-word phrases containing the adnominal future marker (*-l*), the tensification of the consonant following the adnominal is predicted by the phrasal frequency. That is, in high-frequency phrases, consonants following the adnominal became tense at a higher rate than in low-frequency phrases.[^staub_rep_ext-1]

[^staub_rep_ext-1]: A similar effect has been demonstrated on the word-level as well in Korean, where epenthesis has been documented to occur more often in high-frequency words than in low-frequency words [@leeFrequencyEffectsMorphologisation2015]. This suggests that there may not be a clear division between the representation of high-frequency phrases and high-frequency words.

Evidence for holistic storage is not limited to phonological effects, either. In the Psycholinguistics literature, @siyanova-chanturiaSeeingPhraseTime2011 demonstrated that readers are sensitive to the ordering of binomials in English. In an eye-tracking experiment, participants read frequent binomial expressions in English in their preferred order (e.g., *bride and groom*) and their reversed order (*groom and bride*). They found that the preferred orderings were read faster. Further, @morganAbstractKnowledgeDirect2016 investigated whether the results from @siyanova-chanturiaSeeingPhraseTime2011 could be attributed to abstract knowledge of binomial orderings (e.g., a preference for male names before female names) or whether they were due to participants' direct experience with those items (e.g., hearing one ordering of a specific binomial more often than the complementary ordering). They developed a probabilistic model to approximate native English speakers' ordering preferences and combined that with a forced-choice and a self-paced reading task in order to investigate whether ordering preferences were driven by abstract knowledge or direct experience of the expression. They found that reading times for frequent binomials were influenced only by relative frequency (i.e., direct experience), not abstract knowledge. That is to say, ordering preferences of frequent binomials weren't explained by abstract ordering preferences, but rather by linguistic experience with the specific binomial, suggesting that high-frequency binomials are stored holistically.

Similarly, @odonnellProductivityReuseLanguage2016 tested 4 probabilistic models on their ability to learn the English past tense and derivational morphology. Specifically, they tested a Full-parsing model, which stores minimal-sized units only, a Full-listing model, which stores the entirety of units only, an Exemplar-based model, which stores all units and all sub-units consistent with the data, and finally a Productivity as an Inference model, which, similar to the Exemplar-based Inference model, can store both smaller and larger structures, but probabilistically determines which items to store based on the data. They found that the Inference-based model performed the best overall for both past tenses and derivational morphology. In other words, storing units of varying sizes (as opposed to just minimal or maximal-sized units) seems to be the most conducive approach to learning the various morphological paradigms in English.

Despite the clear evidence for the holistic storage of some multi-word units, however, it is still largely unclear what determines whether a unit is stored holistically. For example, it is possible that storage is driven by either **phrasal frequency** [@bybee2001] or by the mutual **predictability** of a phrase's component parts [i.e., how predictable the whole phrase is from part of the phrase; @odonnellProductivityReuseLanguage2016]. For example, as previously stated, there is an abundance of evidence that high-frequency phrases are more susceptible to phonetic reduction than low-frequency phrases [@bybeeEffectUsageDegrees1999; @bybee2003]. Additionally, high-frequency phrases have been shown to lose the recognizability of their component parts relative to low-frequency phrases [@kapatsinskiFrequencyEmergencePrefabs2009]. For example, *up* is harder to recognize in *pick up* than in *run up*. On the other hand, in the learning literature, there is significant evidence that learning is driven by prediction error as opposed to raw co-occurrence statistics. For example, @ramscarChildrenValueInformativity2013 demonstrated that in word learning, children rely on more than simple co-occurrence statistics but also on how *informative* -- that is, how *predictive* -- a cue is of an outcome (relative to other cues). Specifically, they demonstrated that children rely on not only co-occurrence rate, but also background rate (how often a cue is present without an outcome). In other words, assuming doors have a higher co-occurrence rate and lower background rate than all the other competing cues (e.g., brown, house, room) for the word *door*, then children will learn that doors are the best predictor of the word *door* [@ramscarChildrenValueInformativity2013].

A similar debate persists in the speech perception literature, where @pierrehumbertExemplarDynamicsWord2001 argued that internal representations reflect the raw frequency distribution of the input. On the other hand, @olejarczukDistributionalLearningErrordriven2018 argued that the learning of phonemes is driven not by co-occurrence statistics (i.e., raw frequency), but rather by surprisal (i.e., prediction error). In other words, learners are actively predicting upcoming phonemes and update their beliefs in proportion to how surprising the upcoming phoneme is. Thus the debate between co-occurrence vs predictability in the role of learning is not unique to the word learning literature.

Additionally, if learners are storing more than just single-word units, what are the processing consequences of this? For example, as mentioned earlier, @kapatsinskiFrequencyEmergencePrefabs2009 investigated the recognition of the particle *up* in phrases of varying frequencies and found that the recognition of the particle *up* is significantly more difficult in high-frequency phrases than in low-frequency phrases, suggesting that high-frequency units \`fuse' together, losing some of the recognizability of their individual parts.

On the other hand, @staubTimeCoursePlausibility2007 investigated the effects of plausibility on the reading times of familiar and novel compound nouns, which were compound nouns with high and low phrasal frequency respectively. Participants read sentences which contained a novel compound noun or a familiar compound noun (See Example \ref{staubitems}) in a plausible condition (e.g., Example \ref{novelplaus}) or an implausible condition (e.g., Example \ref{novelimplaus}). Crucially, the second noun in the compound eliminated the local implausibility such that every sentence was plausible after reading the second noun. For example, in \ref{novelimplaus}, *The zookeeper spread out the monkey...* is locally implausible, however upon reading the second noun in the compound, *medicine*, the local implausibility is eliminated.

\begin{exe}
\ex \label{staubitems}
\begin{xlist}
\ex Novel Compound \label{compound}
\begin{xlist}
\ex The zookeeper picked up the monkey medicine that was in the enclosure. \hfill \emph{plausible} \label{novelplaus}
\ex The zookeeper spread out the monkey medicine that was in the enclosure. \hfill \emph{implausible}\label{novelimplaus}
\end{xlist}
\ex Familiar Compound
\begin{xlist}
\ex Jenny looked out on the huge mountain lion pacing in its cage. \hfill \emph{plausible}\label{familiarplaus}
\ex Jenny heard the huge mountain lion pacing in its cage. \hfill \emph{implausible} \label{familiarimplaus}
\end{xlist}
\end{xlist}
\end{exe}

\noindent They found that the size of the plausibility effect was the same for both novel and familiar compound nouns. That is to say, while familiar items were read more quickly than novel items, and there was an increase in reading times in the implausible condition, the size of the plausibility effect was not different for familiar items (relative to novel items). However, if familiar items are stored holistically, one might expect that readers would predict the second noun upon reading the first, thus eliminating the local implausibility. Thus, if these items are stored holistically it begs the question of what the processing consequences of storage are. Alternatively, it may just be that these items are not stored. For example, it is possible that, as has been previewed throughout the introduction, phrasal frequency may not be the driving factor of storage. Instead, it may actually be predictability that drives storage. If this is the case, then it is possible that the reason for a lack of an interaction effect in @staubTimeCoursePlausibility2007's results is due to their stimuli being low-predictability compound nouns. For example, while *mountain lion* has a high phrasal frequency, *mountain* is not very predictable of *lion* (that is, the probability of *lion* following *mountain* is fairly low, despite the overall phrase having a relatively high-frequency).

Thus there are two main problems that the present study aims to provide insight on: does predictability drive storage, and what are the processing consequences of storage? In Experiment 1, we first replicate @staubTimeCoursePlausibility2007's experiment using a maze task [@boyceMazeMadeEasy2020]. In Experiment 2, we use the same methodology, but instead of using high (phrasal) frequency compound nouns, we use high-*predictability* compound nouns (e.g., *peanut butter*). By using the highest predictability compound nouns from the Google *n*-grams corpus [@michel2011quantitativeanalysisculture], we ask whether the difference in reaction times between the locally implausible and plausible contexts differs depending on whether the compound noun is highly predictable or not. For example, if highly predictable compound nouns are stored holistically, it is possible that when listeners hear, or read, the first noun in a highly predictable compound noun they may access the second noun as well and/or a holistic compound noun representation. If this is the case, then locally implausible contexts should not incur as much processing difficulty when the compound noun is highly predictable because the second noun eliminates the local implausibility. Lastly in Experiments 3 and 4 we replicate these with eye-tracking.

## Experiment 1

### Methods

#### Participants

Participants were presented with sentences online via ibex farm, a web-based experiment software platform that is freely-available ([github.com/addrummond/ibex](github.com/addrummond/ibex)) and were recruited through the University of California Linguistics/Psychology Human Subjects Pool. To prevent selection bias, participants signed up for the experiment blindly, without knowledge of the content of the experiment. 146 participants were recruited, however 30 participants were excluded for having an overall accuracy below 70% (in this case, accuracy is operationalized as choosing the correct word; an inaccuracy would be choosing the ungrammatical distractor word), leaving a total of 116 participants. All participants self-reported being native English speakers.

#### Stimuli

The experimental sentences were sentences containing compound nouns [from @staubTimeCoursePlausibility2007] which varied upon two dimensions: local plausibility and familiarity. Locally plausible sentences were sentences in which the reading at the first noun was plausible and locally implausible sentences were sentences in which the reading at the first noun in the compound was implausible (see example sentences \ref{novelimplaus} and \ref{familiarimplaus}). Local plausibility was a within-item effect and familiarity (the frequency of the compound noun) was a between-item effect. The sentences in the previous example sentences (Example \ref{staubitems}) exemplify each condition: the first sentence in each is locally plausible while the second one is locally implausible. For example, in sentence \ref{familiarplaus}, it is semantically plausible that *Jenny looked out on the huge mountain...* but not semantically plausible that *Jenny heard the huge mountain* (sentence \ref{familiarimplaus}). Altogether, our stimuli consisted of 24 novel items, 24 familiar items [taken from @staubTimeCoursePlausibility2007], and 188 filler sentences in order to avoid participants discerning the experimental design.

#### Procedure

Experiment 1 is a direct replication of @staubTimeCoursePlausibility2007 using the A-Maze task [@boyceMazeMadeEasy2020] instead of eye-tracking. In the A-maze task, participants are presented with the first word in the sentence and then have to correctly choose between an ungrammatical distractor word and the next word in the sentence. When participants select the correct word, they continue to the next word in the sentence until the sentence is finished. The distractor words for the A-maze were generated automatically following @boyceMazeMadeEasy2020 using the Gulordava model [@gulordava2018colorlessgreenrecurrent]. The locations of the distractor word and target word were counterbalanced so that they appeared an equal number of times on the left and right side of the screen. For each word, the reaction time was recorded along with whether the subject chose the correct item or not. See @fig-mazevisualization for a visualization of the maze task, reproduced from @boyceMazeMadeEasy2020.

```{r echo = F, fig.align = 'center', warning = F, message = F, out.width = '50%'}
#| label: fig-mazevisualization
#| fig-cap: "A visualization of the maze task, reproduced from @boyceMazeMadeEasy2020."
#| fig-align: center
knitr::include_graphics("Figures/mazevisualization.pdf")

```

Sentences were presented in a random order and each word was presented an equal number of times on the left and right side of the screen. Additionally, each item appeared an equal number of times in the implausible and plausible context and no participant was presented with the same item in more than one condition. The complete dataset included 9994 response tokens.

#### Analysis

The data was analyzed using Bayesian linear regression models, as implemented in the *brms* package [@burknerBrmsPackageBayesian2017] within the R programming environment @Rpackage.[^staub_rep_ext-2] We subsetted the data into two sets based on the region: one set for the first noun in the compound noun and one set for the second noun in the compound. The primary dependent variable was log reaction time for both of these regions [following @boyceMazeMadeEasy2020]. The primary independent variables were plausibility and familiarity [following @staubTimeCoursePlausibility2007]. We modeled reaction time as a function of plausibility and familiarity, including their interaction, with maximal random effects [@barrRandomEffectsStructure2013]. The formula used for the model is presented in equation @eq-modelfamil below, with *Plaus* as plausibility and *Famil* as familiarity. All variables in our model were sum-coded. $$
Reaction Time \sim Plaus*Famil+(Plaus*Famil|Subject)+(Plaus|Item)
$$ {#eq-modelfamil}

[^staub_rep_ext-2]: For more details about the analyses, our materials are available at the following link: <https://github.com/znhoughton/dissertation_writeup/tree/master/Chapters/Compound%20Nouns>.

### Results

As mentioned in the methods section, for the purpose of the analysis, the data was divided into two regions: the N1 region and the N2 region, which were the first and second noun in the compound noun respectively. The results of the Bayesian regression model for the N1 region are presented in @tbl-N1Staub and in @fig-N1Staub, and the results of the N2 region are presented in @tbl-N2Staub and in @fig-N2Staub.

For the N1 region, there was an increase in reaction time for the implausible condition relative to the plausible condition. There was no such effect for familiarity. In other words, while participants took longer selecting the correct word in the implausible condition, their reaction times were not affected by the familiarity of the compound noun. This is expected given that the familiarity measurement was not the frequency of the first noun, but rather the frequency of the compound noun as a whole. Additionally, there was no interaction effect between plausibility and familiarity.

Following @wagenmakers2010bayesianhypothesistesting, a post-hoc Bayes factor analysis was conducted to compare the interaction effect to the null hypothesis (interaction effect = 0). We found a Bayes Factor value of 18.15 in favor of the null, which constitutes strong support for the null hypothesis.[^staub_rep_ext-3] Specifically, we used the Save-Dickey density ratio method, which involves comparing two models: one in which the value of interest is fixed (in this case, the interaction effect was fixed at zero) and one in which the value of interest is free to vary (in this case, the interaction effect was allowed to vary). The Bayes factor is then obtained by dividing the height of the posterior for the parameter under the model that allows the interaction effect to vary by the height of the prior for that parameter [@wagenmakers2010bayesianhypothesistesting].[^staub_rep_ext-4] In this case, the prior of interest was a prior that specified that the interaction effect was zero.[^staub_rep_ext-5] We computed the Bayes factor for the N1 region and not the N2 region because the interaction at the N2 region is not of particular interest to our theoretical question. Specifically, whether the effect of familiarity is mediated by plausibility at the N2 region is not directly relevant to our theoretical questions.

[^staub_rep_ext-3]: More accurately, it indicates that the results are $1/18.15=0.05$ times more likely under the alternate hypothesis than under the null hypothesis.

[^staub_rep_ext-4]: The code to reproduce the Bayes factor is included at the following link: <https://github.com/znhoughton/dissertation_writeup/blob/master/Chapters/Compound%20Nouns/Analysis%20Scripts/data_analysis.qmd>.

[^staub_rep_ext-5]: An example of this approach being implemented in *brms* is included here: <https://vuorre.com/posts/2017-03-21-bayes-factors-with-brms/>.

At the N2 region, there was an increase in reaction time in the plausible condition and a decrease in reaction time in the familiar condition, but no interaction effect. In other words, participants were slower to choose the correct word in the plausible condition. They were also quicker to choose the correct word if the compound noun was familiar. However, plausibility did not mediate the effects of familiarity. That is to say, the size of the plausibility effect was not different for familiar versus novel compound nouns.

```{r, echo = F, message = F}
#| label: tbl-N1Staub
#| tbl-cap: "Model results examining the effect of plausibility and frequency for the N1 region."



percent_greater_zero = data.frame(fixef(maze_staub_n1, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'familiarity1', 'plausibility1:familiarity1')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_staub_n1)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Familiarity", "Plausibility:Familiarity"), times = 1))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')


fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(n1_all_measures$measure)

fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')

```

```{r, echo = F, message = F}
#| label: tbl-N2Staub
#| tbl-cap: "Model results examining the effect of plausibility and frequency for the N2 region."



percent_greater_zero = data.frame(fixef(maze_staub_n2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'familiarity1', 'plausibility1:familiarity1')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_staub_n2)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Familiarity", "Plausibility:Familiarity"), times = 1))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')


fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(n1_all_measures$measure)

fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N1Staub
#| fig-cap: "Plot of log reaction time at the N1 region as a function of plausibility and familiarity."


model_n1_binary_plot_staub = plot(conditional_effects(maze_staub_n1), plot = F)[[3]] +
  theme_bw()

model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") +
  scale_x_discrete(labels = c("implausible" = "Implausible", "plausible" = "Plausible")) +
  ylab("Log Reaction Time") +
  xlab("Plausibility") +
  theme_bw() +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14)
  )


model_n1_binary_plot_staub

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N2Staub
#| fig-cap: "Plot of log reaction time at the N2 region as a function of plausibility and familiarity."


model_n1_binary_plot_staub = plot(conditional_effects(maze_staub_n2), plot = F)[[3]] +
  theme_bw()



model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") +
  scale_x_discrete(labels = c("implausible" = "Implausible", "plausible" = "Plausible")) +
  ylab("Log Reaction Time") +
  xlab("Plausibility") +
  theme_bw() +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14)
  )

model_n1_binary_plot_staub


```

### Discussion

Our results directly replicate @staubTimeCoursePlausibility2007 using the Maze task, demonstrating the viability of this method for the tasks at hand. For the N1 region, while there was a clear increase in reaction time for items in the implausible condition, there was no interaction effect between plausibility and familiarity. In other words, the effect of plausibility was the same for both familiar and novel compound nouns. If familiar compound nouns are stored holistically, however, it is possible that we would see less of a (im)plausibility effect relative to novel items, because readers might be predicting the second noun in the compound upon reading the first noun. Recall the earlier example, reproduced below for convenience:

\begin{exe}
\ex Local Plausible and Local Implausible Sentences
\begin{xlist}
\ex Novel Compound
\begin{xlist}
\ex The zookeeper picked up the monkey medicine that was in the enclosure. \hfill \emph{plausible}
\ex The zookeeper spread out the monkey medicine that was in the enclosure. \hfill \emph{implausible}
\end{xlist}
\ex Familiar Compound
\begin{xlist}
\ex Jenny looked out on the huge mountain lion pacing in its cage. \hfill \emph{plausible}
\ex Jenny heard the huge mountain lion pacing in its cage. \hfill \emph{implausible} 
\end{xlist}
\end{xlist}
\end{exe}

\noindent It is possible that if *mountain lion* was stored holistically, then upon reading *Jenny heard the huge mountain...*, the reader might have less difficulty with the local implausibility (relative to a low-frequency compound noun) because they would predict *lion*, which would eliminate the implausibility (*heard the mountain lion* is not implausible). However, we do not see this. Instead, the effect of plausibility is similar for both familiar and novel items. One possible explanation for these results is that the familiar phrases are not necessarily stored. Instead storage might be driven by predictability. If this is the case then it would explain why we do not see this effect in @staubTimeCoursePlausibility2007 or in Experiment 1, especially since all of the items used in @staubTimeCoursePlausibility2007 are low-predictability compound nouns.

At the N2 region, the decrease in reaction time for familiarity is not surprising given that familiarity, as previously mentioned, was based on the frequency of the compound noun as a whole, however the increase in reaction time for the plausible condition is interesting, especially since the sentences were only locally implausible on the N1 region: the second noun in the compound always eliminated the local implausibility. It is possible this increase in reaction time is a garden path effect for committing to an interpretation of the sentence with the N1 and having to reanalyze the sentence. For example, when reading *Jenny looked upon the huge mountain...*, after reading *lion*, the reader may need to reanalyze the sentence, as the subject is not looking upon a mountain at all, but rather they are looking at a *mountain lion*. However, in the implausible condition participants may not fully commit to the interpretation since it is locally implausible, and thus may be waiting for a choice that eliminates the implausibility, thus explaining the absence of a similar slowdown in the implausible condition.

## Experiment 2

Experiment 1 demonstrated that readers are slower to read the first noun in a compound noun that is in a locally implausible context, regardless of the frequency of the compound noun. In Experiment 2, we examine whether readers can overcome this slowdown due to the local implausibility if the compound noun is a high-predictability compound noun.

### Methods

#### Participants

Participant recruitment was identical to Experiment 1. 105 participants were recruited, and 19 participants were excluded for having an accuracy of below 70%, leaving a total of 86 participants. All participants self-reported being native English speakers.

#### Stimuli

We operationalized predictability through @eq-oddsratio. Specifically, predictability is the number of times the compound noun occurs divided by the number of times that the first noun occurs without the second noun occurring immediately after.

$$
\frac{\mathrm{count(\textit{peanut butter})}}{\mathrm{count(\textit{peanut})} - \mathrm{count(\textit{peanut butter})}} 
$$ {#eq-oddsratio}

In non-mathematical terms, @eq-oddsratio quantifies how predictable the first noun is of the second noun (i.e., how likely the second noun is to follow after the first noun, relative to every other word that could follow). For example, the odds ratio of *peanut butter* would be the odds ratio of the compound noun -- *peanut butter* -- to the first noun -- *peanut* -- when *butter* does not follow it.

In order to collect the most predictable compound nouns, we searched the Google *n*-grams corpus [@michel2011quantitativeanalysisculture] using the ZS Python package [@smithZSFileFormat2014]. We then collected the compound nouns with the highest predictability values, using the following exclusion criteria: excluding words with a match count below 90,000,[^staub_rep_ext-6] excluding nonsense words, proper nouns, technical words (e.g., *tenth circuit*), and words in which we could not create locally plausible and implausible sentences.[^staub_rep_ext-7] We gathered a total of 37 compound nouns for our high-predictability condition.

[^staub_rep_ext-6]: This was done in order to help filter out nonsense words (e.g., *teawhit head*) as well as eliminate words that had high predictability scores but were just a product of the corpus and unlikely to reflect the input of human learners (e.g., *broomwheat tea* which has a predictability score of 287 in the corpus).

[^staub_rep_ext-7]: Given our methodology, we needed to be able to make sentences that were plausible and implausible using the same compound noun. This restriction meant we had to exclude words like *Parmesan cheese* where it would be impossible for the reading at the N1 region to be implausible without the reading of the compound noun also being implausible.

We subsequently normed the sentences we created using the high-predictability compounds, as well as the sentences from @staubTimeCoursePlausibility2007 which we confirmed were all low-predictability compounds relative to our compound nouns.

We followed the same methodology as @staubTimeCoursePlausibility2007 for our norming procedure: we provided participants with each item in four conditions (see below) and asked participants to rate each sentence on a 7-point Likert scale in terms of how well the last word fit in the sentence. No participant rated more than one version of each sentence. Crucially, for each item we ensured that sentence 3c received a lower rating than the other 3 versions of the sentences.

\begin{exe}
\ex 
\begin{xlist}
\ex Jimmy picked up the peanut. \hfill \emph{plausible, through the first noun} 
\ex Jimmy picked up the peanut butter. \hfill \emph{plausible, through the second noun} 
\ex Jimmy spread out the peanut. \hfill \emph{implausible, through the first noun} 
\ex Jimmy spread out the peanut butter. \hfill \emph{implausible, through the second noun} 
\end{xlist}
\end{exe}

Finally, we excluded items in which the implausible sentence through the first noun was rated more or similarly well to the other conditions (i.e., the plausible sentence through the first noun, the plausible sentence through the second noun, and the implausible sentence through the second noun). It is important to note that due to our experimental design, the implausible sentence through the second noun is technically plausible at the second noun, because the second noun eliminates the local implausibility. Thus this condition should also receive a high rating, despite being the implausible condition. The mean values for each condition are as follows: plausible, through the first noun: 5.58 (sd = 0.78); plausible, through the second noun: 5.41 (sd = 0.71); implausible, through the first noun: 3.13 (0.63); implausible, through the second noun: 5.47 (sd = 0.82).

After norming, we selected sentences such that the difference in plausibility values between the plausible and implausible conditions were roughly the same for the high-predictability and low-predictability conditions. This was done to avoid conflating an interaction effect between predictability and plausibility with an item-specific effect. That is, if the plausibility effect was smaller for high-predictability sentences relative to the low-predictability sentences, then it would be impossible to tell if the interaction effect between predictability and plausibility is meaningful or just a product of our stimuli. The mean plausibility difference for the low-predictability items was 2.47 and the mean plausibility difference for the high-predictability items was 2.48. We confirmed that there was not a significant difference in plausibility values through a t-test (t = 0.0446, df = 39, p = 0.52). After accounting for this, we ended up with 21 high-predictability and 21 low-predictability items [which were taken from @staubTimeCoursePlausibility2007], for a total of 42 items. Lastly, in order to avoid participants discerning the experimental design we also included 188 filler items.

#### Procedure

Following Experiment 1, we used the A-maze task [@boyceMazeMadeEasy2020] with automatically-generated distractor items [@gulordava2018colorlessgreenrecurrent]. Our dependent variable was reaction time and our independent variables were plausibility and predictability. We again used ibex farm to run our maze task. Sentences were presented in a random order and each word was presented an equal amount of times on the left and right side of the screen. Additionally, each item appeared an equal number of times in the implausible and plausible context and no participant was presented with the same item in more than one condition.

#### Analysis

The data was analyzed using Bayesian linear regression models, as implemented in the *brms* package [@burknerBrmsPackageBayesian2017] within the R programming environment [@Rpackage]. We subsetted the data into two sets based on the region: one set for the first noun in the compound noun and one set for the second noun in the compound. The primary dependent variable was log reaction time for both of these regions [following @boyceMazeMadeEasy2020]. The independent variables were plausibility and predictability. Reaction time was modeled as a function of plausibility and predictability, along with their interaction, with maximal random effects [@barrRandomEffectsStructure2013]. The formula used for the model is presented in @eq-model2 below, with *Plaus* as plausibility and *Predic* as predictability.

$$
Reaction Time\sim Plaus*Predict+(Plaus*Predict|Subject)+(Plaus|Item) 
$$ {#eq-model2}

### Results

As mentioned in the methods section, for the purpose of the analysis, the data was divided into two regions: the N1 region and the N2 region, which were the first and second noun in the compound noun respectively. The results of the Bayesian regression models for the N1 region are presented in @tbl-N1Predictability and @tbl-N1LogOdds, and visualized in @fig-N1Predictability and @fig-N1LogOdds. The results of the N2 region are presented in @tbl-N2Predictability and @tbl-N2LogOdds, and visualized in @fig-N2Predictability and @fig-N2LogOdds.

With regards to the N1 region, @tbl-N1Predictability presents the results of the analysis we ran with predictability as a binary predictor (high or low), while @tbl-N1LogOdds presents the results of the analysis we ran with predictability as a continuous predictor (operationalized as the log odds ratio). Our results demonstrate that, similar to experiment 1, there was an increase in reaction time for the implausible condition, but no effect of predictability or the interaction between the two.

As in Experiment 1, we once again conducted a post-hoc Bayes factor analysis to compare the interaction effect to the null hypothesis (interaction effect = 0). We found a Bayes Factor value of 15.67 which constitutes strong support for the null hypothesis. Specifically, we used the Save-Dickey density ratio method, which involves comparing two models: one in which the value of interest is fixed (in this case, the interaction effect was fixed at zero) and one in which the value of interest is free to vary (in this case, the interaction effect was allowed to vary). The Bayes factor is then obtained by dividing the height of the posterior for the parameter under the model that allows the interaction effect to vary by the height of the prior for that parameter [@wagenmakers2010bayesianhypothesistesting]. We computed the Bayes factor for the N1 region and not the N2 region because the interaction at the N2 region is not of particular interest to our theoretical question.

With regards to the N2 region, @tbl-N2Predictability presents the results of the analysis we ran with predictability as a binary predictor (high or low), while @tbl-N2LogOdds presents the results of the analysis we ran with predictability as a continuous predictor (operationalized as the log odds ratio). Our results, as in Experiment 1, demonstrate an increase in reaction time in the plausible condition and and a decrease in reaction time in the high-predictability condition, but no interaction effect between plausibility and predictability.

@fig-N1Predictability and @fig-N2Predictability provide visualizations of the analyses run with predictability as a binary variable while @fig-N1LogOdds and @fig-N2LogOdds present analyses with predictability as a continuous variable.

```{r, echo = F, message = F}
#| label: tbl-N1Predictability
#| tbl-cap: "Regression analysis results for the N1 region with predictability as a binary predictor (high or low)."



percent_greater_zero = data.frame(fixef(maze_pred_n1_v1, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'Predictability1', 'plausibility1:Predictability1')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_pred_n1_v1)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Predictability", "Plausibility:Predictability"), times = 1))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')


fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')



```

```{r, echo = F, message = F}
#| label: tbl-N1LogOdds
#| tbl-cap: "Regression analysis results for the N1 region with predictability as a continuous predictor (log odds ratio)."



percent_greater_zero = data.frame(fixef(maze_pred_n1_v2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'logoddsratio', 'plausibility1:logoddsratio')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_pred_n1_v2)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "Plausibility", "LogOdds", "Plausibility:LogOdds"), times = 1))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'LogOdds', 'Plausibility:LogOdds')

fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')




```

```{r, echo = F, message = F}
#| label: tbl-N2Predictability
#| tbl-cap: "Regression analysis results for the N2 region with predictability as a binary predictor (high or low)."



percent_greater_zero = data.frame(fixef(maze_pred_n2_v1, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'Predictability1', 'plausibility1:Predictability1')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_pred_n2_v1)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability'))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')

fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')



```

```{r, echo = F, message = F}
#| label: tbl-N2LogOdds
#| tbl-cap: "Regression analysis results for the N2 region with predictability as a continuous predictor (log odds ratio)."



percent_greater_zero = data.frame(fixef(maze_pred_n2_v2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'LogOddsRatio', 'plausibility1:LogOddsRatio')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_pred_n2_v2)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = c('Intercept', 'Plausibility', 'LogOdds', 'Plausibility:LogOdds'))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'LogOdds', 'Plausibility:LogOdds')

fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')



```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N1Predictability
#| fig-cap: "Plot of the N1 region with predictability as a binary variable (high or low)."


model_n1_binary_plot_staub = plot(conditional_effects(maze_pred_n1_v1), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Log Reaction Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 14)) 

model_n1_binary_plot_staub

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N1LogOdds
#| fig-cap: "Plot of the N1 region with predictability as a continuous variable."


model_n1_binary_plot_staub = plot(conditional_effects(maze_pred_n1_v2), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Plausibility", fill = "Plausibility") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Log Reaction Time') +
  xlab('Log Odds Ratio') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 14)) 

model_n1_binary_plot_staub

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N2Predictability
#| fig-cap: "Plot of the N2 region with predictability as a binary variable (high or low)."


model_n1_binary_plot_staub = plot(conditional_effects(maze_pred_n2_v1), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Predictability", fill = "Predictability") +
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Log Reaction Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 14)) 

model_n1_binary_plot_staub

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N2LogOdds
#| fig-cap: "Plot of the N2 region with predictability as a continuous variable."


model_n1_binary_plot_staub = plot(conditional_effects(maze_pred_n2_v2), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Plausibility", fill = "Plausibility") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Log Reaction Time') +
  xlab('Log Odds Ratio') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 14)) 

model_n1_binary_plot_staub

```

### Discussion

Experiment 2 replicates and extends Experiment 1 using predictability instead of familiarity (i.e., phrasal frequency). Interestingly, the results of Experiment 2 were extremely similar to the results of Experiment 1: There was no interaction effect between predictability and plausibility on the RTs for the N1 condition. Additionally, while we see an effect of implausibility on the N1 region, we don't see an effect of predictability. This is expected since predictability is defined as the odds that the N2 appears given the N1, so we should see this effect on the N2 region, not the N1 region.

The results of the N2 region also bear remarkable similarities to our results in Experiment 1: There was a plausibility and predictability effect, but no interaction between the two. Specifically, there was an *increase* in reaction time for items in the plausible condition relative to the implausible condition. It is possible that, as mentioned in the discussion section of Experiment 1, this increase in reaction time is a garden path effect for committing to an interpretation of the sentence with the N1 as the head noun and having to reanalyze the sentence.

## Experiment 3

In Experiments 1 and 2, we demonstrated that there is a slowdown at the N1 region in locally implausible contexts. However, the maze task requires participants to actively decide on the continuation of the sentence (by selecting the correct word). This may encourage readers to commit to an interpretation more than they would in a more naturalistic reading task. Thus, in Experiment 3, we directly replicate Experiment 1 using eye-tracking. We use the same Experimental items and Filler items as in Experiment 1, however we also included comprehension questions to check participants' attention.

### Methods

#### Participants

46 native English speakers were recruited from the University of California, Davis subjects pool. They were given course credit in exchange for their participation. All participants had normal or corrected vision.

#### Materials

The materials were identical to Experiment 1, however they also included comprehension questions.

We recorded participants' right pupil movements using the Eyelink 1000 Plus. Participants were seated 850mm away from the screen, which was 531.3mm in width, 298.8mm in height, and had a resolution of 1920x1080.

Comprehension was checked for non-experimental trials and participants below 80% accuracy were excluded. Out of our 46 participants, 2 were excluded for falling below the accuracy threshold.

#### Analyses

Prior to our analyses, sentences with blinks were excluded and fixations less than 80ms in duration and within one character of the nearest fixation were merged into that fixation [following @staubTimeCoursePlausibility2007]. For our regions of interest (the first noun and the second noun in the compound noun), we computed first fixation duration, first pass time, go-past time, and first-pass regression.

For each analysis, our independent variables were plausibility (high or low), familiarity (high or low), and their interaction. We also included random slopes for condition and predictability by subject and plausibility by compound noun as well as intercepts for subject and compound noun. For each of our models, categorical variables were sum-coded, where the intercept represents the grand mean and the fixed-effect coefficient estimates represent the distance from the grand mean.

### Results

#### N1 Region

Our results at the N1 region are demonstrated in @tbl-fullmodelresultsn1staub and visualized in @fig-fullmodelresultsn1staub. At the N1 region, we find main-effects of plausibility for first fixation duration, go-past time, and first-pass regression, but not for gaze times. Additionally, we find no effects of familiarity. Finally, for go-past times we find an interaction between plausibility and predictability such that the slowdown of the implausible context was greater for familiar items than novel items.

```{r, echo = F, message = F}
#| label: tbl-fullmodelresultsn1staub
#| tbl-cap: "Model results for each eye-tracking measure at the N1 region."



percent_greater_zero = data.frame(fixef(model_n1_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm1 = as.data.frame(fixef(model_n1_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First Fixation Duration')

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n1_gaze_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))


fixefsm2 = as.data.frame(fixef(model_n1_gaze_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'Gaze/First-Pass Duration')

rownames(fixefsm2) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n1_regression_path_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))


fixefsm3 = as.data.frame(fixef(model_n1_regression_path_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure='Go-Past Time')

rownames(fixefsm3) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n1_regression_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm4 = as.data.frame(fixef(model_n1_regression_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First-Pass Regression')

rownames(fixefsm4) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')

n1_all_measures = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Familiarity", "Plausibility:Familiarity"), times = 4))


n1_all_measures = n1_all_measures %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

groupings=rle(n1_all_measures$measure)

n1_all_measures %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, fig.align = 'center', warning = F, message = F}
#| label: fig-fullmodelresultsn1staub
#| fig-cap: "Visualization of the effects of plausibility and familiarity on each eye-tracking measure at the N1 region."
#| fig-width: 7
#| fig-height: 7


model_n1_binary_plot_staub = plot(conditional_effects(model_n1_staub), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Familiarity", fill = "Familiarity") + 
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  ylab('First Fixation Duration') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n1_binary_gaze_plot_staub = plot(conditional_effects(model_n1_gaze_staub), plot = F)[[3]] +
  theme_bw()


model_n1_binary_gaze_plot_staub = model_n1_binary_gaze_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('Gaze/First-Pass Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n1_go_past_plot_staub = plot(conditional_effects(model_n1_regression_path_staub), plot = F)[[3]] +
  theme_bw()


model_n1_go_past_plot_staub = model_n1_go_past_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('Go-Past Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 

model_n1_binary_first_pass_plot_staub = plot(conditional_effects(model_n1_regression_staub), plot = F)[[3]] +
  theme_bw()


model_n1_binary_first_pass_plot_staub = model_n1_binary_first_pass_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('First-Pass Regression') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_n1_binary_plot_staub, model_n1_binary_gaze_plot_staub, model_n1_go_past_plot_staub, model_n1_binary_first_pass_plot_staub, nrow = 2, ncol = 2, common.legend=T, legend = 'top')

```

#### N2 Region

Our results at the N2 region are demonstrated in @tbl-fullmodelresultsn2staub and visualized in @fig-fullmodelresultsn2staub. At the N2 region, we find main-effects of familiarity for first fixation duration and gaze times, and marginal effects for go-past times and first-pass regression. We find no main-effect of plausibility, which is expected because the implausible condition is plausible at the N2 region. We also find no interaction effect between plausibility and familiarity.

```{r, echo = F, message = F}
#| label: tbl-fullmodelresultsn2staub
#| tbl-cap: "Results of models for each eye-tracking measure at the N2 region."



percent_greater_zero = data.frame(fixef(model_n2_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm1 = as.data.frame(fixef(model_n2_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First Fixation Duration')

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n2_gaze_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))


fixefsm2 = as.data.frame(fixef(model_n2_gaze_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'Gaze/First-Pass Duration')

rownames(fixefsm2) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n2_regression_path_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))


fixefsm3 = as.data.frame(fixef(model_n2_regression_path_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure='Go-Past Time')

rownames(fixefsm3) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n2_regression_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm4 = as.data.frame(fixef(model_n2_regression_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First-Pass Regression')

rownames(fixefsm4) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')

n2_all_measures = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Familiarity", "Plausibility:Familiarity"), times = 4))


n2_all_measures = n2_all_measures %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

groupings=rle(n2_all_measures$measure)

n2_all_measures %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, fig.align = 'center', warning = F, message = F}
#| label: fig-fullmodelresultsn2staub
#| fig-cap: "Visualization of the effects of plausibility and familiarity on each eye-tracking measure at the N2 region."
#| fig-width: 7
#| fig-height: 7


model_n2_binary_plot_staub = plot(conditional_effects(model_n2_staub), plot = F)[[3]] +
  theme_bw()


model_n2_binary_plot_staub = model_n2_binary_plot_staub +
  labs(color = "Familiarity", fill = "Familiarity") + 
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  ylab('First Fixation Duration') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n2_binary_gaze_plot_staub = plot(conditional_effects(model_n2_gaze_staub), plot = F)[[3]] +
  theme_bw()


model_n2_binary_gaze_plot_staub = model_n2_binary_gaze_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('Gaze/First-Pass Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n2_go_past_plot_staub = plot(conditional_effects(model_n2_regression_path_staub), plot = F)[[3]] +
  theme_bw()


model_n2_go_past_plot_staub = model_n2_go_past_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('Go-Past Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 

model_n2_binary_first_pass_plot_staub = plot(conditional_effects(model_n2_regression_staub), plot = F)[[3]] +
  theme_bw()


model_n2_binary_first_pass_plot_staub = model_n1_binary_first_pass_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('First-Pass Regression') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_n2_binary_plot_staub, model_n2_binary_gaze_plot_staub, model_n2_go_past_plot_staub, model_n2_binary_first_pass_plot_staub, nrow = 2, ncol = 2, common.legend=T, legend = 'top')

```

### Discussion

Experiment 3 demonstrates that readers have longer first-fixation times, longer go-past times, and more first-pass regressions in implausible contexts than in plausible contexts. Further, we find an interaction effect in the opposite direction from predicted for go-past times: the effect of plausibility is greater for familiar items than in novel items. This suggests that the slowdown generated by the locally implausible context is even greater for familiar items than novel items. However, since we only find this in one eye-tracking measure, it is unclear whether this effect is robust or not.

The results of Experiment 3 mostly replicate the results found in both Experiment 1 and @staubTimeCoursePlausibility2007. That is, in general, readers take longer to process the first noun in the locally implausible condition. Further, the frequency of the compound noun does not alleviate this increase in processing difficulty. In other words, even in cases where the compound noun is frequent, the implausible context causes an increase in processing time equal in magnitude to the increase of the implausible context for lower frequency compound nouns. We do find an interaction effect in the opposite direction as predicted, however since we find it in only one eye-tracking measure it is unlikely that it indicates that the local implausibility is causing readers to have more difficulty processing frequent compound nouns than infrequent compound nouns.

## Experiment 4

Experiment 3 demonstrated an effect of plausibility at the N1 region that was consistent regardless of the frequency of the compound noun. In order to determine whether predictability shows similar effects, in Experiment 4 we replicate Experiment 2 using eye-tracking. The Experimental and Filler items were identical, but we also included comprehension questions to check participants' attention.

### Methods

#### Participants

56 native English speakers were recruited from the University of California, Davis subjects pool. They were given course credit in exchange for their participation. All participants had normal or corrected vision.

#### Materials

The materials here were identical to those in Experiment 2, with the exception of the added comprehension questions.

#### Procedure

We recorded participants' right pupil movements using the Eyelink 1000 Plus. Participants were seated 850mm away from the screen. Our screen resolution was 1920x1080, 531.3mm in width, and 298.8mm in height.

Comprehension was checked for non-experimental trials and participants below 80% accuracy were excluded. Out of our 56 participants, 0 were excluded for falling below the accuracy threshold.

#### Analyses

Prior to our analyses, sentences with blinks were excluded and fixations less than 80ms in duration and within one character of the nearest fixation were merged into that fixation [following @staubTimeCoursePlausibility2007]. For our regions of interest (the first noun and the second noun in the compound noun), we computed first fixation duration, first pass time, go-past time, and first-pass regression.

For each analysis, our independent variables were plausibility (high or low), (log) predictability (high or low), and their interaction. We also included random slopes for condition and predictability by subject and plausibility by compound noun as well as intercepts for subject and compound noun. For each of our models, categorical variables were sum-coded, where the intercept represents the grand mean and the fixed-effect coefficient estimates represent the distance from the grand mean.

### Results

#### N1 Region

Our results at the N1 region are demonstrated in @tbl-fullmodelresultsn1 and visualized in @fig-fullmodelresultsn1. At the N1 region, we find main-effects of plausibility for only first-pass regression. We find no effect of plausibility for first fixation duration, gaze time, and go-past time. Additionally, we find no effects of predictability. Finally, we find an interaction effect between plausibility and predictability for first fixation duration and first-pass regression. These interaction effects are in the opposite direction such that the slowdown caused by the implausible condition was greater for high-predictability items relative to low-predictability items in first-pass regression, but smaller for high-predictability items relative to low-predictability items in first fixation duration.

```{r, echo = F, message = F}
#| label: tbl-fullmodelresultsn1
#| tbl-cap: "Model results for each eye-tracking measure at the N1 region."



percent_greater_zero = data.frame(fixef(model_n1_binary, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))

#table 1
fixefsm1 = as.data.frame(fixef(model_n1_binary)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First Fixation Duration')

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n1_binary_gaze, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))


fixefsm2 = as.data.frame(fixef(model_n1_binary_gaze)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'Gaze/First-Pass Duration')

rownames(fixefsm2) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')



percent_greater_zero = data.frame(fixef(model_n1_binary_go_past, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))


fixefsm3 = as.data.frame(fixef(model_n1_binary_go_past)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure='Go-Past Time')

rownames(fixefsm3) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')



percent_greater_zero = data.frame(fixef(model_n1_binary_first_pass, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))

#table 1
fixefsm4 = as.data.frame(fixef(model_n1_binary_first_pass)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First-Pass Regression')

rownames(fixefsm4) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')

n1_all_measures_exp4 = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Predictability", "Plausibility:Predictability"), times = 4))

n1_all_measures_exp4 = n1_all_measures_exp4 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))



groupings=rle(n1_all_measures_exp4$measure)

n1_all_measures_exp4 %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, out.width = '100%', fig.align = 'center', warning = F, message = F}
#| label: fig-fullmodelresultsn1
#| fig-cap: "Visualization of the effects of plausibility and predictability on each eye-tracking measure at the N1 region."
#| fig-width: 7
#| fig-height: 7


model_n1_binary_plot = plot(conditional_effects(model_n1_binary), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot = model_n1_binary_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('First Fixation Duration') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n1_binary_gaze_plot = plot(conditional_effects(model_n1_binary_gaze), plot = F)[[3]] +
  theme_bw()


model_n1_binary_gaze_plot = model_n1_binary_gaze_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Gaze/First-Pass Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 

model_n1_go_past_plot = plot(conditional_effects(model_n1_binary_go_past), plot = F)[[3]] +
  theme_bw()


model_n1_go_past_plot = model_n1_go_past_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Go-Past Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n1_binary_first_pass_plot = plot(conditional_effects(model_n1_binary_first_pass), plot = F)[[3]] +
  theme_bw()


model_n1_binary_first_pass_plot = model_n1_binary_first_pass_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('First-Pass Regression') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_n1_binary_plot, model_n1_binary_gaze_plot, model_n1_go_past_plot, model_n1_binary_first_pass_plot, nrow = 2, ncol = 2, common.legend=T, legend = 'top')

```

#### N2 Region

Our results at the N2 region are demonstrated in @tbl-fullmodelresultsn2 and visualized in @fig-fullmodelresultsn2. At the N2 region, we find a main-effect of predictability for only the first fixation duration measure. We find no effect of plausibility, as expected because the N2 region eliminates the implausibility effect. We also find no interaction between plausibility and predictability in any of the measures.

```{r, echo = F, message = F}
#| label: tbl-fullmodelresultsn2
#| tbl-cap: "Model results for each eye-tracking measure at the N2 region."



percent_greater_zero = data.frame(fixef(model_n2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))

#table 1
fixefsm1 = as.data.frame(fixef(model_n2)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First Fixation Duration')

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n2_binary_gaze, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))


fixefsm2 = as.data.frame(fixef(model_n2_binary_gaze)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'Gaze/First-Pass Duration')

rownames(fixefsm2) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')



percent_greater_zero = data.frame(fixef(model_n2_binary_go_past, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))


fixefsm3 = as.data.frame(fixef(model_n2_binary_go_past)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure='Go-Past Time')

rownames(fixefsm3) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')



percent_greater_zero = data.frame(fixef(model_n2_binary_first_pass, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))

#table 1
fixefsm4 = as.data.frame(fixef(model_n2_binary_first_pass)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First-Pass Regression')

rownames(fixefsm4) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')

n2_all_measures_exp4 = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Predictability", "Plausibility:Predictability"), times = 4))

n2_all_measures_exp4 = n2_all_measures_exp4 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


groupings=rle(n2_all_measures_exp4$measure)

n2_all_measures_exp4 %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, out.width = '100%', fig.align = 'center', warning = F, message = F}
#| label: fig-fullmodelresultsn2
#| fig-cap: "Visualization of the effects of plausibility and predictability on each eye-tracking measure at the N2 region."
#| fig-width: 7
#| fig-height: 7


model_n2_binary_plot = plot(conditional_effects(model_n2), plot = F)[[3]] +
  theme_bw()


model_n2_binary_plot = model_n2_binary_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('First Fixation Duration') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n2_binary_gaze_plot = plot(conditional_effects(model_n2_binary_gaze), plot = F)[[3]] +
  theme_bw()


model_n2_binary_gaze_plot = model_n2_binary_gaze_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Gaze/First-Pass Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 

model_n2_go_past_plot = plot(conditional_effects(model_n2_binary_go_past), plot = F)[[3]] +
  theme_bw()


model_n2_go_past_plot = model_n2_go_past_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Go-Past Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n2_binary_first_pass_plot = plot(conditional_effects(model_n2_binary_first_pass), plot = F)[[3]] +
  theme_bw()


model_n2_binary_first_pass_plot = model_n2_binary_first_pass_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('First-Pass Regression') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_n2_binary_plot, model_n2_binary_gaze_plot, model_n2_go_past_plot, model_n2_binary_first_pass_plot, nrow = 2, ncol = 2, common.legend=T, legend = 'top')

```

#### Filler Items

In addition to our experimental stimuli, our study also included sentences that contained a frequency manipulation at the word level. For example, in the below sentences, *satchel* is low-frequency and *account* is high-frequency. Thus, in order to confirm that the results in Experiment 4 were not due to measurement error, we examined the effect of frequency in our filler items on each of the eye-tracking measures.

\begin{exe}
\ex Filler Sentences \label{fillers}
\begin{xlist}
\ex{Take your money out of the \textbf{satchel} and pay off the debt. \hfill \emph{low-frequency}}
\ex{Take your money out of the \textbf{account} and pay off the debt. \hfill \emph{high-frequency}}
\end{xlist}
\end{exe}

Our results are presented in @tbl-filleritemsall and visualized in @fig-filleritemsall. We find an effect of frequency in each of the four eye-tracking measures we looked at. These results suggest that the results we found for our experimental items are not due to measurement error.

```{r, echo = F, message = F}
#| label: tbl-filleritemsall
#| tbl-cap: "Model results for filler items for each eye-tracking measure."



percent_greater_zero = data.frame(fixef(first_fixation_filler, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'filler_higher_freq1')))

#table 1
fixefsm1 = as.data.frame(fixef(first_fixation_filler)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = "First Fixation Duration")

rownames(fixefsm1) = c('Intercept', 'Frequency')



percent_greater_zero = data.frame(fixef(gaze_filler, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'filler_higher_freq1')))

#table 1
fixefsm2 = as.data.frame(fixef(gaze_filler)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = "Gaze/First-Pass Duration")

rownames(fixefsm2) = c('Intercept', 'Frequency')



percent_greater_zero = data.frame(fixef(go_past_filler, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'filler_higher_freq1')))

#table 1
fixefsm3 = as.data.frame(fixef(go_past_filler)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = "Go-Past Time")

rownames(fixefsm3) = c('Intercept', 'Frequency')




percent_greater_zero = data.frame(fixef(first_pass_filler, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'filler_higher_freq1')))

#table 1
fixefsm4 = as.data.frame(fixef(first_pass_filler)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = "First-Pass Regression")

rownames(fixefsm4) = c('Intercept', 'Frequency')



fillers_exp4 = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Frequency"), times = 4))





groupings=rle(fillers_exp4$measure)

fillers_exp4 %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')
```

```{r, echo = F, fig.align = 'center', warning = F, message = F}
#| label: fig-filleritemsall
#| fig-cap: "Visualization of the effects of frequency on each eye-tracking measure for filler items."
#| fig-width: 7
#| fig-height: 7


model_first_fixation_filler_plot = plot(conditional_effects(first_fixation_filler), plot = F)[[1]] +
  theme_bw()


model_first_fixation_filler_plot = model_first_fixation_filler_plot +
  ylab('First Fixation') +
  xlab('Frequency') +
  scale_x_discrete(breaks = c(0, 1), labels = c("Low", "High")) +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 



model_gaze_filler_plot = plot(conditional_effects(gaze_filler), plot = F)[[1]] +
  theme_bw()


model_gaze_filler_plot = model_gaze_filler_plot +
  ylab('Gaze/First-Pass Times') +
  xlab('Frequency') +
  scale_x_discrete(breaks = c(0, 1), labels = c("Low", "High")) +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_go_past_filler_plot = plot(conditional_effects(go_past_filler), plot = F)[[1]] +
  theme_bw()


model_go_past_filler_plot = model_go_past_filler_plot +
  ylab('Go-Past Times') +
  xlab('Frequency') +
  scale_x_discrete(breaks = c(0, 1), labels = c("Low", "High")) +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_first_pass_regression_filler_plot = plot(conditional_effects(first_pass_filler), plot = F)[[1]] +
  theme_bw()


model_first_pass_regression_filler_plot = model_first_pass_regression_filler_plot +
  ylab('First-Pass Regressions') +
  xlab('Frequency') +
  scale_x_discrete(breaks = c(0, 1), labels = c("High", "Low")) +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_first_fixation_filler_plot, model_gaze_filler_plot, model_go_past_filler_plot, model_first_pass_regression_filler_plot, nrow = 2, ncol = 2)

```

### Discussion

In Experiment 4, we find an effect of plausibility only in first-pass regressions. Interestingly, we do find an effect of plausibility in first-fixation times for low-predictability items, but not for high-predictability items. While this follows our theoretical prediction that high-predictability items may be able to overcome the local implausibility, the results are difficult to reconcile with the results we see for first-pass regressions. For first-pass regressions, we observe the opposite pattern: there is an effect of plausibility on first-pass regressions for high-predictability items but not low-predictability items. Thus overall it seems unlikely that participants are accessing the holistic representation at the first noun.

We also find no effect of plausibility for gaze duration or go-past times, which was unexpected. It is unclear why we do not find a consistent slowdown at the N1 region in locally implausible contexts. One possibility is that perhaps the difference in plausibility for the sentences wasn't as large as we expected. This seems unlikely given that we normed the plausibility of the items beforehand, however it is possible that our participants in the experimental study had different plausibility interpretations than those who normed our data, since these were different people. It is also unlikely that the lack of an effect of implausibility is due to measurement error, because we do find significant effects in all four eye-tracking measures for the frequency manipulation in our filler items.

Finally, the lack of a predictability effect suggests one of two possibilities. First, it is possible that predictability does not drive storage and that high-predictability compound nouns are not stored holistically. Thus, upon reading the first noun in the compound, readers simply access that noun and no other representations, thus generating a slowdown because it is implausible. Alternatively, it is possible that they are stored holistically, but that processing still unfolds incrementally, and readers are not able to access the representation of the compound noun until they've heard more than just the first noun in the compound. Both of these results explain why we see no interaction effect.

## Conclusion

The present study examined the processing of compound nouns in locally implausible and locally plausible contexts, specifically with respect to their phrasal frequency and predictability. In Experiment 1 we replicated @staubTimeCoursePlausibility2007 using the A-maze task [@boyceMazeMadeEasy2020] and found an increase in reaction time for the implausible condition at N1 region, but no interaction effect between plausibility and familiarity. Additionally at the N2 region, we found an increase in reaction time for the plausible condition relative to the implausible condition and a decrease in reaction time for high-predictability items relative to low-predictability items.

In Experiment 2 we extended Experiment 1 but manipulating predictability instead of phrasal frequency. Similar to Experiment 1, we found an increase in reaction time for the implausible condition at the N1 region, but again found no interaction effect between plausibility and predictability. Also similar to Experiment 1, we found an increase in reaction time at the N2 region for the plausible condition and a decrease in reaction time for the high-predictability items.

In Experiments 3 and 4 we replicated the two experiments with eye-tracking. In Experiment 3, we found an effect of plausibility in first fixation times, go-past times, and first-pass regressions. We also found an interaction effect in go-past times such that high-frequency items had higher go-past times in the implausible condition, but low-frequency items did not. In Experiment 4, we find an effect of plausibility in first-fixation times for low-predictability items (but not high-predictability items) and an an effect of plausibility in first-pass regressions for high-predictability items but not low-predictability items.

Overall the results of experiments 1 and 2 suggest that the frequency or predictability of the second noun in the compound (given the first noun) has very little facilitatory effect on the processing of the first noun in implausible contexts relative to plausible contexts. That is, the increase in reaction time in the implausible condition for the N1 region was not mediated by the frequency or predictability of the compound noun. If participants were predicting the second noun upon reading the first noun, then we might expect to have seen a decrease in reaction time for the high-predictable items in the implausible condition relative to the low-predictability items because the second noun always eliminated the local implausibility.

Experiments 3 and 4 provide mixed evidence with respect to the effects of frequency and predictability on the processing of locally implausible contexts. On one hand, there seems to be a general effect of implausibility regardless of frequency or predictability, however in some reading measures such as first-fixation times in Experiment 4, predictability seemed to alleviate the slowdown in the implausible condition. On the other hand, for other measures, the slowdown generated by the implausible contexts was actually exacerbated for high-frequency or high-predictability items (e.g., first-pass regression times in both Experiment 3 and Experiment 4).

These results taken together suggest that in general there is an increase in processing time for locally implausible contexts. Additionally, this increase in processing time is not alleviated by the frequency or predictability of the compound noun. If compound nouns are stored holistically and participants are able to access the holistic representation at the first noun, then participants should have been able to overcome the increased processing time for the locally implausible contexts, because accessing the representation of the compound noun would have eliminated the implausibility.

There are a few possible explanations for the results we found. One possibility is simply that our high-predictability compound nouns aren't stored holistically. It is important to note that our compound nouns were the most predictable compound nouns in the entire Google *n*-grams corpus, thus it seems unlikely that they weren't predictable enough to be stored. However, it may be that English compound nouns have relatively low predictability relative to other multi-word phrases. The slowdown in the locally implausible context is not surprising if the compound nouns are not stored holistically because they would be processed incrementally. Thus readers would access the first noun in the compound noun initially, generating a slowdown in the locally implausible condition.

For example, in the sentence *The zookeeper spread out the monkey medicine that was in the enclosure*, *monkey medicine* is quite low-frequency, so it is unsurprising that it isn't stored holistically. Thus participants must process the compound noun incrementally. Upon reaching *...spread out the monkey...*, there is an increase in processing time due to the implausible nature of the interpretation. On the other hand, *mountain lion* is a frequent compound noun, and thus could in theory be stored holistically. If that were the case, in the sentence *Jenny heard the huge mountain lion pacing in its cage*, upon reaching *mountain*, the reader may be able to access the holistic representation *mountain lion*, which would eliminate the local implausibility because one cannot hear a *mountain*, but one certainly can hear a *mountain lion*. However, instead we see an increase in processing time regardless of frequency of the compound noun. Further, we find analogous results for predictability. That is, similarly, readers are not able to overcome the increase in processing time when the first noun is highly predictive of the second noun as well. Taken together, these results suggest that both high-frequency compound nouns and high-predictability compound nouns may not be stored holistically.

Another possibility is that the high-frequency and high-predictability compound nouns are stored holistically, but the processing consequences of them being stored holistically are such that there is no facilitatory effect in the processing of the first noun in the compound noun. That is, perhaps despite being stored holistically, readers might not commit to accessing the holistic representation of the compound noun until they've heard enough acoustics to eliminate possible competitors. For example, after hearing *peanut*, there are still a number of possible words other than *butter* that could occur. Even though *butter* has a high probability of occurring after *peanut*, it would be costly to the reader to commit to *butter* and then have to reinterpret the sentence upon hearing a different word. Thus, the reader may avoid committing to *peanut butter* until they are confident that it is what the speaker intended to say.

An interesting question that comes out of this is at what point readers do access the holistic representation? For example, in the case of *peanut butter*, do participants need to hear the entire phrase before accessing the holistically stored representation? To address this question, we turn to a literature that has a much longer history: word-recognition. A similar question has been asked on the word-level for over a century: Do people recognize a word by its component letters or by the whole [@reicher1969perceptualrecognitionfunction; @wheeler1970processeswordrecognition; @johnston1974perceptionletterswords; @huey1908psychologypedagogyreading; @pelli2003remarkableinefficiencyword; @goel2013wholegreatersum]? For example, readers are more accurate at identifying a letter when it occurs within a word [@johnston1980experimentaltestshierarchical]. Additionally, @goel2013wholegreatersum demonstrated that a word-recognition model that identifies the word based on the whole image performs better than a model that detects individual characters and then combines them. On the other hand, @pelli2003remarkableinefficiencyword demonstrated that human word-recognition accuracy is closer in accuracy to a feature-based model, than a holistic model. In disentangling these results, @johnston1980experimentaltestshierarchical proposed a hierarchical model where in features were recognized, then letters, then words. They found that this model accounted for the result that words are identified better in word-contexts than in isolation because identifying the word maintained the activation of the letter-representations longer than simply identifying the letter in isolation. These results suggest that word-recognition does involve recognizing each of the parts of the word, but it is still unclear how many of the parts of the word must be recognized to give rise to the recognition of the entire word.

Word-recognition is further complicated by the question of when a word is activated over its competitors. For example, words with many phonological neighbors are harder to recognize in noise and show longer lexical decision times than those with fewer phonological neighbors [where phonological neighbors are words that vary by exactly 1 phoneme from the target word, @goldinger1989priminglexicalneighbors]. Similar effects have been observed in visual word-recognition as well [@coltheart1977access]. These results suggest that readers are able to recognize a word before seeing all of the parts of the word. That is, if it was the case that readers wait until reading all of the letters to process a word, then the number of phonological competitors should not matter. However, if readers process the word at the point in which it is unlikely that the text refers to a different word, regardless of whether all the letters have been processed, then a word with fewer competitors will be activated earlier.

These results taken together provide some inspiration for how holistic representations of phrases may be activated: a holistically stored phrase may be processed incrementally, and the holistic representation may be accessed at the point where the evidence for the holistic representation of the phrase vastly outweights the evidence for its competitors. For example, the holistic representation for *peanut butter* may be accessed once evidence for *peanut butter* greatly outweighs the evidence for other bigrams beginning with *peanut*. This evidence may not be enough even for high-predictability compound nouns, because the first noun is still occasionally followed by other words. Although this account may make exceptions for extremely predictable words, such as *Habeus Corpus*, where there are extremely few competitors.

Finally, with respect to the increase in reaction time at the N2 region in the plausible condition that we found in Experiments 1 and 2, we do not see this effect in Experiments 3 and 4, suggesting that this effect may be a task-specific effect. This result suggests that in the maze task, participants may have a bias to analyze the first noun as the head noun and then have to reinterpret the sentence once it is clear that the noun is not the head noun. Further, since we don't see the same effect in the implausible condition, participants may not fully commit to an interpretation that is implausible.

In summary, the present study contributes to the current theories of sentence processing by demonstrating that during sentence processing, readers do not seem to access the holistic representation at the first noun (because then they would be able to overcome the implausibility). Instead, it is possible that high-frequency and high-predictability compound nouns are either not stored holistically, or if they are stored holistically, perhaps readers don't access the holistic representation until they have heard sufficient evidence for that representation. That is, even though *peanut* is predictive of *butter*, there are still many other words that can occur after *peanut*. Thus, readers may not access the holistic representation until they hear enough of the compound noun to rule out other competing possibilities.
