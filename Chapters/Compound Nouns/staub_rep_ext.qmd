```{r echo = F, warning = F, message = F}
options (contrasts = c("contr.sum","cont.sum"))
library(tidyverse)
library(brms)
library(RColorBrewer)
library(ggpubr)
library(knitr)
library(kableExtra)


data_n1 = read_csv('Data/data_analysis_n1.csv')



model_n1_binary = brm(FIRST_FIXATION_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n1, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       control = list(adapt_delta = 0.95),
       file = './Data/model_n1_binary')

model_n1_binary_gaze = brm(GAZE_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n1, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_binary_gaze')

model_n1_binary_first_pass = brm(FIRST_PASS_REGRESSION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_n1, 
       family = bernoulli(),
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_binary_regression')

model_n1_binary_go_past = brm(REGRESSION_PATH_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_path_n1, 
       chains = 4, 
       cores = 4, 
       warmup = 6000, 
       iter = 12000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_binary_regression_path')


model_n2 = brm (FIRST_FIXATION_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + plausibility|item), 
                data = data_analysis_first_fixation_n2, 
                chains = 4, 
                cores = 4, 
                warmup = 4000, 
                iter = 8000, 
                thin = 4, 
                control = list(adapt_delta = 0.99),
                file = './Data/model_n2'
)

model_n2_binary_gaze = brm (GAZE_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + plausibility|item), 
                data = data_analysis_first_fixation_n2, 
                chains = 4, 
                cores = 4, 
                warmup = 4000, 
                iter = 8000, 
                thin = 4, 
                prior = priors,
                control = list(adapt_delta = 0.99),
                file = './Data/model_n2_gaze'
)

model_n2_binary_go_past = brm (REGRESSION_PATH_DURATION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + plausibility|item), 
                data = data_analysis_regression_path_n2, 
                chains = 4, 
                cores = 4, 
                warmup = 4000, 
                iter = 8000, 
                thin = 4, 
                prior = priors,
                control = list(adapt_delta = 0.99),
                file = './Data/model_n2_regression_path'
)

model_n2_binary_first_pass = brm(FIRST_PASS_REGRESSION ~ condition*predictability_binary + (1 + condition*predictability_binary|subject) + (1 + plausibility|item), 
                data = data_analysis_regression_n2, 
                family = bernoulli(),
                chains = 4, 
                cores = 4, 
                warmup = 4000, 
                iter = 8000, 
                thin = 4, 
                prior = priors,
                control = list(adapt_delta = 0.99),
                file = './Data/model_n2_regression'
)


first_fixation_filler =
  brm(FIRST_FIXATION_DURATION ~ filler_higher_freq + (1 + filler_higher_freq||subject) + (1 + filler_higher_freq||filler_sentence) + (1|filler_word), 
       data = data_filler_first_fixation_filler, 
       chains = 4, 
       cores = 4, 
       warmup = 6000,  
       init = 0,
       iter = 12000, 
       thin = 4,
       #control = list(adapt_delta = 0.95),
       file = './Data/model_filler_first_fixation')


gaze_filler =
  brm(GAZE_DURATION ~ filler_higher_freq + (1 + filler_higher_freq||subject) + (1 + filler_higher_freq||filler_sentence) + (1|filler_word), 
       data = data_filler_gaze, 
       chains = 4, 
       cores = 4, 
       warmup = 6000, 
       init = 0,
       iter = 12000, 
       thin = 4,
       #control = list(adapt_delta = 0.95),
       file = './Data/model_filler_gaze')


go_past_filler =
  brm(REGRESSION_PATH_DURATION ~ filler_higher_freq + (1 + filler_higher_freq||subject) + (1 + filler_higher_freq||filler_sentence) + (1|filler_word), 
       data = data_filler_go_past, 
       chains = 4, 
       cores = 4, 
       warmup = 6000, 
       init = 0,
       iter = 12000, 
       thin = 4,
       #control = list(adapt_delta = 0.95),
       file = './Data/model_filler_go_past')

first_pass_filler =
  brm(FIRST_PASS_REGRESSION ~ filler_higher_freq + (1 + filler_higher_freq||subject) + (1 + filler_higher_freq||filler_sentence) + (1|filler_word), 
       data = data_filler_first_pass, 
       family = bernoulli(),
       chains = 4, 
       cores = 4, 
       warmup = 6000, 
       init = 0,
       iter = 12000, 
       thin = 4,
       #control = list(adapt_delta = 0.95),
       file = './Data/model_filler_first_pass')


model_n1_staub =
  brm(FIRST_FIXATION_DURATION ~ frequency*condition + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n1, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       control = list(adapt_delta = 0.95),
       file = './Data/model_n1_staub')


model_n1_gaze_staub =
  brm(GAZE_DURATION ~ frequency*condition + (1 + frequency*condition|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n1_staub, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_gaze_staub')



model_n1_regression_path_staub =
  brm(REGRESSION_PATH_DURATION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_path_n1_staub, 
       chains = 4, 
       cores = 4, 
       warmup = 10000, 
       iter = 20000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_regression_path_staub')


model_n1_regression_staub =
  brm(FIRST_PASS_REGRESSION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_n1_staub, 
       family = bernoulli(),
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n1_regression_staub')


model_n2_staub =
  brm(FIRST_FIXATION_DURATION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n2, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       control = list(adapt_delta = 0.95),
       file = './Data/model_n2_staub')

model_n2_gaze_staub =
  brm(GAZE_DURATION ~ condition*frequency + (1 + frequency*condition|subject) + (1 + condition|compound_noun), 
       data = data_analysis_first_fixation_n2_staub, 
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n2_gaze_staub2')

model_n2_regression_path_staub =
  brm(REGRESSION_PATH_DURATION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_path_n2_staub, 
       chains = 4, 
       cores = 4, 
       warmup = 10000, 
       iter = 20000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n2_regression_path_staub')


model_n2_regression_staub =
  brm(FIRST_PASS_REGRESSION ~ condition*frequency + (1 + condition*frequency|subject) + (1 + condition|compound_noun), 
       data = data_analysis_regression_n2_staub, 
       family = bernoulli(),
       chains = 4, 
       cores = 4, 
       warmup = 4000, 
       iter = 8000, 
       thin = 4,
       prior = priors,
       control = list(adapt_delta = 0.99),
       file = './Data/model_n2_regression_staub')

maze_staub_n1 = readRDS('./Data/model_n1_sum_coding.rds')
maze_staub_n2 = readRDS('./Data/model_n2_sum_coding.rds')

maze_pred_n1_v1 = readRDS('./Data/N1_simple1.rds')
maze_pred_n2_v1 = readRDS('./Data/N2_simple1.rds')
maze_pred_n1_v2 = readRDS('./Data/N1_simple2.rds')
maze_pred_n2_v2 = readRDS('./Data/N2_simple2.rds')

```

# Does Predictability Drive the Holistic Storage of Compound Nouns?

## Introduction

Learning a language is not a trivial task. In order to be successful, learners must accurately segment the continuous speech stream into smaller segments, including phrases, words, morphemes, and phonemes. One of the main questions that arises out of this task is what exactly is the size of the units that learners are storing. That is, are they storing individual words, entire sentences, phrases, or some combination of all of these? One possibility is that learners store very little outside of words and idioms. For example, traditional theories have argued that learners don't store any more than they need to: they store only what they can't form compositionally using a set of rules, and generate everything else [e.g., @chomsky1965]. According to these theories, inflected words, such as *walked* would be generated by accessing the stored root, *walk*, and then applying a past tense rule that generates *walked* from the root. Similarly, a phrase like *I don't know* would be generated by accessing each of the individually stored words *I*, *don't*, and *know*.

On the opposite side of this theoretical spectrum, it's also possible that learners store everything, including entire sentences. @ambridgeStoredAbstractionsRadical2020 argued for exactly this, specifically arguing that everything a learner hears "is stored with its meaning, as understood in that individual situation" and that unwitnessed novel-forms are produced using on-the-fly analogy across stored exemplars @ambridgeStoredAbstractionsRadical2020. For example, producing a novel plural form, like *wugs*, would consist of analogizing (on-the-fly) over multiple stored exemplars (e.g., *cats*, *chairs*, *dogs*, etc).

It is also possible that what gets stored is somewhere in between these two extremes. For example, usage-based construction grammar approaches have posited that a lot more than just words are stored -- including high frequency phrases -- but rather than storing everything, or storing only the most basic units, instead humans store units of varying size depending on usage-based factors such as frequency [@bybee2003; @bybee2001; @goldbergConstructionsNewTheoretical2003; @morgan2024; @morganAbstractKnowledgeDirect2016; @morgan2015; @arnonMoreWordsFrequency2010; @baayenAmorphousModelMorphological2011; @odonnellProductivityReuseLanguage2016; @tomaselloConstructingLanguageUsagebased2005]. That is to say, the size of the units stored is driven by the statistical distribution of the language that the learner is producing and perceiving. For example, @bybee2003 drew an analogy to learning to play a piece on the piano:

> An important result of learning to play several pieces is that new pieces are then easier to master. Why is this? I hypothesize that the player can access bits of old stored pieces and incorporate them into new pieces. The part of a new piece that uses parts of a major scale is much easier to master if the player has practiced scales than is a part with a new melody that does not hearken back to common sequences. This means that snatches of motor sequences can be reused in new contexts. The more motor sequences stored, the greater ease with which the player can master a new piece [@bybee2003, p. 14-15].

\noindent In this same line of thinking, @bybee2003 further argued against a strictly traditional view, stating that learning the English past tense *-ed* requires learning a series of words that contain that segment (e.g., *played*, *spilled*, *talked*) and that these are not necessarily flushed from memory after learning the English past tense marker.

There is no shortage of evidence for the holistic storage of multi-word phrases. For example, high-frequency phrases, such as *I don't know*, have been shown to undergo phonetic reduction that isn't seen in other low or mid-frequency phrases containing *don't* [@bybeeEffectUsageDegrees1999] suggesting that the representation of *I don't know* is separate from the representation of each of the individual words. In other words, the susceptibility of high-frequency phrases to phonological change is strong evidence that they may come to have a mental representation for the whole expression (i.e., holistic storage). This example is not an outlier either, there are many examples of high-frequency phrases undergoing phonetic reduction: *going to*, want to*, have to*, etc [@bybee2003]. Additionally, in Korean, @yiEumun2002 demonstrated that in multi-word phrases containing the adnominal future marker (*-l*), the tensification of the consonant following the adnominal is predicted by the phrasal frequency. That is, in high-frequency phrases, consonants following the adnominal became tense at a higher rate than in low-frequency phrases.[^staub_rep_ext-1]

[^staub_rep_ext-1]: A similar effect has been demonstrated on the word-level as well in Korean, where epenthesis has been documented to occur more often in high-frequency words than in low-frequency words [@leeFrequencyEffectsMorphologisation2015]. This suggests that there may not be a clear division between the representation of high-frequency phrases and high-frequency words.

Evidence for holistic storage is not limited to phonological effects, either. In the Psycholinguistics literature, @siyanova-chanturiaSeeingPhraseTime2011 demonstrated that readers are sensitive to the ordering of binomials in English. In an eye-tracking experiment, participants read frequent binomial expressions in English in their preferred order (e.g., *bride and groom*) and their reversed order (*groom and bride*). They found that the preferred orderings were read faster. Further, @morganAbstractKnowledgeDirect2016 investigated whether the results from @siyanova-chanturiaSeeingPhraseTime2011 could be attributed to abstract knowledge of binomial orderings (e.g., a preference for male names before female names) or whether they were due to participants' direct experience with those items (e.g., hearing one ordering of a specific binomial more often than the complementary ordering). They developed a probabilistic model to approximate native English speakers' ordering preferences and combined that with a forced-choice and a self-paced reading task in order to investigate whether ordering preferences were driven by abstract knowledge or direct experience of the expression. They found that reading times for frequent binomials were influenced only by relative frequency (i.e., direct experience), not abstract knowledge. That is to say, ordering preferences of frequent binomials weren't explained by abstract ordering preferences, but rather by linguistic experience with the specific binomial, suggesting that high-frequency binomials are stored holistically.

Similarly, @odonnellProductivityReuseLanguage2016 tested 4 probabilistic models on their ability to learn the English past tense and derivational morphology. Specifically, they tested a Full-parsing model, which stores minimal-sized units only, a Full-listing model, which stores the entirety of units only, an Exemplar-based model, which stores all units and all sub-units consistent with the data, and finally a Productivity as an Inference model, which, similar to the Exemplar-based Inference model, can store both smaller and larger structures, but probabilistically determines which items to store based on the data. They found that the Inference-based model performed the best overall for both past tenses and derivational morphology. In other words, storing units of varying sizes (as opposed to just minimal or maximal-sized units) seems to be the most conducive to learning the various morphological paradigms in English.

Despite the clear evidence for the holistic storage of some multi-word units, however, it is still largely unclear what determines whether a unit is stored holistically. For example, it is possible that storage is driven by either **phrasal frequency** [@bybee2001] or by the mutual **predictability** of a phrase's component parts [i.e., how predictable the whole phrase is from part of the phrase\; @odonnellProductivityReuseLanguage2016]. For example, as previously stated, there is an abundance of evidence that high-frequency phrases are more susceptible to phonetic reduction than low-frequency phrases [@bybeeEffectUsageDegrees1999; @bybee2003]. Additionally, high-frequency phrases have been shown to lose the recognizability of their component parts relative to low-frequency phrases [@kapatsinskiFrequencyEmergencePrefabs2009]. For example, *up* is harder to recognize in *pick up* than in *run up*. On the other hand, in the learning literature, there is significant evidence that learning is driven by prediction error as opposed to raw co-occurrence statistics. For example, @ramscarChildrenValueInformativity2013 demonstrated that in word learning, children rely on more than simple co-occurrence statistics but also on how *informative* -- that is, how *predictive* -- a cue is of an outcome (relative to other cues). Specifically, they demonstrated that children rely on not only co-occurrence rate, but also background rate (how often a cue is present without an outcome). In other words, assuming doors have a higher co-occurrence rate and lower background rate than all the other competing cues (e.g., brown, house, room) for the word *door*, then children will learn that doors are the best predictor of the word *door* [@ramscarChildrenValueInformativity2013].

A similar debate persists in the speech perception literature, where @pierrehumbertExemplarDynamicsWord2001 argued that internal representations reflect the raw frequency distribution of the input. On the other hand, @olejarczukDistributionalLearningErrordriven2018 argued that the learning of phonemes is driven not by co-occurrence statistics (i.e., raw frequency), but rather by surprisal (i.e., prediction error). In other words, learners are actively predicting upcoming phonemes and update their beliefs in proportion to how surprising the upcoming phoneme is. Thus the debate between co-occurrence vs predictability in the role of learning is not unique to the word learning literature.

Additionally, if learners are storing more than just single-word units, what are the processing consequences of this? For example, as mentioned earlier, @kapatsinskiFrequencyEmergencePrefabs2009 investigated the recognition of the particle *up* in phrases of varying frequencies and found that the recognition of the particle *up* is significantly more difficult in a high-frequency phrases than in low frequency phrases, suggesting that high frequency units \`fuse' together, losing some of the recognizability of their individual parts.

On the other hand, @staubTimeCoursePlausibility2007 investigated the effects of plausibility on the reading times of familiar and novel compound nouns, which were compound nouns with high and low phrasal frequency respectively. Participants read sentences which contained a novel compound noun or a familiar compound noun (See the sentences below) in a plausible condition (a) or an implausible condition (b). Crucially, the second noun in the compound eliminated the local implausibility such that every sentence was plausible after reading the second noun. For example, in 1b *The zookeeper spread out the monkey...* is locally implausible, however upon reading the second noun in the compound, *medicine*, the local implausibility is eliminated.

\begin{enumerate} 

    \item \textbf{Novel Compound}
    \begin{enumerate}
        \item[\textbf{1a}] The zookeeper picked up the monkey medicine that was in the enclosure.
        \item[\textbf{1b}] The zookeeper spread out the monkey medicine that was in the enclosure.
    \end{enumerate} \label{staubsentencenovel}
    \item \textbf{Familiar Compound}
    \begin{enumerate}
        \item[\textbf{2a}] Jenny looked out on the huge mountain lion pacing in its cage. \label{familiarplaus}
        \item[\textbf{2b}] Jenny heard the huge mountain lion pacing in its cage. \label{familiarimplaus}
    \end{enumerate} \label{staubsentencefamiliar}
\end{enumerate}

\noindent They found that the size of the plausibility effect was the same for both novel and familiar compound nouns. That is to say, while familiar items were read more quickly than novel items, and there was an increase in reading times in the implausible condition, the size of the plausibility effect was not different for familiar items (relative to novel items). However, if familiar items are stored holistically, one might expect that readers would predict the second noun upon reading the first, thus eliminating the local implausibility. Thus, if these items are stored holistically it begs the question of what the processing consequences of storage are. On the other hand, it may just be that these items are not stored. For example, it is possible that, as has been previewed throughout the introduction, phrasal frequency may not be the driving factor of storage and that it is actually predictability that might be driving storage. If this is the case, then it is possible that the reason for a lack of an interaction effect in @staubTimeCoursePlausibility2007's results is due to their stimuli being low predictability compound nouns. For example, while *mountain lion* has a high phrasal frequency, *mountain* is not very predictable of *lion* (that is, the probability of *lion* following *mountain* is fairly low, despite the overall phrase having a relatively high frequency).

Thus there are two main problems that the present study aims to provide insight on: what exactly drives holistic storage, and what are the processing consequences of storage? In Experiment 1, we first replicate @staubTimeCoursePlausibility2007's experiment using a maze task [@boyceMazeMadeEasy2020]. In Experiment 2, we use the same methodology, but instead of using high (phrasal) frequency compound nouns, we use high *predictability* compound nouns (e.g., *peanut butter*). By using the highest predictability compound nouns from the google *n*-grams corpus \[@michel2011quantitativeanalysisculture, we ask whether the difference in reaction times between the locally implausible and plausible contexts differs depending on whether the compound noun is highly predictable or not. For example, if highly predictable compound nouns are stored holistically, it is possible that when listeners hear, or read, the first noun in a highly predictable compound noun they may access the second noun as well and/or a holistic compound noun representation. If this is the case, then locally implausible contexts should not incur as much processing difficulty when the compound noun is highly predictable because the second noun eliminates the local implausibility. Lastly in Experiments 3 and 4 we replicate these with eye-tracking.

## Experiment 1

### Methods

#### Participants

Participants were presented with sentences online via ibex farm, a web-based experiment software platform that is freely-available ([github.com/addrummond/ibex](github.com/addrummond/ibex)) and were recruited through the University of California Linguistics/Psychology Human Subjects Pool. To prevent selection bias, participants signed up for the experiment blindly, without knowledge of the content of the experiment. 146 participants were recruited, however 30 participants were excluded for having an overall accuracy below 70% (in this case, accuracy is operationalized as choosing the correct word; an inaccuracy would be choosing the ungrammatical distractor word), leaving a total of 116 participants. All participants self-reported being native English speakers.

#### Stimuli

The experimental sentences were sentences containing compound nouns from [@staubTimeCoursePlausibility2007] which varied upon two dimensions: local plausibility and familiarity. Locally plausible sentences were sentences in which the reading at the first noun was plausible and locally implausible sentences were sentences in which the reading at the first noun in the compound was implausible (see example sentences \ref{staubsentencenovel} and \ref{staubsentencefamiliar}). Local plausibility was a within-item effect and familiarity (the frequency of the compound noun) was a between-item effect. Examples \ref{staubsentencenovel} and \ref{staubsentencefamiliar} above exemplify each condition: the first sentence in each is locally plausible while the second one is locally implausible. For example, in sentence \hyperref[staubsentencefamiliar]{2a}, it is semantically plausible that *Jenny looked out on the huge mountain...* but not semantically plausible that *Jenny heard the huge mountain* (\hyperref[staubsentencefamiliar]{2b}). Altogether, our stimuli consisted of 24 novel items, 24 familiar items [taken from @staubTimeCoursePlausibility2007], and 188 filler sentences in order to avoid participants discerning the experimental design.

#### Procedure

Experiment 1 is a direct replication of @staubTimeCoursePlausibility2007 using the A-Maze task [@boyceMazeMadeEasy2020] instead of eye-tracking.[^staub_rep_ext-2] In the A-maze task, participants are presented with the first word in the sentence and then have to correctly choose between an ungrammatical distractor word and the next word in the sentence. When participants select the correct word, they continue to the next word in the sentence until the sentence is finished. The distractor words for the A-maze were generated automatically following @boyceMazeMadeEasy2020 using the Gulordava model @gulordava2018colorlessgreenrecurrent. The locations of the distractor word and target word were counterbalanced so that they appeared an equal number of times on the left and right side of the screen. For each word, the reaction time was recorded along with whether the subject chose the correct item or not. See @fig-mazevisualization for a visualization of the maze task, reproduced from @boyceMazeMadeEasy2020.

[^staub_rep_ext-2]: The maze task was used due to the limitations of the COVID-19 pandemic.

```{r echo = F, fig.align = 'center', warning = F, message = F, out.width = '50%'}
#| label: fig-mazevisualization
#| fig-cap: "A visualization of the maze task, reproduced from @boyceMazeMadeEasy2020."
#| fig-align: center
knitr::include_graphics("Figures/mazevisualization.pdf")

```

Sentences were presented in a random order and each word was presented an equal number of times on the left and right side of the screen. Additionally, each item appeared an equal number of times in the implausible and plausible context and no participant was presented with the same item in more than one condition. The complete dataset included 9994 response tokens.

#### Analysis

The data was analyzed using Bayesian linear regression models, as implemented in the *brms* package [@burknerBrmsPackageBayesian2017] within the R programming environment @Rpackage. We subsetted the data into two sets based on the region: one set for the first noun in the compound noun and one set for the second noun in the compound. The primary dependent variable was log reaction time for both of these regions [following @boyceMazeMadeEasy2020]. The primary independent variables were plausibility and familiarity [following @staubTimeCoursePlausibility2007]. We modeled reaction time as a function of plausibility and familiarity, including their interaction, with maximal random effects [@barrRandomEffectsStructure2013]. The formula used for the model is presented in equation @eq-modelfamil below, with *Plaus* as plausibility and *Famil* as familiarity. All models were sum-coded $$
Reaction Time \sim Plaus*Famil+(Plaus*Famil|Subject)+(Plaus|Item)
$$ {#eq-modelfamil}

### Results

As mentioned in the methods section, for the purpose of the analysis, the data was divided into two regions: the N1 region and the N2 region, which were the first and second noun in the compound noun respectively. The results of the Bayesian regression model for the N1 region are presented in @tbl-N1Staub and in @fig-N1Staub, and the results of the N2 region are presented in @tbl-N2Staub and in @fig-N2Staub.

For the N1 region, there was an increase in reaction time for the implausible condition relative to the plausible condition. There was no such effect for familiarity. In other words, while participants took longer selecting the correct word in the implausible condition, their reaction times were not affected by the familiarity of the compound noun. This is expected given that the familiarity condition was not the frequency of the first noun, but rather the frequency of the compound noun as a whole. Additionally, there was no interaction effect between plausibility and familiarity.

At the N2 region, there was an increase in reaction time in the plausible condition and a decrease in reaction time in the familiar condition, but no interaction effect. In other words, participants were slower to choose the correct word in the plausible condition. They were also quicker to choose the correct word if the compound noun was familiar. However, plausibility did not mediate the effects of familiarity. That is to say, the size of the plausibility effect was not different for familiar versus novel compound nouns. Following @wagenmakers2010bayesianhypothesistesting, a post-hoc Bayes factor analysis was conducted to compare the interaction effect to the null hypothesis (interaction effect = 0). We found a Bayes Factor value of 18.15 which constitutes strong support for the null hypothesis.

```{r, echo = F, message = F}
#| label: tbl-N1Staub
#| tbl-cap: "Model results examining the effect of plausibility and frequency for the N1 region."



percent_greater_zero = data.frame(fixef(maze_staub_n1, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_staub_n1)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Familiarity", "Plausibility:Familiarity"), times = 1))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')


fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(n1_all_measures$measure)

fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')

```

```{r, echo = F, message = F}
#| label: tbl-N2Staub
#| tbl-cap: "Model results examining the effect of plausibility and frequency for the N2 region."



percent_greater_zero = data.frame(fixef(maze_staub_n2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_staub_n2)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Familiarity", "Plausibility:Familiarity"), times = 1))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')


fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(n1_all_measures$measure)

fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N1Staub
#| fig-cap: "Plot of log reaction time at the N1 region as a function of plausibility and familiarity."


model_n1_binary_plot_staub = plot(conditional_effects(maze_staub_n1), plot = F)[[3]] +
  theme_bw()

model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") +
  scale_x_discrete(labels = c("implausible" = "Implausible", "plausible" = "Plausible")) +
  ylab("Log Reaction Time") +
  xlab("Plausibility") +
  theme_bw() +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14)
  )


model_n1_binary_plot_staub

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N2Staub
#| fig-cap: "Plot of log reaction time at the N2 region as a function of plausibility and familiarity."


model_n1_binary_plot_staub = plot(conditional_effects(maze_staub_n2), plot = F)[[3]] +
  theme_bw()



model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") +
  scale_x_discrete(labels = c("implausible" = "Implausible", "plausible" = "Plausible")) +
  ylab("Log Reaction Time") +
  xlab("Plausibility") +
  theme_bw() +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14)
  )

model_n1_binary_plot_staub


```

### Discussion

Our results directly replicate @staubTimeCoursePlausibility2007 using the Maze task, demonstrating the viability of this method for the tasks at hand. For the N1 region, while there was a clear increase in reaction time for items in the implausible condition, there was no interaction effect between plausibility and familiarity. In other words, the effect of plausibility was the same for both familiar and novel compound nouns. If familiar compound nouns are stored holistically, however, it is possible that we would see less of a (im)plausibility effect relative to novel items, because readers might be predicting the second noun in the compound upon reading the first noun. Recall the earlier example, reproduced below for convenience:

\begin{enumerate}
   \item \textbf{Novel Compound}
    \begin{enumerate}
        \item[\textbf{1a}] The zookeeper picked up the monkey medicine that was in the enclosure.
        \item[\textbf{1b}] The zookeeper spread out the monkey medicine that was in the enclosure.
    \end{enumerate} \label{staubsentencenovel}
    \item \textbf{Familiar Compound}
    \begin{enumerate}
        \item[\textbf{2a}] Jenny looked out on the huge mountain lion pacing in its cage. \label{familiarplaus}
        \item[\textbf{2b}] Jenny heard the huge mountain lion pacing in its cage. \label{familiarimplaus}
    \end{enumerate} \label{figanext}
\end{enumerate}

\noindent It is possible that if *mountain lion* was stored holistically, then upon reading *Jenny heard the huge mountain...*, the reader might have less difficulty with the local implausibility (relative to a low-frequency compound noun) because they would predict *lion*, which would eliminate the implausibility (*heard the mountain lion* is not implausible). However, we do not see this. Instead, the effect of plausibility is similar for both familiar and novel items. One possible explanation for these results is that the familiar phrases are not necessarily stored. Instead storage might be driven by predictability. If this is the case then it would explain why we do not see this effect in @staubTimeCoursePlausibility2007 or in Experiment 1, especially since all of the items used in @staubTimeCoursePlausibility2007 are low predictability compound nouns.

At the N2 region, the decrease in reaction time for familiarity is not surprising given that familiarity, as previously mentioned, was based on the frequency of the compound noun as a whole, however the increase in reaction time for the plausible condition is interesting, especially since the sentences were only locally implausible on the N1 region: the second noun in the compound always eliminated the local implausibility. It is possible this increase in reaction time is a garden path effect for committing to an interpretation of the sentence with the N1 and having to reanalyze the sentence. For example, when reading *Jenny looked upon the huge mountain...*, after reading *lion*, the reader may need to reanalyze the sentence, as the subject is not looking upon a mountain at all, but rather they are looking at a *mountain lion*. However, in the implausible condition participants may not fully commit to the interpretation since it is locally implausible, and thus may be waiting for a choice that eliminates the implausibility, thus explaining the absence of a similar slowdown in the implausible condition.

In Experiment 3, we examine whether readers can overcome this local implausibility for high-predictability items.

## Experiment 2

### Methods

#### Participants

Participant recruitment was identical to Experiment 1. 105 participants were recruited, and 19 participants were excluded for being below 70% accuracy, leaving a total of 86 participants. All participants self-reported being native English speakers.

#### Stimuli

We operationalized predictability through the odds ratio of the compound noun to the first word when that word is not followed by the second word in the compound noun which is exemplified in @eq-oddsratio.

$$
\frac{\mathrm{count(\textit{peanut butter})}}{\mathrm{count(\textit{peanut})} - \mathrm{count(\textit{peanut butter})}} 
$$ {#eq-oddsratio}

In non-mathematical terms, @eq-oddsratio quantifies how predictable the first noun is of the second noun (i.e., how likely the second noun is to follow after the first noun, relative to every other word that could follow). For example, the odds ratio of *peanut butter* would be the odds ratio of the compound noun -- *peanut butter* -- to the first noun -- *peanut* -- when *butter* does not follow it.

In order to collect the most predictable compound nouns, we searched the Google *n*-grams corpus [@michel2011quantitativeanalysisculture] using the ZS Python package [@smithZSFileFormat2014]. We then collected the compound nouns with the highest predictability values, using the following exclusion criteria: excluding words with a match count below 90,000,[^staub_rep_ext-3] excluding nonsense words, proper nouns, technical words (e.g., *tenth circuit*), and words in which we could not create locally plausible and implausible sentences.[^staub_rep_ext-4] We gathered a total of 37 compound nouns for our high predictability condition. We subsequently normed the sentences we created using the high predictability compounds, as well as the sentences from @staubTimeCoursePlausibility2007 which we confirmed were all low predictability compounds relative to our compound nouns.

[^staub_rep_ext-3]: This was done in order to help filter out nonsense words (e.g., *teawhit head*) as well as eliminate words that had high predictability scores but were just a product of the corpus and unlikely to reflect the input of human learners (e.g., *broomwheat tea* which has a predictability score of 287 in the corpus).

[^staub_rep_ext-4]: Given our methodology, we needed to be able to make sentences that were plausible and implausible using the same compound noun. This restriction meant we had to exclude words like *Parmesan cheese* where it would be impossible for the reading at the N1 region to be implausible without the reading of the compound noun also being implausible.

We followed the same methodology as @staubTimeCoursePlausibility2007 for our norming procedure: we provided participants with each item in four conditions (see below) and asked participants to rate each sentence on a 7-point Likert scale in terms of how well the last word fit in the sentence. No participant rated more than one version of each sentence. Crucially, for each item we ensured that sentence 3c received a lower rating than the other 3 versions of the sentences.

\begin{enumerate} \setcounter{enumi}{2}
   \item \textbf{Norming Conditions}
    \begin{enumerate}
        \item[\textbf{3a}] Jimmy picked up the peanut (plausible, through the first noun).
        \item[\textbf{3b}] Jimmy picked up the peanut butter (plausible, through the second noun).
        \item[\textbf{3c}] Jimmy spread out the peanut (implausible, through the first noun).
        \item[\textbf{3d}] Jimmy spread out the peanut butter (implausible, through the second noun).
    \end{enumerate} \label{figanext}
\end{enumerate}

Finally, we excluded items in which the implausible sentence through the first noun was rated more or similarly well to the other conditions (i.e., the plausible sentence through the first noun, the plausible sentence through the second noun, and the implausible sentence through the second noun). It is important to note that due to our experimental design, the implausible sentence through the second noun is technically plausible at the second noun, because the second noun eliminates the local implausibility. Thus this condition should also receive a high rating, despite being the implausible condition. The mean values for each condition are as follows: plausible, through the first noun: 5.58 (sd = 0.78); plausible, through the second noun: 5.41 (sd = 0.71); implausible, through the first noun: 3.13 (0.63); implausible, through the second noun: 5.47 (sd = 0.82).

After norming, we selected sentences such that the difference in plausibility values between the plausible and implausible conditions were roughly the same for the high predictability and low predictability conditions. This was done to avoid conflating an interaction effect between predictability and plausibility with an item-specific effect. That is, if the plausibility effect was smaller for high predictability sentences relative to the low predictability sentences, then it would be impossible to tell if the interaction effect between predictability and plausibility is meaningful or just a product of our stimuli. The mean plausibility difference for the low predictability items was 2.47 and the mean plausibility difference for the high predictability items was 2.48. We confirmed that there was not a significant difference in plausibility values through a t-test (t = 0.0446, df = 39, p = 0.52). After accounting for this, we ended up with 21 high predictability and 21 low predictability items [which were taken from @staubTimeCoursePlausibility2007], for a total of 42 items. Lastly, in order to avoid participants discerning the experimental design we also included 188 filler items.

#### Procedure

Following Experiment 1, we used the A-maze task [@boyceMazeMadeEasy2020] with automatically-generated distractor items [@gulordava2018colorlessgreenrecurrent]. Our dependent variable was reaction time and our independent variables were plausibility and predictability. We again used ibex farm to run our maze task. Sentences were presented in a random order and each word was presented an equal amount of times on the left and right side of the screen. Additionally, each item appeared an equal number of times in the implausible and plausible context and no participant was presented with the same item in more than one condition.

#### Analysis

The data was analyzed using Bayesian linear regression models, as implemented in the *brms* package [@burknerBrmsPackageBayesian2017] within the R programming environment [@Rpackage]. We subsetted the data into two sets based on the region: one set for the first noun in the compound noun and one set for the second noun in the compound. The primary dependent variable was log reaction time for both of these regions [following @boyceMazeMadeEasy2020]. The independent variables were plausibility and predictability. Reaction time was modeled as a function of plausibility and predictability, along with their interaction, with maximal random effects [@barrRandomEffectsStructure2013]. The formula used for the model is presented in @eq-model2 below, with *Plaus* as plausibility and *Predic* as predictability.

$$
Reaction Time\sim Plaus*Predict+(Plaus*Predict|Subject)+(Plaus|Item) 
$$ {#eq-model2}

### Results

As mentioned in the methods section, for the purpose of the analysis, the data was divided into two regions: the N1 region and the N2 region, which were the first and second noun in the compound noun respectively. The results of the Bayesian regression models for the N1 region are presented in @tbl-N1Predictability and @tbl-N1LogOdds, and visualized in @fig-N1Predictability and @fig-N1LogOdds. The results of the N2 region are presented in @tbl-N2Predictability and @tbl-N2LogOdds, and visualized in @fig-N2Predictability and @fig-N2LogOdds.

With regards to the N1 region, @tbl-N1Predictability presents the results of the analysis we ran with predictability as a binary predictor (high or low), while @tbl-N1LogOdds presents the results of the analysis we ran with predictability as a continuous predictor (operationalized as the log odds ratio). Our results demonstrate that, similar to experiment 1, there was an increase in reaction time for the implausible condition, but no effect for predictability or the interaction between the two.

With regards to the N2 region, @tbl-N2Predictability presents the results of the analysis we ran with predictability as a binary predictor (high or low), while @tbl-N2LogOdds presents the results of the analysis we ran with predictability as a continuous predictor (operationalized as the log odds ratio). Our results, as in Experiment 1, demonstrate an increase in reaction time in the plausible condition and and a decrease in reaction time in the high-predictability condition, but no interaction effect between plausibility and predictability. As in Experiment 1, we once again conducted a post-hoc Bayes factor analysis to compare the interaction effect to the null hypothesis (interaction effect = 0). We found a Bayes Factor value of 15.67 which constitutes strong support for the null hypothesis.

@fig-N1Predictability and @fig-N2Predictability provide visualizations of the analyses run with predictability as a binary variable while @fig-N1LogOdds and @fig-N2LogOdds present analyses with predictability as a continuous variable.

```{r, echo = F, message = F}
#| label: tbl-N1Predictability
#| tbl-cap: "Regression analysis results for the N1 region with predictability as a binary predictor (high or low)."



percent_greater_zero = data.frame(fixef(maze_pred_n1_v1, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'Predictability1', 'plausibility1:Predictability1')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_pred_n1_v1)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Predictability", "Plausibility:Predictability"), times = 1))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')


fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')



```

```{r, echo = F, message = F}
#| label: tbl-N1LogOdds
#| tbl-cap: "Regression analysis results for the N1 region with predictability as a continuous predictor (log odds ratio)."



percent_greater_zero = data.frame(fixef(maze_pred_n1_v2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'logoddsratio', 'plausibility1:logoddsratio')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_pred_n1_v2)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "Plausibility", "LogOdds", "Plausibility:LogOdds"), times = 1))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'LogOdds', 'Plausibility:LogOdds')

fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')




```

```{r, echo = F, message = F}
#| label: tbl-N2Predictability
#| tbl-cap: "Regression analysis results for the N2 region with predictability as a binary predictor (high or low)."



percent_greater_zero = data.frame(fixef(maze_pred_n2_v1, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'Predictability1', 'plausibility1:Predictability1')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_pred_n2_v1)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability'))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')

fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')



```

```{r, echo = F, message = F}
#| label: tbl-N2LogOdds
#| tbl-cap: "Regression analysis results for the N2 region with predictability as a continuous predictor (log odds ratio)."



percent_greater_zero = data.frame(fixef(maze_pred_n2_v2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'plausibility1', 'LogOddsRatio', 'plausibility1:LogOddsRatio')))

#table 1
fixefsm1 = as.data.frame(fixef(maze_pred_n2_v2)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = c('Intercept', 'Plausibility', 'LogOdds', 'Plausibility:LogOdds'))

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'LogOdds', 'Plausibility:LogOdds')

fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')



```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N1Predictability
#| fig-cap: "Plot of the N1 region with predictability as a binary variable (high or low)."


model_n1_binary_plot_staub = plot(conditional_effects(maze_pred_n1_v1), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Log Reaction Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 14)) 

model_n1_binary_plot_staub

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N1LogOdds
#| fig-cap: "Plot of the N1 region with predictability as a continuous variable."


model_n1_binary_plot_staub = plot(conditional_effects(maze_pred_n1_v2), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Plausibility", fill = "Plausibility") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Log Reaction Time') +
  xlab('Log Odds Ratio') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 14)) 

model_n1_binary_plot_staub

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N2Predictability
#| fig-cap: "Plot of the N2 region with predictability as a binary variable (high or low)."


model_n1_binary_plot_staub = plot(conditional_effects(maze_pred_n2_v1), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Predictability", fill = "Predictability") +
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Log Reaction Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 14)) 

model_n1_binary_plot_staub

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-N2LogOdds
#| fig-cap: "Plot of the N2 region with predictability as a continuous variable."


model_n1_binary_plot_staub = plot(conditional_effects(maze_pred_n2_v2), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Plausibility", fill = "Plausibility") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Log Reaction Time') +
  xlab('Log Odds Ratio') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 14)) 

model_n1_binary_plot_staub

```

### Discussion

Experiment 2 replicates and extends Experiment 1 using predictability instead of familiarity (i.e., phrasal frequency). Interestingly, the results of Experiment 2 were extremely similar to the results of Experiment 1: There was no interaction effect between predictability and plausibility on the RTs for the N1 condition. Additionally, while we see an effect of implausibility on the N1 region, we don't see an effect of predictability. This is expected since predictability is defined as the odds that the N2 appears given the N1, so we should see this effect on the N2 region, not the N1 region.

The results of the N2 region also bear remarkable similarities to our results in Experiment 1: There was a plausibility and predictability effect, but no interaction between the two. Specifically, there was an *increase* in reaction time for items in the plausible condition relative to the implausible condition. It is possible that, as mentioned in the discussion section of Experiment 1, this increase in reaction time is a garden path effect for committing to an interpretation of the sentence with the N1 and having to reanalyze the sentence.

## Experiment 3

In Experiment 3, we directly replicate Experiment 1 using eye-tracking.

### Methods

#### Participants

46 native English speakers were recruited from the University of California, Davis subjects pool. They were given course credit in exchange for their participation. All participants had normal or corrected vision.

#### Materials

The materials were identical to Experiment 1.

We recorded participants' eye movements using the Eyelink 1000 Plus. We recorded pupil movements from the right eye. Participants were seated 850mm away from the screen. Our screen resolution was 1920x1080, 531.3mm in width, and 298.8mm in height.

Comprehension was checked for non-experimental trials and participants below 80% accuracy were excluded. Out of our 46 participants, 2 were excluded for falling below the accuracy threshold.

#### Analyses

Prior to our analyses, sentences with blinks were excluded and fixations less than 80ms in duration and within one character of the nearest fixation were merged into that fixation [following @staubTimeCoursePlausibility2007]. For our regions of interest (the first noun and the second noun in the compound noun), we computed first fixation duration, first pass time, go-past time, and first-pass regression.

For each analysis, our independent variables were plausibility (high or low) and familiarity (high or low) and their interaction. We also included random slopes for condition and predictability by subject and plausibility by compound noun as well as intercepts for subject and compound noun. For each of our models, we used sum-coding, where the intercept represents the grand mean and the fixed-effect coefficient estimates represent the distance from the grand mean.

### Results

#### N1 Region

Our results at the N1 region are demonstrated in @tbl-fullmodelresultsn1staub and visualized in @fig-fullmodelresultsn1staub.

At the N1 region, we find main-effects of plausibility for first fixation duration, go-past time, and first-pass regression, but not for gaze times. Additionally, we find no effects of familiarity. Finally, for go-past times we find an interaction between plausibility and predictability such that the slowdown of the implausible context was greater for familiar items than novel items.


```{r, echo = F, message = F}
#| label: tbl-fullmodelresultsn1staub
#| tbl-cap: "Model results for each eye-tracking measure at the N1 region."



percent_greater_zero = data.frame(fixef(model_n1_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm1 = as.data.frame(fixef(model_n1_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First Fixation Duration')

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n1_gaze_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))


fixefsm2 = as.data.frame(fixef(model_n1_gaze_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'Gaze/First-Pass Duration')

rownames(fixefsm2) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n1_regression_path_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))


fixefsm3 = as.data.frame(fixef(model_n1_regression_path_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure='Go-Past Time')

rownames(fixefsm3) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n1_regression_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm4 = as.data.frame(fixef(model_n1_regression_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First-Pass Regression')

rownames(fixefsm4) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')

n1_all_measures = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Familiarity", "Plausibility:Familiarity"), times = 4))


n1_all_measures = n1_all_measures %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

groupings=rle(n1_all_measures$measure)

n1_all_measures %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, fig.align = 'center', warning = F, message = F}
#| label: fig-fullmodelresultsn1staub
#| fig-cap: "Visualization of the effects of plausibility and familiarity on each eye-tracking measure at the N1 region."
#| fig-width: 7
#| fig-height: 7


model_n1_binary_plot_staub = plot(conditional_effects(model_n1_staub), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot_staub = model_n1_binary_plot_staub +
  labs(color = "Familiarity", fill = "Familiarity") + 
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  ylab('First Fixation Duration') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n1_binary_gaze_plot_staub = plot(conditional_effects(model_n1_gaze_staub), plot = F)[[3]] +
  theme_bw()


model_n1_binary_gaze_plot_staub = model_n1_binary_gaze_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('Gaze/First-Pass Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n1_go_past_plot_staub = plot(conditional_effects(model_n1_regression_path_staub), plot = F)[[3]] +
  theme_bw()


model_n1_go_past_plot_staub = model_n1_go_past_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('Go-Past Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 

model_n1_binary_first_pass_plot_staub = plot(conditional_effects(model_n1_regression_staub), plot = F)[[3]] +
  theme_bw()


model_n1_binary_first_pass_plot_staub = model_n1_binary_first_pass_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('First-Pass Regression') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_n1_binary_plot_staub, model_n1_binary_gaze_plot_staub, model_n1_go_past_plot_staub, model_n1_binary_first_pass_plot_staub, nrow = 2, ncol = 2, common.legend=T, legend = 'top')

```

#### N2 Region


Our results at the N2 region are demonstrated in @tbl-fullmodelresultsn2staub and visualized in @fig-fullmodelresultsn2staub.

At the N2 region, we find main-effects of familiarity for first fixation duration and gaze times, and marginal effects for go-past times and first-pass regression. We find no main-effect of plausibility, which is expected because the implausible condition is plausible at the N2 region. We also find no interaction effect between plausibility and familiarity.



```{r, echo = F, message = F}
#| label: tbl-fullmodelresultsn2staub
#| tbl-cap: "Results of models for each eye-tracking measure at the N2 region."



percent_greater_zero = data.frame(fixef(model_n2_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm1 = as.data.frame(fixef(model_n2_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First Fixation Duration')

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n2_gaze_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))


fixefsm2 = as.data.frame(fixef(model_n2_gaze_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'Gaze/First-Pass Duration')

rownames(fixefsm2) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n2_regression_path_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))


fixefsm3 = as.data.frame(fixef(model_n2_regression_path_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure='Go-Past Time')

rownames(fixefsm3) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n2_regression_staub, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'frequency1', 'condition1:frequency1')))

#table 1
fixefsm4 = as.data.frame(fixef(model_n2_regression_staub)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First-Pass Regression')

rownames(fixefsm4) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')

n2_all_measures = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Familiarity", "Plausibility:Familiarity"), times = 4))


n2_all_measures = n2_all_measures %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

groupings=rle(n2_all_measures$measure)

n2_all_measures %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, fig.align = 'center', warning = F, message = F}
#| label: fig-fullmodelresultsn2staub
#| fig-cap: "Visualization of the effects of plausibility and familiarity on each eye-tracking measure at the N2 region."
#| fig-width: 7
#| fig-height: 7


model_n2_binary_plot_staub = plot(conditional_effects(model_n2_staub), plot = F)[[3]] +
  theme_bw()


model_n2_binary_plot_staub = model_n2_binary_plot_staub +
  labs(color = "Familiarity", fill = "Familiarity") + 
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  ylab('First Fixation Duration') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n2_binary_gaze_plot_staub = plot(conditional_effects(model_n2_gaze_staub), plot = F)[[3]] +
  theme_bw()


model_n2_binary_gaze_plot_staub = model_n2_binary_gaze_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('Gaze/First-Pass Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n2_go_past_plot_staub = plot(conditional_effects(model_n2_regression_path_staub), plot = F)[[3]] +
  theme_bw()


model_n2_go_past_plot_staub = model_n2_go_past_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('Go-Past Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 

model_n2_binary_first_pass_plot_staub = plot(conditional_effects(model_n2_regression_staub), plot = F)[[3]] +
  theme_bw()


model_n2_binary_first_pass_plot_staub = model_n1_binary_first_pass_plot_staub +
  scale_color_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("Familiar", "Novel"), type = c('#298c8c', '#800074')) +
  labs(color = "Familiarity", fill = "Familiarity") + 
  ylab('First-Pass Regression') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_n2_binary_plot_staub, model_n2_binary_gaze_plot_staub, model_n2_go_past_plot_staub, model_n2_binary_first_pass_plot_staub, nrow = 2, ncol = 2, common.legend=T, legend = 'top')

```



### Discussion

Experiment 3 demonstrates that readers have longer first-fixation times, longer go-past times, and more first-pass regressions in implausible contexts than in plausible contexts. Further, we find an interaction effect in the opposite direction from predicted for go-past times: the effect of plausibility is greater for familiar items than in novel items. We will expand on this result in the Conclusion section.

## Experiment 4

In Experiment 4, we replicate the results we found in Experiment 2 using eye-tracking.

### Methods

#### Participants

56 native English speakers were recruited from the University of California, Davis subjects pool. They were given course credit in exchange for their participation. All participants had normal or corrected vision.

#### Materials

The materials here were identical to those in Experiment 2.

#### Procedure

We recorded participants' eye movements using the Eyelink 1000 Plus. We recorded pupil movements from the right eye. Participants were seated 850mm away from the screen. Our screen resolution was 1920x1080, 531.3mm in width, and 298.8mm in height.

Comprehension was checked for non-experimental trials and participants below 80% accuracy were excluded. Out of our 56 participants, 0 were excluded for falling below the accuracy threshold.

#### Analyses

Prior to our analyses, sentences with blinks were excluded and fixations less than 80ms in duration and within one character of the nearest fixation were merged into that fixation [following @staubTimeCoursePlausibility2007]. For our regions of interest (the first noun and the second noun in the compound noun), we computed first fixation duration, first pass time, go-past time, and first-pass regression.

For each analysis, our independent variables were plausibility (high or low) and (log) predictability (high or low) and their interaction. We also included random slopes for condition and predictability by subject and plausibility by compound noun as well as intercepts for subject and compound noun. For each of our models, we used sum-coding, where the intercept represents the grand mean and the fixed-effect coefficient estimates represent the distance from the grand mean.

### Results


#### N1 Region

Our results at the N1 region are demonstrated in @tbl-fullmodelresultsn1 and visualized in @fig-fullmodelresultsn1.

At the N1 region, we find main-effects of plausibility for only first-pass regression. We find no effect of plausibility for first fixation duration, gaze time, and go-past time. Additionally, we find no effects of predictability. Finally, we find an interaction effect between plausibility and predictability for first fixation duration and first-pass regression. These interaction effects are in the opposite direction such that the slowdown caused by the implausible condition was greater for high-predictability items relative to low-predictability items in first-pass regression, but smaller for high-predictability items relative to low-predictability items in first fixation duration.

```{r, echo = F, message = F}
#| label: tbl-fullmodelresultsn1
#| tbl-cap: "Model results for each eye-tracking measure at the N1 region."



percent_greater_zero = data.frame(fixef(model_n1_binary, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))

#table 1
fixefsm1 = as.data.frame(fixef(model_n1_binary)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First Fixation Duration')

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n1_binary_gaze, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))


fixefsm2 = as.data.frame(fixef(model_n1_binary_gaze)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'Gaze/First-Pass Duration')

rownames(fixefsm2) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')



percent_greater_zero = data.frame(fixef(model_n1_binary_go_past, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))


fixefsm3 = as.data.frame(fixef(model_n1_binary_go_past)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure='Go-Past Time')

rownames(fixefsm3) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')



percent_greater_zero = data.frame(fixef(model_n1_binary_first_pass, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))

#table 1
fixefsm4 = as.data.frame(fixef(model_n1_binary_first_pass)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First-Pass Regression')

rownames(fixefsm4) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')

n1_all_measures_exp4 = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Predictability", "Plausibility:Predictability"), times = 4))

n1_all_measures_exp4 = n1_all_measures_exp4 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))



groupings=rle(n1_all_measures_exp4$measure)

n1_all_measures_exp4 %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, out.width = '100%', fig.align = 'center', warning = F, message = F}
#| label: fig-fullmodelresultsn1
#| fig-cap: "Visualization of the effects of plausibility and predictability on each eye-tracking measure at the N1 region."
#| fig-width: 7
#| fig-height: 7


model_n1_binary_plot = plot(conditional_effects(model_n1_binary), plot = F)[[3]] +
  theme_bw()


model_n1_binary_plot = model_n1_binary_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('First Fixation Duration') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n1_binary_gaze_plot = plot(conditional_effects(model_n1_binary_gaze), plot = F)[[3]] +
  theme_bw()


model_n1_binary_gaze_plot = model_n1_binary_gaze_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Gaze/First-Pass Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 

model_n1_go_past_plot = plot(conditional_effects(model_n1_binary_go_past), plot = F)[[3]] +
  theme_bw()


model_n1_go_past_plot = model_n1_go_past_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Go-Past Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n1_binary_first_pass_plot = plot(conditional_effects(model_n1_binary_first_pass), plot = F)[[3]] +
  theme_bw()


model_n1_binary_first_pass_plot = model_n1_binary_first_pass_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('First-Pass Regression') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_n1_binary_plot, model_n1_binary_gaze_plot, model_n1_go_past_plot, model_n1_binary_first_pass_plot, nrow = 2, ncol = 2, common.legend=T, legend = 'top')

```

#### N2 Region


Our results at the N2 region are demonstrated in @tbl-fullmodelresultsn2 and visualized in @fig-fullmodelresultsn2.

At the N2 region, we find a main-effect of predictability for only the first fixation duration measure. We find no effect of plausibility, as expected because the N2 region eliminates the implausibility effect. We also find no interaction between plausibility and predictability in any of the measures.

```{r, echo = F, message = F}
#| label: tbl-fullmodelresultsn2
#| tbl-cap: "Model results for each eye-tracking measure at the N2 region."



percent_greater_zero = data.frame(fixef(model_n2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))

#table 1
fixefsm1 = as.data.frame(fixef(model_n2)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First Fixation Duration')

rownames(fixefsm1) = c('Intercept', 'Plausibility', 'Familiarity', 'Plausibility:Familiarity')



percent_greater_zero = data.frame(fixef(model_n2_binary_gaze, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))


fixefsm2 = as.data.frame(fixef(model_n2_binary_gaze)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'Gaze/First-Pass Duration')

rownames(fixefsm2) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')



percent_greater_zero = data.frame(fixef(model_n2_binary_go_past, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))


fixefsm3 = as.data.frame(fixef(model_n2_binary_go_past)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure='Go-Past Time')

rownames(fixefsm3) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')



percent_greater_zero = data.frame(fixef(model_n2_binary_first_pass, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'predictability_binary1', 'condition1:predictability_binary1')))

#table 1
fixefsm4 = as.data.frame(fixef(model_n2_binary_first_pass)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = 'First-Pass Regression')

rownames(fixefsm4) = c('Intercept', 'Plausibility', 'Predictability', 'Plausibility:Predictability')

n2_all_measures_exp4 = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Plausibility", "Predictability", "Plausibility:Predictability"), times = 4))

n2_all_measures_exp4 = n2_all_measures_exp4 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


groupings=rle(n2_all_measures_exp4$measure)

n2_all_measures_exp4 %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, out.width = '100%', fig.align = 'center', warning = F, message = F}
#| label: fig-fullmodelresultsn2
#| fig-cap: "Visualization of the effects of plausibility and predictability on each eye-tracking measure at the N2 region."
#| fig-width: 7
#| fig-height: 7


model_n2_binary_plot = plot(conditional_effects(model_n2), plot = F)[[3]] +
  theme_bw()


model_n2_binary_plot = model_n2_binary_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('First Fixation Duration') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n2_binary_gaze_plot = plot(conditional_effects(model_n2_binary_gaze), plot = F)[[3]] +
  theme_bw()


model_n2_binary_gaze_plot = model_n2_binary_gaze_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Gaze/First-Pass Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 

model_n2_go_past_plot = plot(conditional_effects(model_n2_binary_go_past), plot = F)[[3]] +
  theme_bw()


model_n2_go_past_plot = model_n2_go_past_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('Go-Past Time') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_n2_binary_first_pass_plot = plot(conditional_effects(model_n2_binary_first_pass), plot = F)[[3]] +
  theme_bw()


model_n2_binary_first_pass_plot = model_n2_binary_first_pass_plot +
  labs(color = "Predictability", fill = "Predictability") + 
  scale_color_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  scale_fill_discrete(labels = c("High", "Low"), type = c('#298c8c', '#800074')) +
  ylab('First-Pass Regression') +
  xlab('Plausibility') +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_n2_binary_plot, model_n2_binary_gaze_plot, model_n2_go_past_plot, model_n2_binary_first_pass_plot, nrow = 2, ncol = 2, common.legend=T, legend = 'top')

```


#### Filler Items

In addition to our experimental stimuli, we also included sentences that varied with respect to the frequency of the word. For example, in the below sentences, *satchel* is low-frequency and *account* is high-frequency. Thus, in order to confirm that the results in Experiment 4 were not due to measurement error, we examined the effect of frequency in our filler items on each of the eye-tracking measures.

\begin{enumerate} 

    \item Take your money out of the satchel and pay off the debt.
    \item Take your money out of the account and pay off the debt.
    
\end{enumerate}

Our results are presented in @tbl-filleritemsall and visualized in @fig-filleritemsall. We find an effect of frequency in each of the four eye-tracking measures we looked at.



```{r, echo = F, message = F}
#| label: tbl-filleritemsall
#| tbl-cap: "Model results for filler items for each eye-tracking measure."



percent_greater_zero = data.frame(fixef(first_fixation_filler, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'filler_higher_freq1')))

#table 1
fixefsm1 = as.data.frame(fixef(first_fixation_filler)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = "First Fixation Duration")

rownames(fixefsm1) = c('Intercept', 'Frequency')



percent_greater_zero = data.frame(fixef(gaze_filler, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'filler_higher_freq1')))

#table 1
fixefsm2 = as.data.frame(fixef(gaze_filler)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = "Gaze/First-Pass Duration")

rownames(fixefsm2) = c('Intercept', 'Frequency')



percent_greater_zero = data.frame(fixef(go_past_filler, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'filler_higher_freq1')))

#table 1
fixefsm3 = as.data.frame(fixef(go_past_filler)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = "Go-Past Time")

rownames(fixefsm3) = c('Intercept', 'Frequency')




percent_greater_zero = data.frame(fixef(first_pass_filler, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'filler_higher_freq1')))

#table 1
fixefsm4 = as.data.frame(fixef(first_pass_filler)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(measure = "First-Pass Regression")

rownames(fixefsm4) = c('Intercept', 'Frequency')



fillers_exp4 = bind_rows(fixefsm1, fixefsm2, fixefsm3, fixefsm4, .id = "ID") %>%
  mutate(term = rep(c("Intercept", "Frequency"), times = 4))





groupings=rle(fillers_exp4$measure)

fillers_exp4 %>%
  select(term, everything(), -measure, -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')
```



```{r, echo = F, fig.align = 'center', warning = F, message = F}
#| label: fig-filleritemsall
#| fig-cap: "Visualization of the effects of frequency on each eye-tracking measure for filler items."
#| fig-width: 7
#| fig-height: 7


model_first_fixation_filler_plot = plot(conditional_effects(first_fixation_filler), plot = F)[[1]] +
  theme_bw()


model_first_fixation_filler_plot = model_first_fixation_filler_plot +
  ylab('First Fixation') +
  xlab('Frequency') +
  scale_x_discrete(breaks = c(0, 1), labels = c("Low", "High")) +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 



model_gaze_filler_plot = plot(conditional_effects(gaze_filler), plot = F)[[1]] +
  theme_bw()


model_gaze_filler_plot = model_gaze_filler_plot +
  ylab('Gaze/First-Pass Times') +
  xlab('Frequency') +
  scale_x_discrete(breaks = c(0, 1), labels = c("Low", "High")) +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_go_past_filler_plot = plot(conditional_effects(go_past_filler), plot = F)[[1]] +
  theme_bw()


model_go_past_filler_plot = model_go_past_filler_plot +
  ylab('Go-Past Times') +
  xlab('Frequency') +
  scale_x_discrete(breaks = c(0, 1), labels = c("Low", "High")) +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


model_first_pass_regression_filler_plot = plot(conditional_effects(first_pass_filler), plot = F)[[1]] +
  theme_bw()


model_first_pass_regression_filler_plot = model_first_pass_regression_filler_plot +
  ylab('First-Pass Regressions') +
  xlab('Frequency') +
  scale_x_discrete(breaks = c(0, 1), labels = c("High", "Low")) +
  theme_bw() +
  theme(axis.text=element_text(size = 12),
        axis.title=element_text(size  = 12)) 


ggarrange(model_first_fixation_filler_plot, model_gaze_filler_plot, model_go_past_filler_plot, model_first_pass_regression_filler_plot, nrow = 2, ncol = 2)

```


### Discussion

In Experiment 4, we find an effect of plausibility only in first-pass regressions. Interestingly, we do find an effect of plausibility in first-fixation times for low-predictability items, but not for high-predictability items. While this follows our theoretical prediction that high-predictability items may be able to overcome the local implausibility, the results are difficult to reconcile with the results we see for first-pass regressions. For first-pass regressions, we observe the opposite pattern: there is an effect of plausibility on first-pass regressions for high-predictability items but not low-predictability items.

We also find no effect of plausibility for gaze duration or go-past times, which was unexpected. Further, upon inspecting our filler items we were able to confirm that this was not a consequence of measurement error.

## Conclusion

The present study examined the processing of compound nouns in locally implausible and locally plausible contexts, specifically with respect to their phrasal frequency and predictability. In Experiment 1 we replicated @staubTimeCoursePlausibility2007 using the A-maze task [@boyceMazeMadeEasy2020] and found an increase in reaction time for the implausible condition at N1 region, but no interaction effect between plausibility and familiarity. Additionally at the N2 region, we found an increase in reaction time for the plausible condition relative to the implausible condition and a decrease in reaction time for high predictability items relative to low predictability items.

In Experiment 2 we extended Experiment 1 using predictability as the key measure instead of phrasal frequency. Similar to Experiment 1, we found an increase in reaction time for the implausible condition at the N1 region, but again found no interaction effect between plausibility and predictability. Also similar to Experiment 1, we found an increase in reaction time at the N2 region for the plausible condition and a decrease in reaction time for the high predictability items.

In Experiments 3 and 4 we replicated the two experiments with eye-tracking. In Experiment 3, we found an effect of plausibility in first fixation times, go-past times, and first-pass regressions. We also found an interaction effect in go-past times such that high-frequency items had higher go-past times in the implausible condition, but low-frequency items did not. In Experiment 4, we find an effect of plausibility in first-fixation times for low-predictability items (but not high-predictability items) and an an effect of plausibility in first-pass regressions for high-predictability items but not low-predictability items.

Overall the results of experiments 1 and 2 suggest that the frequency or predictability of the second noun in the compound (given the first noun) has very little facilitatory effect on the processing of the first noun in implausible contexts relative to plausible contexts. That is, the increase in reaction time in the implausible condition for the N1 region was not mediated by the frequency or predictability of the compound noun. If participants were predicting the second noun upon reading the first noun, then we might expect to have seen a decrease in reaction time for the high-predictable items in the implausible condition relative to the low-predictability items because the second noun always eliminated the local implausibility.

Experiments 3 and 4 provide mixed-evidence. On one hand, there seems to be a general effect of implausibility regardless of frequency or predictability, however in some reading measures such as first-fixation times in Experiment 4, predictability seemed to alleviate the slowdown in the implausible condition. On the other hand, for other measures, the slowdown generated by the implausible contexts was actually exacerbated for high-frequency or high-predictability items (e.g., first-pass regression times in both Experiment 3 and Experiment 4).

There are a few possible explanations for the results we found. One possibility is simply that our high-predictability compound nouns aren't stored holistically. It is important to note that our compound nouns were the most predictable compound nouns in the entire Google *n*-grams corpus, thus it seems unlikely that they weren't predictable enough to be stored, though it may be that English compound nouns have relatively low predictability relative to other multi-word phrases. 

Another possibility is that the high-predictability compound nouns are stored holistically, but the processing consequences of them being stored holistically are such that there is no facilitatory effect in the processing of the first noun in the compound noun. This would certainly beg the question, however, of what exactly the processing consequences of holistic storage are. Perhaps the primary advantages to holistic storage are in the domain of production, rather than processing.

<!-- It is also possible that there may be competing forces. On the one hand, high-predictability items may help overcome the local implausibility (as evidenced by the alleviation in first-fixation times found in Experiment 4). On the other hand, there's also evidence from the literature that people are generally more acceptable of low-frequency words in novel contexts than high-frequency words [@harmonPuttingOldTools2017]. It is possible that encountering a low-frequ It is possible that these competing forces explain the mixed-results in the eye-tracking experiments. NOTE: This actually can't be the case, because the frequency was not a measure of the first word -->

Finally, with respect to the increase in reaction time at the N2 region in the plausible condition that we found in Experiments 1 and 2, we do not see this effect in Experiments 3 and 4, suggesting that this effect may be a task-specific effect. This result suggests that in the maze task, participants may have a bias to analyze the first noun as the head noun and then have to reinterpret the sentence once it is clear that the noun is not the head noun. Further, since we don't see the same effect in the implausible condition, participants may not fully commit to an interpretation that is implausible.

In summary, the present study contributes to the current theories of sentence processing by demonstrating that during sentence processing, readers do not seem to access the holistic representation at the first noun (because then they would be able to overcome the implausibility). Instead, it is possible that if compound nouns are stored holistically, perhaps readers don't access the holistic representation until they have heard sufficient evidence for that representation. That is, even though *peanut* is predictive of *butter*, there are still many other words that can occur after *peanut*. Thus, readers may not access the holistic representation until they hear enough of the compound noun to rule out other competing possibilities. 