```{r, include = F}
library(tidyverse)
library(brms)
library(here)
library(ggh4x)
library(knitr)
library(kableExtra)

data_main_model = read_csv(paste0(here("Chapters/LLM Ordering Prefs/Data"), '/allenai_OLMo-7B-0424-hf.csv'))
corpus = read_csv(paste0(here("Chapters/LLM Ordering Prefs/Data"), '/nonce_binoms.csv'))

data_main_model = data_main_model %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)


data_main_model = data_main_model %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))
  #mutate(log_freq = log(OverallFreq))# %>%
  #mutate(OverallFreq = log_freq - mean(log_freq))# %>%
  #mutate(GenPref = GenPref - 0.5) %>%
  #mutate(RelFreq = RelFreq - 0.5)

data_main_model = data_main_model %>%
  rename(no_final_stress = `*BStress`)



data_main_model = data_main_model %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
         bigram2_alpha = paste0('and ', Word2),
         bigram1_nonalpha = paste0(Word2, ' and'),
         bigram2_nonalpha = paste0('and ', Word1)
         )

bigram_counts = read_csv(paste0(here("Chapters/LLM Ordering Prefs/Data"), '/olmo_bigram_freqs.csv'))
onegram_corpus_size = 1560674367427
count_and = 43378012800

bigram_counts = bigram_counts %>%
  mutate(ngram = str_replace_all(ngram, '\t', ' '))

data_main_model = data_main_model %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and)


data_main_model = data_main_model %>%
  mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
         bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
         log_bigram_prob_alpha = log(bigram_prob_alpha),
         log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
         log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)

Olmo_main_genpref = brm(log_odds ~ GenPref,
                       data = data_main_model,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = paste0(here("Chapters/LLM Ordering Prefs/Data"), '/model1_main')
                      )


Olmo_main_genpref_log_odds = brm(log_odds ~ GenPref * log_bigram_odds_ratio,
                       data = data_main_model,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = paste0(here("Chapters/LLM Ordering Prefs/Data"), '/model2_main')
                      )



Olmo_main_individual_constraints = brm(data = data_main_model,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = paste0(here("Chapters/LLM Ordering Prefs/Data"), '/model3_main')
                      )




data_path = here("Chapters/LLM Ordering Prefs/Data")





combined_df = read_csv(paste0(here("Chapters/LLM Ordering Prefs/Data"), "/combined_df.csv"))

checkpoint_tokens_key = combined_df %>%
  group_by(checkpoint) %>%
  slice_head(n=1) %>%
  select(checkpoint, num_tokens, n_billion_tokens)


fixefs_m1 = read_csv(paste0(here("Chapters/LLM Ordering Prefs/Data"), "/fixefs_m1.csv"))
fixefs_m3 = read_csv(paste0(here("Chapters/LLM Ordering Prefs/Data"), "/fixefs_m3.csv"))
```

# Emergent Ordering Preferences in Large Language Models

## Introduction

Large language models have stormed the media in the last few years, becoming a popular topic in the scientific literature. Their rise to fame has brought with them many heated debates regarding whether large language models constitute human-like models of language or whether their behavior is completely different from humans [@piantadosiChapterModernLanguage; @piantadosiMeaningReferenceLarge2022; @benderDangersStochasticParrots2021; @bender2020climbingnlumeaning]. For example, there has been an ongoing debate about whether large language models learn any knowledge about the meaning of words. @bender2020climbingnlumeaning has argued that large language models, which are only trained on the form, have no way of learning anything about meaning. They pointed out that large language models do not have the rich information that humans receive, such as the referent of the form. However, @piantadosiMeaningReferenceLarge2022 rebutted this claim by arguing that co-occurrence statistics can be extremely informative about a word's meaning. For example, they argued that many words, such as "justice", contain no clear referent and instead have to be learned by humans based on the context that they occur in. It seems plausible that large language models could learn at least some information about the meaning of words in a similar manner.

Other debates have centered around the tradeoff between computation and storage: how much are these models simply reproducing from their training data vs how much of their productions are novel utterances using learned linguistic patterns? On one hand, there is no doubt that large language models store and reproduce large chunks of language. In fact, OpenAI is even being sued by *The New York Times* for allegedly reproducing entire articles verbatim.[^writeup-1] This sentiment – that large language models are nothing but glorified copy cats – has been echoed by several other prominent linguists [@bender2020climbingnlumeaning; @benderDangersStochasticParrots2021; c.f., @piantadosiMeaningReferenceLarge2022].

[^writeup-1]: <https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf>

Specifically, proponents of the "LLMs as copy cats" argument have pointed out that large language models are trained on an inconceivably large amount of data. For example, the OLMo models were trained on trillions of tokens [@groeneveldOLMoAcceleratingScience2024].[^writeup-2] As such, it is difficult to determine how much of the text generated by an LLM is truly novel, and how much is simply reproduced from its training data. This is further complicated by the fact that training data for LLMs is typically either not publicly available, or so huge that it's incredibly difficult to work with. On the other hand, it is clear that large language models are learning at least some linguistic patterns. For example, @mccoyHowMuchLanguage2023 demonstrated that GPT-2 is able to generate well-formed novel words as well as well-formed novel syntactic structures, despite copying extensively.

[^writeup-2]: This is magnitudes larger than the 350 million words that the average college-aged speaker has seen in their lifetime [@levyProcessingExtraposedStructures2012].

These debates, however, have been highly theoretical and speculative and very few empirical studies have been done to actually investigate these questions [c.f., @lasriSubjectVerbAgreement2022; @lebrun2022evaluatingdistributionaldistortion; @mccoyHowMuchLanguage2023; @panareexplicitbelief]. Thus in the present paper we address these debates by taking an in-depth look at large language models' abilities to learn abstract knowledge beyond simply the statistics of individual tokens. Specifically we examine the ordering of novel binomials in English (Noun *and* Noun constructions, e.g., *cats and dogs*). The nouns in binomials can be ordered without affecting the meaning of the phrase much (e.g., *cats and dogs* vs *dogs and cats*). Despite this, human preferences for which noun should be placed first vary in strength. For example, there is a pretty strong preference for *bread and butter* as opposed to *butter and bread*. However, both *computers and monitors* and *monitors and computers* are natural. Binomials are a useful test case because there is a great deal of evidence demonstrating that human ordering preferences are driven by abstract preferences, such as a preference for more powerful words to be placed first (e.g., *god and man* vs *man and god*). Thus by examining binomials varying in frequency, we can gain insight into whether large language models are learning abstract preferences and to what degree they learn similarly to humans.

### Abstractions in Large Language Models

The evidence for learned abstractions in large language models is extremely mixed. Investigations into BERT have yielded mixed results for their ability to learn and apply abstract knowledge [@haleyThisBERTNow2020; @liAreNeuralNetworks2021; @lasriSubjectVerbAgreement2022; @liAssessingCapacityTransformer2023]. For example, @haleyThisBERTNow2020 demonstrated that many of the BERT models are not able to reliably determine the plurality of novel words. Additionally, @liAreNeuralNetworks2021 demonstrated that when tasked with producing the correct tense for a word, BERT tends to rely on memorization from its training data as opposed to learning the more general linguistic pattern.

On the other hand, @lasriSubjectVerbAgreement2022 demonstrated that BERT can generalize well to novel subject-verb pairs. They tested BERT's performance on novel sentences along with semantically incoherent but syntactically sensible sentences (e.g., *colorless green ideas sleep furiously*) and found that when masking the verb for these sentences BERT still applies higher probability to the correct inflection of the verb. Additionally, @liAssessingCapacityTransformer2023 demonstrated that BERT is able to use abstract knowledge to correctly predict subject-verb and object-past participle agreements in French.

Research using other language models have also yielded similar results. For example, as mentioned earlier, @mccoyHowMuchLanguage2023 found that while GPT-2 copies extensively, it also produces both novel words as well as novel syntactic structures. Additionally @misraLanguageModelsLearn2024 examined whether a language model trained on a comparable amount of data as humans can learn article-adjective-numeral-noun expressions (a beautiful five days). Specifically, without having a great deal of experience with them, humans learn that *a beautiful five days* is perfectly grammatical, but *a blue five days* is not. @misraLanguageModelsLearn2024 demonstrated that language models learn this even if they have no AANNs in their training data. They further demonstrated that they do this by generalizing across similar constructions, such as *a few days*. Further, @yaoBothDirectIndirect2025 examined whether language models trained on a comparable amount to humans can learn the length and animacy preferences that drive dative alternations in humans. They found that language models can learn these preferences, even without much experience with the dative alternation. These results suggest that language models can learn generalizations without a great amount of data.

Additionally, there's evidence that inducing abstractions facilitates performance in large language models. For example, @zhengTakeStepBack2024 used a novel prompting technique to enable LLMs to use abstractions when reasoning. They found that LLMs hallucinate less when they implement abstractions in their reasoning. Similarly, @mccoyUniversalLinguisticInductive2020 demonstrated that large language models can use abstractions, such as an abstract preference for certain syllable structures, to learn language more easily. Their results suggest that inducing abstractions may help reduce the amount of training that large language models require.

Finally, there is also evidence that transformer models can learn abstractions from other domains as well. For example, @tartagliniDeepNeuralNetworks2023 examined the ability of a transformer model in a same-different task (i.e., determining if two entities in an image are the same). They found that some models can reach near perfect accuracy on items they have never seen before. Since models are learning abstractions in other domains, it also stands to reason that they may be learning abstractions in language-related tasks as well.

### Abstractions in Humans

Abstractions have been a part of just about every linguistic theory, including both generativist and non-generativist theories. This is not surprising since one of the hallmarks of human language learning is the ability to produce novel, never-heard-before utterances. In order to do so, most theories posit that humans leverage their remarkable ability to learn linguistic patterns beyond simple co-occurrence rates [c.f., @ambridgeStoredAbstractionsRadical2020]. For example, when presented a novel noun, children are able to consistently produce the proper plural form of that noun [@berkoChildsLearningEnglish1958]. Similarly, children are able to leverage similarities across different contexts to learn a word's general meaning [@yuRapidWordLearning2007].

Abstractions also play a large part in the way that humans linearize their thoughts into sentences [@morgan2024; @morganAbstractKnowledgeDirect2016]. For example, there have been several studies showing human ordering preferences for binomials are driven, at least in part, by abstract ordering preferences [@morganAbstractKnowledgeDirect2016; @morgan2015; @morgan2024]. In order to examine this, @morganAbstractKnowledgeDirect2016 coded a list of binomials for a variety of semantic, phonological, and metric constraints that affect human ordering preferences for binomials [@benorChickenEggProbabilistic2006]. They trained a logistic regression model on this corpus to predict the probability that a binomial will occur in alphabetical order (A and B, a neutral reference order) as a function of these constraints. They demonstrated that their logistic regression model performs significantly better than chance in predicting the ordering of attested binomials from @siyanova-chanturiaSeeingPhraseTime2011's items (which the model was not trained on).

The logistic regression model they used combines the various constraints into a single generative preference constraint that can be used to examine human ordering preferences. Thus, @morganAbstractKnowledgeDirect2016 created a separate list of 42 novel and 42 attested binomials and once again coded them for the previously-mentioned constraints. They then examined how human ordering preferences (operationalized as self-paced reading times) are affected by 1) generative preferences (estimated using the logistic regression model), 2) relative frequency (for a given binomial, the proportion of occurrences in alphabetical order to non alphabetical order), and 3) overall frequency (the total occurrences of the binomial in alphabetical and nonalphabetical ordering). They found a significant effect of generative preferences for low-frequency binomials, but not for high-frequency binomials. This suggests that humans rely on their compositional knowledge when they have little experience with an item, but rely on their item-specific experience when they have enough experience with an item. Interestingly, more recently @morgan2024 also demonstrated that generative preferences exert a constant effect throughout the frequency spectrum, affecting even high-frequency binomials (although the effect is weaker for high-frequency binomials). In other words, even in cases where humans have experienced a binomial many times, their ordering preferences aren't driven exclusively by their experience with the binomial.

Since human ordering preferences deviate from the observed preferences [i.e., humans aren't simply reproducing binomials in the same order they they heard them; @morgan2024], ordering preferences thus present a useful test case for large language models. If large language models learn representations beyond simply memorizing the training dataset or superficially reproducing word co-occurrences, they may learn abstract ordering preferences similar to humans, and this may be reflected in their binomial ordering preferences.

### Present Study

In the present study we examine whether large language models are simply copying their input, or whether they are behaving more similarly to humans and learning abstract linguistics patterns. We use binomials as a test case because human ordering preferences deviate from the observed preferences for them. While binomials are a single linguistic construction, they are well-studied in the linguistics literature and thus provide us with a strong human baselines that we can compare the LLM's performance to.

In Experiment 1, we examine whether large language models are sensitive to ordering preferences for binomials that range from low-frequency to high-frequency. In Experiment 2, we go more in-depth and explore whether OLMo-7B [@groeneveldOLMoAcceleratingScience2024] is sensitive to abstract ordering preferences for novel binomials that the model has never seen before. Finally, in Experiment 3, we examine the same questions at different stages of the model's training in order to determine how these abstract ordering preferences emerge as a function of the training.

## Experiment 1

### Methods

#### Dataset

In order to examine the ordering preferences of binomial constructions in large language models, we use a corpus of binomials from @morgan2015. The corpus contains 594 binomial expressions which have been annotated for various phonological, semantic, and lexical constraints that are known to affect binomial ordering preferences. The corpus also includes:

1.  The estimated generative preferences for each binomial representing the ordering preference for the alphabetical ordering (a relatively unbiased reference form), estimated from the above constraints (independent of frequency). The generative preferences take a value between 0 and 1, with 0 being a stronger preference for the nonalphabetical form, and 1 being a stronger preference for the alphabetical form. The generative constraints were calculated using [@morgan2015]'s model.

2.  The observed binomial orderings which are the proportion of binomial orderings that are in alphabetical order for a given binomial, gathered from the Google *n*-grams corpus [@linSyntacticAnnotationsGoogle2012]. The Google *n*-grams corpus is magnitudes larger than the language experience of an individual speaker and thus provides reliable frequency estimates.

3.  The overall frequency of a binomial expression (the number of times the binomial occurs in either alphabetical or non-alphabetical order). Overall frequencies were also obtained from the Google *n*-grams corpus [@linSyntacticAnnotationsGoogle2012].

#### Language Model Predictions

In order to derive predictions for large language models, we used the following models from the GPT-2 [@radfordLanguageModelsAre2019] family, the Llama-2 [@touvronLlama2Open2023] family, Llama-3 family ([https://github.com/meta-llama/llama3](#0)), and the OLMo [@groeneveldOLMoAcceleratingScience2024] family. From smallest to largest in number of parameters: GPT-2 (124M paramters), OLMo 1B (1B parameters), GPT-2 XL (1.5B parameters), Llama-2 7B (7B parameters), OLMo 7B (7B parameters), Llama-3 8B (8B parameters), Llama-2 13B (13B parameters), and Llama-3 70B (70B parameters). For each model, we calculated the ordering preferences of the alphabetical form for each binomial in the dataset. The predicted probability of the alphabetical form was calculated as the product of the model's predicted probability of each word in the binomial. In order to accurately calculate the probability of the first word in the binomial, each binomial was prepended with the prefix "Next item: ". This is necessary because the large language models each provide the predicted probability of the next word given the current word, so in order to get the probability of the first word in the binomial, it must occur after some other token. Following this, the probability of the alphabetical form, *A and B* is:

$$
\begin{aligned}
    P_{alphabetical} & = P(A|Next\:item: )\\
      & \times P(and|Next\:item: A)\\
      & \times P(B|Next\:item: A\: and)
\end{aligned}
$$ \noindent where *A* is the alphabetically first word in the binomial and *B* is the other word. Additionally, the probability of the nonalphabetical form, *B and A* is:

$$
\begin{aligned}
    P_{nonalphabetical} & = P(B|Next\:item: )\\
      & \times P(and|Next\:item: B)\\
      & \times P(A|Next\:item: B\: and)
\end{aligned}
$$

Finally, to get an overall ordering preference for the alphabetical form, we calculated the (log) odds ratio of the probability of the alphabetical form to the probability of the nonalphabetical form:

$$
LogOdds(AandB) = log(\frac{P_{alphabetical}}{P_{nonalphabetical}})
$$

#### Analysis

The data was analyzed using Bayesian linear regression models, implemented in *brms* [@burknerBrmsPackageBayesian2017] with weak, uninformative priors. For each model, the dependent variable was the log odds of the alphabetical form to the nonalphabetical form. The fixed-effects were abstract ordering preference (represented as *AbsPref* below), observed preference (*ObservedPref*), overall frequency (*Freq*), an interaction between overall frequency and abstract ordering preference (*Freq:AbsPref*), and an interaction between overall frequency and observed preference (*Freq:ObservedPref*). The model equation is presented below:

$$
\begin{aligned}
    LogOdds(AandB) & \sim AbsPref \\
      & + ObservedPref\\
      & + Freq\\
      & + Freq:AbsPref\\ 
      & + Freq:ObservedPref
\end{aligned}
$$ {#eq-model}

\noindent Frequency was logged and centered, and abstract ordering preference and observed preference were centered such that they ranged from -0.5 to 0.5 (instead of from 0 to 1). Note that since abstract ordering preference and observed preference are on the same scale, we can directly draw comparisons between the coefficient estimates for these fixed-effects in our regression model.

### Results

Our full model results are presented in the appendix (@tbl-model_results_full) and visualized in @fig-results. For each model, the figure shows the values for each of the coefficients from the model in @eq-model, representing how strongly each language model relies on observed preference, abstract ordering preference, overall frequency, the interaction between abstract ordering preference and overall frequency, and the interaction between observed preference and overall frequency.

Our results are similar across all the large language models we tested. Specifically, we find no effect of abstract ordering preferences and no interaction effect between abstract ordering preference and overall frequency. We do find an effect of observed preference suggesting that the models are mostly reproducing the ordering preferences found in their training. We also find an interaction effect between observed preference and overall frequency, suggesting that the effect of observed frequency is stronger for high-frequency items.

![Results for each beta coefficient estimate from each model. Models are arranged from smallest to largest from left to right. The x-axis contains each coefficient and the y-axis contains the predicted beta coefficient of the respective model. Error bars indicate 95\\% credible intervals.](/Chapters/LLM%20Ordering%20Prefs/Figures/full_plot.pdf){#fig-results width=100%}

### Discussion

In the present study we examined the extent to which abstract ordering preferences and observed preferences drive binomial ordering preferences in large language models. We find that their ordering preferences are driven primarily by the observed preferences. Further, they rely more on observed preferences for higher frequency items than lower frequency items. Finally, they don't seem to be using abstract ordering preferences at all in their ordering of binomials.

Our results give us insight into the differences between humans and large language models with respect to the ways in which they trade off between abstract and observed preferences. For example, our dataset contains low-frequency binomials (e.g., *alibis and excuses*), including binomials that a college-age speaker would have heard only once in their life. Due to their low frequency, humans rely substantially on abstract ordering preferences to process these lower frequency items [@morgan2024]. This is not the case, however, for large language models, which rely exclusively on observed preferences for these items. This is true even for the smallest models we tested, such as GPT-2. We conclude that, although large language models can produce human-like language, in the case of binomials, they accomplish this in a quantitatively different way than humans do: they rely on observed statistics from the input in at least some cases when humans would rely on abstract representations.

## Experiment 2

In Experiment 1, we demonstrated that large language models don't use abstract ordering preferences when producing binomials. However, it is possible that this is because they have experienced the binomials before, even the low-frequency ones. Thus in Experiment 2 and Experiment 3 we examine whether OLMo-7B is sensitive to abstract ordering preferences for novel binomials that the model has never seen before. We also examine the individual constraints that drive abstract ordering preferences in humans, such as the preference for short words before long words, to determine whether OLMo is sensitive to the same constraints in the same way as humans.

Specifically, in Experiment 2, we examine whether OLMo-7B's ordering preferences are driven by abstract ordering preferences for novel binomials. In order to do so, we created a list of binomials and searched the Dolma corpus we created to confirm that they did not occur in either alphabetical or nonalphabetical ordering. We then coded the binomials for each of the constraints mentioned earlier. Finally, we examined whether OLMo-7B shows any preference for one ordering over the other for each binomial. If OLMo has developed any abstract ordering preferences, it should show a systematic preference for one ordering over the other. If it has not, then it should show no preference for one ordering over the other.

### Datasets

#### Dolma

For Experiments 2 and 3, we use the dataset described in this section. In order to examine whether large language models learn preferences above and beyond simply memorizing co-occurrence rates, we created a 1-grams, 2-grams, and 3-grams corpus of Dolma [@soldainiDolmaOpenCorpus2024]. Specifically, we used Dolma version 1_7 (2.05 trillion tokens), which was used to train OLMo-7B-v1.7 [@groeneveldOLMoAcceleratingScience2024]. Our corpus contains every n-gram (ignoring punctuation and capitalization) in the Dolma corpus, as well as the number of times that n-gram appeared.

We then created a list of binomials and searched the corpus to find a list of binomials that did not occur. We eliminated binomials which occurred more than zero times in either possible ordering. Thus, OLMo has had no experience with either ordering of any of our binomials. We also calculated their unigram and bigram frequencies for each binomial. Our full list of items comprises 131 binomials and is reproduced in the appendix section (Section @sec-full-list-of-stimuli).

<!--# revise footnote with actual github link. This should probably get its own github repo, so I will need to change this as well. -->

#### Abstract Ordering Preferences Corpus

In order to examine whether large language models are learning preferences similar to humans, we calculated the abstract ordering preference value for each of our binomials [following @morganAbstractKnowledgeDirect2016]. @morganAbstractKnowledgeDirect2016 demonstrated that their model's estimated abstract ordering preference value is a significant predictor of human binomial ordering preferences, even after accounting for the frequency of each ordering. Abstract ordering preferences are calculated from a mix of semantic and phonological properties that human binomial ordering preferences have been shown to be sensitive to [@benorChickenEggProbabilistic2006]. For each of these constraints, a positive value indicates a preference for the alphabetically first word to be placed first (a neutral reference order). A negative value indicates a preference for the nonalphabetical word to be placed first. For example, a positive value of frequency indicates that the alphabetical word is more frequent and thus is predicted to be placed first, while a negative value indicates that the nonalphabetical word is more frequent. The constraints along with the estimated weights in humans are as follows [taken from @morgan2015]:

-   **Length**: The shorter word should appear first, e.g. *abused and neglected*. In human data, the weight of this constraint was estimated to be 0.15.

-   **No Final Stress**: The final syllable of the second word should not be stressed, e.g. *abused and neglected*. In human data, the weight of this constraint was estimated to be 0.36.

-   **Lapse:** Avoid unstressed syllables in a row, e.g. *FARMS and HAY-fields* vs *HAY-fields and FARMS.* In human data, the weight of this constraint was estimated to be 0.19.

-   **Frequency**: The more frequent word comes first, e.g. *bride and groom*. In human data, the weight of this constraint was estimated to be 0.09.

-   **Formal Markedness**: The word with more general meaning or broader distribution comes first, e.g. *boards and two-by-fours*. In human data, the weight of this constraint was estimated to be 0.24.

-   **Perceptual Markedness**: Elements that are more closely connected to the speaker come first. This constraint encompasses @cooper1975worldorder's \`Me First' constraint and includes numerous subconstraints, e.g.: animates precede inanimates; concrete words precede abstract words; e.g. *deer and trees*. In human data, the weight of this constraint was estimated to be 0.25.

-   **Power**: The more powerful or culturally prioritized word comes first, e.g. *clergymen and parishioners*. In human data, the weight of this constraint was estimated to be 0.26.

-   **Iconic/scalar sequencing**: Elements that exist in sequence should be ordered in sequence, e.g. *achieved and maintained*. In human data, the weight of this constraint was estimated to be 1.30.

-   **Cultural Centrality**: The more culturally central or common element should come first, e.g. *oranges and grapefruits*. In human data, the weight of this constraint was estimated to be 0.42.

-   **Intensity**: The element with more intensity appear first, e.g. *war and peace*. In human data, the weight of this constraint was estimated to be 0.02.

\noindent @morgan2015 then used a logistic regression model to combine these constraints into a single constraint (overall abstract preference) for each binomial.

### Methods

#### Language Model Predictions

We use the same methods as Experiment 1 to obtain language model predictions for our items.

#### Analyses

We present three Bayesian linear mixed-effects models implemented in *brms* [@burknerBrmsPackageBayesian2017] with weak, uninformative priors. For each of our models, the intercept represents the grand mean and the coefficient estimates represent the distance of the effect from the grand mean. Bayesian statistics don't force us into a binary interpretation of significance, however we can consider an estimate to be statistically significant if the credible interval for that estimate excludes zero.

For all three analyses, the dependent variable is LogOdds(AandB), which was described above. Our dependent variable in the first analysis is the abstract ordering preference for each binomial (AbsPref). In order to rule out bigram probabilities as driving the model's preferences, our second analysis contains the binomial's bigram probabilities (the odds ratio of product of bigram probabilities of the alphabetical order to the product of bigram probability of the nonalphabetical order) as well. Finally, our dependent variables in the third analysis are the individual constraints that are used to calculate AbsPref. The model equations are below in @eq-m1, @eq-m2, and @eq-m3. Note that Formal Markedness and Iconicity were dropped from the second model because the constraint values were zero for all of the binomials. Further, our constraints demonstrated a level of co-linearity. Co-linearity can result in poor model estimates and inflated credible intervals. In order to deal with this, we dropped the constraint with the highest variance inflation factor (which turned out to be the lapse constraint). All other constraints had a variance inflation factor value below 1.5. We then performed backward model selection and dropped the predictors whose credible intervals were most centered around zero until the remaining predictors' credible intervals were all at least 75% greater than or less than zero. This resulted in dropping the no final stress, intense, and percept constraints. We acknowledge that this approach is quite exploratory and thus interpretations at the level of the individual constraint must be taken with a grain of salt.

$$
LogOdds(AandB) \sim AbsPref
$$ {#eq-m1}

$$
LogOdds(AandB) ~ AbsPref \cdot BigramProbs
$$ {#eq-m2}

$$
LogOdds(AandB) \sim Culture + Power + Freq + Len
$$ {#eq-m3}

### Results

The results for the first analysis are presented below in @tbl-exp1m1. Our results suggest that there is a main-effect of abstract ordering preference for OLMo's 7B model. A visualization of these results can be found below in @fig-exp1m1.

```{r, echo = F, message = F}
#| label: tbl-exp1m1
#| tbl-cap: "Model results examining the effect of AbsPref on LogOdds(AandB)."

#table 1

percent_greater_zero = data.frame(fixef(Olmo_main_genpref, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'AbsPref')))

#table 1
fixefsm1 = as.data.frame(fixef(Olmo_main_genpref)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c("Intercept", "AbsPref"), times = 1))

rownames(fixefsm1) = c('Intercept', 'AbsPref')

fixefsm1 = fixefsm1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(n1_all_measures$measure)

fixefsm1 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')

```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-exp1m1
#| fig-cap: "Visualization of the effects of AbsPref on LogOdds(AandB)"

gen_pref_data = data_main_model 



main_model_plot = ggplot(data = gen_pref_data, aes(x=GenPref, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
  ylab('LogOdds(AandB)') +
  xlab('Abstract Ordering Preference') +
  #facet_wrap(~constraint) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 16, color = 'black'),   # Increase axis label size
    axis.text = element_text(size = 14, color = 'black'),    # Increase tick mark labels
    axis.ticks.length = unit(0.3, "cm")     # Make tick marks longer
  )

main_model_plot

```

The results of our second model suggest that the model's ordering preferences were not driven by bigram probabilities (see @tbl-exp1bigrams).

```{r, echo = F, message = F}
#| label: tbl-exp1bigrams
#| tbl-cap: "Model results examining the effect of AbsPref and Bigram Probabilities on LogOdds(AandB)."



percent_greater_zero = data.frame(fixef(Olmo_main_genpref_log_odds, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'AbsPref')))

#table 1
fixefsm2 = as.data.frame(fixef(Olmo_main_genpref_log_odds)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = rep(c('Intercept', 'AbsPref', 'BigramProbs', 'AbsPref:BigramProbs'), times = 1))

rownames(fixefsm2) = c('Intercept', 'AbsPref', 'BigramProbs', 'AbsPref:BigramProbs')


fixefsm2 = fixefsm2 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(n1_all_measures$measure)

fixefsm2 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')

```

While these results suggest that the large language models' ordering preferences are sensitive abstract ordering preferences and not bigram probabilities, it's unclear whether their behavior is similar to humans on the level of the individual constraints. Thus, in the third analysis we examined which specific constraints the model is sensitive to, and to what extent.[^writeup-3] For this analysis, following @houghtonTaskdependentConsequencesDisfluency2024, we also present the percentage of posterior samples greater than zero. The results of this analysis can be found below in @tbl-exp1m2.

[^writeup-3]: It's also important to consider how many of our binomials the constraint even applied to (i.e., how many binomials were the constraints non-zero). For the Culture constraint, 62 of our 131 binomials had a non-zero value. For the Power constraint, 54 were non-zero, for the Frequency constraint, all 131 binomials were non-zero, and for the Len constraint, 85 were.

```{r, echo = F, message = F}
#| label: tbl-exp1m2
#| tbl-cap: "Model results examining the effect of each individual constraint on LogOdds(AandB)."

percent_greater_zero = data.frame(fixef(Olmo_main_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

#table2
fixefsm3 = as.data.frame(fixef(Olmo_main_individual_constraints)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)





percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefsm3 = fixefsm3 %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`) %>%
  mutate(term = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))

rownames(fixefsm3) = c('Intercept', 'Culture', 'Power', 'Freq', 'Len')


fixefsm3 = fixefsm3 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(n1_all_measures$measure)

fixefsm3 %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')
```

The model is most sensitive to the Power constraint, however there appears to be a marginal effect of Culture as well, since nearly 95% of the posterior samples are greater than zero despite the credible interval crossing zero. Surprisingly, there also appears to be a negative effect of length with a slight preference to place the longer word first, which is the opposite direction from what we see in humans. Length is often correlated with frequency, since frequent words tend to be shorter. As such, we ran a model without frequency to determine whether the negative effect of length was do to co-linearity with frequency. However, dropping frequency from the model did not affect the effect of length. Further, we also ran a model with only length as the predictor and for that model as well the estimate of length remained negative.

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F, eval = F}
#| label: fig-exp1m2
#| fig-cap: "Visualization of the effects of each individual constraint on LogOdds(AandB)."
#removed this figure since Emily isn't a fan
main_model = data_main_model %>%
  rename('Final Stress' = no_final_stress) %>%
  pivot_longer(c(Culture, Power, Freq, Len), names_to = 'constraint', values_to = 'constraint_value')


main_model_plot = ggplot(data = main_model, aes(x=constraint_value, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
  xlab('Constraint Value') +
  ylab('LogOdds(AandB)') +
  facet_wrap(~constraint) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 16, color = 'black'),   # Increase axis label size
    axis.text = element_text(size = 14, color = 'black'),    # Increase tick mark labels
    axis.ticks.length = unit(0.3, "cm")     # Make tick marks longer
  )

main_model_plot

```

### Discussion

Experiment 2 found that OLMo-7B has learned abstract ordering preferences even for novel binomials that it has never seen before. Further, these ordering preferences aren't simply based on the individual word or bigram frequencies. Specifically, we find a main-effect of abstract ordering preferences on the model's binomial ordering preferences. Additionally, we find a strong preference to place the more powerful word first, a weak preference to place the more culturally central word first, and a weak preference to place the longer word first.

These results together suggest that the model is learning abstract ordering preferences but in a way that is not identical to humans. For example, while both LLMs and humans show a preference for placing the more powerful words first and the more culturally central word first, humans also show a sensitivity to formal markedness, perceptual markedness, and frequency [@morganAbstractKnowledgeDirect2016], which we do not find evidence for in large language models' binomial ordering preferences. Additionally, humans prefer to place the shorter word first [@morganAbstractKnowledgeDirect2016, @morgan2015]. However, we find the opposite finding here: large language models prefer to place the longer word first. One explanation for this is a difference in terms of the input between humans and large language models. The length constraint is determined by the number of syllables. While syllables are salient cues in the audio that humans receive during learning, it's less clear how salient of a cue they are for large language models which receive sub-word tokens (which vary in their size, from being individual orthographic symbols to being entire words.

However, it's unclear how large language models learn these preferences in the first place. Thus, in Experiment 3 we examine these constraints at different points in OLMo's training.

## Experiment 3

In Experiment 2 we demonstrated that large language models are not simply copying their training, but are learning some abstract ordering preferences from their input. However, OLMo makes public various checkpoints during the model's training, thus allowing us the opportunity to examine how these preferences arise as a function of the training. Thus, in Experiment 3 we examine the evolution of these learned abstract ordering preferences as the model learns over time.

### Methods

#### Language Model Predictions

The language model predictions in Experiment 3 were obtained using the same procedure as in Experiments 1 and 2. However, instead of calculating these metrics only for the main model, we calculated them at various checkpoints. These checkpoints are listed below, in terms of the steps as well as the number of billions of tokens the model had been trained on at that checkpoint:

-   Step 0, 0B Tokens

-   Step 1000, 2B Tokens

-   Step 10000, 41B Tokens

-   Step 50000, 209B Tokens

-   Step 100000, 419B Tokens

-   Step 200000, 838B Tokens

-   Step 400000, 1677B Tokens

#### Analysis

The analyses in Experiment 3 were identical to Experiment 2, however we ran these analyses for each of the checkpoints listed above.

### Results

Our model estimates for the effect of AbsPref on LogOdds(AandB) at each checkpoint are presented below in @tbl-exp2m1 and visualized in @fig-exp2m1.

```{r, echo = F, message = F}
#| label: tbl-exp2m1
#| tbl-cap: "Model results examining the effect of AbsPref on LogOdds(AandB) for each checkpoint."


#table 1
fixefsexp2m1 = fixefs_m1 %>%
  left_join(checkpoint_tokens_key) %>%
  filter(Parameter == 'GenPref') %>%
  filter(n_billion_tokens %in% c(0, 2, 41, 209, 419, 838, 1677)) %>%
  arrange(n_billion_tokens) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  select(-n_billion_tokens, -checkpoint, -Parameter) %>%
  mutate()


fixefsexp2m1 = fixefsexp2m1[,c('num_tokens', 'Estimate', 'Est.Error', 'Q2.5', 'Q97.5', '% Samples > 0')]
fixefsexp2m1 = rename(fixefsexp2m1, 'Number of Tokens' = num_tokens)

fixefsexp2m1 = fixefsexp2m1 %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(n1_all_measures$measure)

fixefsexp2m1 %>%
  #select(term, everything()) %>%
  #rename(` ` = term) %>%
  kable(align = c('l', 'r', 'r', 'r', 'r', 'r'), digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 12, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')

```

The model results are visualized below in @fig-exp2m1.

```{r, echo = F, out.width = '70%', fig.align = 'center', warning = F, message = F}
#| label: fig-exp2m1
#| fig-cap: "Visualization of the model predictions for the effect of AbsPref on LogOdds(AandB) for each checkpoint."


fixefs_main_model = as.data.frame(fixef(Olmo_main_genpref)) %>%
  mutate(checkpoint = 'main')
  
fixefs_main_model$Parameter = rownames(fixefs_main_model)
rownames(fixefs_main_model) = NULL


models_all = fixefs_m1 %>%
  full_join(fixefs_main_model) %>%
  left_join(checkpoint_tokens_key)

models_all$checkpoint = factor(models_all$checkpoint, levels = c('step0-tokens0B', 'step500-tokens2B', 'step1000-tokens4B', 'step1500-tokens6B', 'step2000-tokens8B', 'step2500-tokens10B', 'step3000-tokens12B', 'step3500-tokens14B', 'step4000-tokens16B', 'step5000-tokens20B', 'step5500-tokens23B', 'step6000-tokens25B', 'step6500-tokens27B', 'step7000-tokens29B', 'step7500-tokens31B', 'step8000-tokens33B', 'step8500-tokens35B', 'step9000-tokens37B', 'step9500-tokens39B', 'step10000-tokens41B', 'step20000-tokens83B', 'step30000-tokens125B', 'step40000-tokens167B', 'step50000-tokens209B', 'step100000-tokens419B', 'step200000-tokens838B', 'step400000-tokens1677B', 'main'))


models_for_plotting = models_all %>%
  filter(Parameter == 'GenPref') %>%
  filter(n_billion_tokens %in% c(0, 2, 41, 209, 419, 838, 1677)) %>%
  mutate(checkpoint_numeric = as.numeric(factor(checkpoint))) 



plot_all_m1 = ggplot(data = models_for_plotting, aes(x=checkpoint_numeric, y = Estimate)) +
  geom_point() +
  #geom_smooth(method='lm') +
  geom_errorbar(aes(ymin=Q2.5, ymax = Q97.5), position=position_dodge(0.05)) +
  scale_x_continuous(breaks = unique(models_for_plotting$checkpoint_numeric), labels = models_for_plotting$num_tokens) +
  ylab('Coefficient Estimate') +
  xlab('Number of Tokens') +
  theme_bw() +
  theme(
    axis.title = element_text(size = 16, color = 'black'),   # Increase axis label size
    axis.text = element_text(size = 14, color = 'black'),    # Increase tick mark labels
    axis.ticks.length = unit(0.3, "cm")     # Make tick marks longer
  )
  

plot_all_m1
```

Our results demonstrate that it takes quite a large number of tokens for the model to learn the abstract ordering preferences. As @fig-exp2m1 demonstrates, the effect of abstract ordering preference isn't convincing until the model has experienced 1677B tokens. However, it does appear that the model develops a slight preference quite rapidly. For example, by 2 billion tokens there appears to be a very slight (though unconvincing) effect of abstract ordering preferences on the ordering of binomials.

Similar to Experiment 2, in our second analysis we present a breakdown of the effects of each individual constraint. In this analysis, however, we demonstrate the effect of each constraint at each checkpoint. The full table results can be found in the the appendix section (@sec-individual-constraints-at-each-checkpoint), but we present a visualization below in @fig-exp2m2.

```{r echo = F, fig.width=8, fig.height=9.5, fig.align = 'center', warning = F, message = F}
#| label: fig-exp2m2
#| fig-cap: "Visualization of the effect of each constraint on the ordering preference at each checkpoint."

data_main_model2 = data_main_model %>%
  mutate(checkpoint = 'main') %>%
  mutate(n_billion_tokens = 2050)

models_for_plotting2 = combined_df %>%
  full_join(data_main_model2) %>%
  pivot_longer(c(Culture, Power, Freq, Len), names_to = 'constraint', values_to = 'constraint_value') %>%
  filter(n_billion_tokens %in% c(0, 10, 20, 35, 125, 419, 1677, 2050))



models_for_plotting2$n_billion_tokens = factor(models_for_plotting2$n_billion_tokens, levels = c(0, 10, 20, 35, 125, 419, 1677, 2050),
                           labels = c('0', '10B', '20B', '35B', '125B', '419B', '1677B', '2050B'))

models_for_plotting2$constraint = factor(models_for_plotting2$constraint, levels = c("Culture", "Power", "Freq", "Len"), labels = c("Culture", "Power", "Freq", "Len"))

main_model_plot2 = ggplot(data = models_for_plotting2, aes(x=constraint_value, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1, color='black') +
  facet_nested(constraint~n_billion_tokens) +
  #ggtitle('Number of Billions of Tokens') +
  theme_bw() +
  theme(axis.title.x = element_text(size = 15, face = 'bold'),
        axis.title.y = element_text(size = 15, face = 'bold'),
        strip.text.x = element_text(size = 12, face = 'bold'),
        strip.text.y = element_text(size = 12, face = 'bold'),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)) +
  xlab('Constraint Value') +
  ylab('Log Odds') +
  scale_x_continuous(limits = c(-3, 3))

main_model_plot2

```

Interestingly, it appears that early on the model already shows evidence of learning human-like preferences. For example, by 10 billion tokens the model has learned to place more powerful words first and shorter words first. However, the model seems slower to learn to place more culturally central words first. Further, as it receives more training the effect of length undergoes a reversal in direction.

### Discussion

Our results demonstrate that OLMo learns ordering preferences early on for the power, frequency, and length constraints, but is slower to learn ordering preferences for the culture constraint. Further, the model is human-like in its predictions for length early on, but as it receives more training data it learns the opposite length prediction. It is unclear what exactly is causing this reversal, but it may be a function of the tokenization differences between human input and large language models' input.

It is interesting that the model is quick to learn the power constraint, but slower to learn the culture constraint. Both constraints require a level of world-knowledge but the model learns which entities are more powerful relatively quickly, but not which is more culturally central. One interesting question is whether humans also take longer to learn the culture constraint, or whether they learn this early on. If children learn this constraint early on it may suggest differences with respect to how easily large language models learn world knowledge. On the other hand, if children also take longer to learn the culture constraint it may suggest that the power constraint is simply easier to learn.

## Conclusion

In the present study, we examined the ordering preferences of binomials in various large language models. We found that for binomials that the models have experienced, they do not use abstract preferences but instead reproduce them proportionately to their training data. This is the case even for low-frequency binomials. Interestingly, for novel binomials, we found that they do learn and use abstract ordering preferences.

Additionally, while overall the model does show evidence of having learned abstract preferences, these preferences are not identical to human ordering preferences. For example, the model shows a preference for longer words placed before shorter words, which is the opposite of what humans prefer.

Further, show that while the effect of abstract ordering preference on a whole takes a great deal of time (over 1677B tokens to be convincing), the model seems to learn human-like preferences at the level of some individual constraints quite early on.

Overall, our results suggest that large language models are not simply copying their input, but are learning interesting, human-like phenomena from their training. However, they are not learning identically to humans, as demonstrated by the opposite direction of the length preference. Further, while humans rely on abstract preferences even for binomials that they have encountered before, even high-frequency ones [@morgan2024], large language models rely on abstract ordering preferences only for items that they have not encountered at all. In other words, while large language models are able to learn abstract ordering preferences, in cases where humans would rely on these preferences, large language models seem to rely instead on their experience with the item.
