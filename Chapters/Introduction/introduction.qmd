\pagenumbering{arabic}

\doublespacing

# Introduction

How much of language is memorized and how much is improvised? Every time we speak, we are faced with the choice between familiar expressions, like *I don't know*, and novel constructions, like *to me it is uncertain*. In other words, we constantly navigate a trade-off between stored, item-specific knowledge – our stored knowledge of particular words or phrases – and generative knowledge, which allows us to combine those stored representations in a systematic manner.

From a young age, humans are capable of generating sentences that we've never encountered before [@berkoChildsLearningEnglish1958]. This ability is largely enabled by an ability to store forms that we've learned and combine them into new forms using our knowledge of the grammar [@stembergerAreInflectedForms2004; @stembergerFrequencyLexicalStorage1986; @morganAbstractKnowledgeDirect2016; @morgan2015; @morgan2024; @berkoChildsLearningEnglish1958]. In theory, storage and computation can be complementary forces: if an item is stored, it does not necessarily need to be computed, and if an item can be generated via computation, it does not necessarily need to be stored. For example, if the word *cats* is stored, then it is not necessary to compute it (e.g., by combining the lexical root, *cat*, with a general plural rule, -*s*). On the other hand, if it can be computed (e.g., if we have learned the word *cat* and we have learned how to make regular forms plural in English), then we do not necessarily need to store it. However, the fact that computation and storage can be independent does not preclude the possibility of items being both stored holistically and able to be formed compositionally. Indeed, a surge of research in the last few decades has suggested the opposite: that a rich amount of language, including multi-morphemic words and multi-word phrases, is stored holistically [e.g., @bybee2003; @stembergerAreInflectedForms2004; @stembergerFrequencyLexicalStorage1986; @morgan2015; @morgan2024; @morganAbstractKnowledgeDirect2016].

The evidence that more complex forms, such as multi-morphemic words and phrases, may be stored holistically raises several important questions. For example, what factors determine whether a phrase is stored holistically or generated compositionally using knowledge of the grammar? If *pick up* is stored holistically, then under what circumstances does a listener use their knowledge of the grammar to form the phrase compositionally and under what circumstances do they access the holistically stored representation? Similarly, how do compositional representations interact with their holistically stored counterparts during processing? For example, if *pick up* is stored holistically then when listeners hear *pick*, are they able to access the representation of the holistically stored form *pick up* before hearing *up*? Finally, how do stored representations differ from the individual word-level representations? Is the representation of *pick up* completely disconnected from the individual representations of *pick* and *up*, despite the fact that *pick up* is clearly related to both of the individual words?

The present section introduces the relevant background for each of these questions. @sec-accounts-of-storage describes the current debates about storage. @sec-evidence-of-storage-in-words-and-phrases explores the evidence for the holistic storage of multi-morphemic words and multi-word phrases. @sec-factors-that-drive-storage reviews the evidence for factors that drive storage. @sec-representations-of-stored-items examines how stored items are represented. @sec-processing-consequences-of-storage examines the processing consequences of holistic storage. @sec-storage-in-humans-vs-large-language-models examines how large language models trade off between storage and computation. Finally, @sec-outline-of-dissertation outlines the rest of the dissertation.

## Accounts of Storage {#sec-accounts-of-storage}

Traditional linguistic theories have assumed that very little is stored and instead that a great deal of language production leverages humans' remarkable ability to generate complex meaning by combining words together [@chomsky1965]. This was based on an assumption that human memory is limited and that storing items that could be generated compositionally would be an inefficient use of memory. These theories posit that stems of words are stored and more complex word forms are generated via regular rules. For example, the word *cat* would be stored and *cats* would be generated using knowledge of the grammar (and thus would not be stored holistically). Similarly, multi-word phrases would be generated so long as they're compositional. Holistic storage of multi-word phrases is instead reserved for idioms [@chomsky1965] and perhaps extremely high-frequency items [@pinker2002]. According to these theories, *I don't know* would be generated by accessing the individual words and then combining the individual representations together.

However, there may be advantages to storing words or phrases that we can compute. For example, if we are producing a combination of words often enough (e.g., *bread and butter*), it may be efficient to store it in memory and retrieve the stored representation instead of composing it every time. Further, the brain may have dramatically more space for storage than we had previously thought, with an upper bound of 10^8432^ bits [@wangDiscoveringCapacityHuman2003]. Given that @mollicaHumansStoreMegabytes2019 have estimated that in terms of linguistic information humans store only somewhere between one million and ten million bits of information, memory constraints may not be the limiting factor that we once thought.

Following this, usage-based theories have posited the possibility of multi-word phrases being stored holistically [e.g., @bybee2003; @bybee2001; @stembergerAreInflectedForms2004; @stembergerFrequencyLexicalStorage1986; @kapatsinski2018; @morgan2015; @morganAbstractKnowledgeDirect2016; @morgan2024; @goldbergConstructionGrammar2010]. These theories posit that multi-word phrases can be stored if they're used often enough. For example @tomaselloConstructingLanguageUsagebased2005 argued that early verb knowledge is holistic in nature, with children reproducing memorized chunks as opposed to generating verbs in novel contexts. Further, @bybee2003 argued that after learning to produce these verbs in novel contexts, children don't necessarily flush these holistic representations from memory. Instead, proponents of usage-based theories argue that high-frequency phrases like *I don't know* are stored holistically while lower and mid-frequency phrases are generated compositionally.

Usage-based theories of storage naturally developed naturally out of the phonetics and phonology literature, being championed by linguists such as Dr. Janet Pierrehumbert and Dr. Joan Bybee, who demonstrated that phonetic representations could not be reduced to abstract representations with no phonetic details [e.g., @bybeeWordFrequencyContext2002; @pierrehumbertPhonologicalRepresentationAbstract2016; @bybee2003; @bybeeEffectUsageDegrees1999]. Instead, abstract representations require some link to the phonetic details which vary across contexts. In other words, the pronunciation for a word cannot be simply reduced to individual phonemes because the pronunciations for those phonemes depend on various factors, such as the frequency of the word and co-articulation with adjacent phonemes. For example, @bybeeWordFrequencyContext2002 demonstrated that phonetically conditioned changes affect high-frequency words before low-frequency words. She demonstrated that the reduction of an unstressed vowel to schwa is more likely in high-frequency words than low-frequency words. She further demonstrated that a word that occurs more often in a context favorable for the phonetically conditioned change will change more quickly than a word that does not occur as often in a context favorable for the change. For example, @bybeeWordFrequencyContext2002 demonstrated that /d/ deletion in regular past-tense forms in English is much less common than /t/ deletion in the negation form (-*nt*). In English, /t/ and /d/ deletions are less common when the following context contains a vowel and @bybeeEffectUsageDegrees1999 demonstrated that past-tense forms in English occur before a vowel-context more often than the negation form does. They argued that this leads to the deletion being less common even outside of the change-blocking context. On the other hand, the negation form occurs before vowels much less frequently and thus occurs in a deletion-favoring context more often. As a consequence, even when encountered outside of that context, it undergoes deletion at a higher rate. In other words, in contexts that don't favor deletion, /t/ deletion is more common in the negation form than in the regular past-tense form because the negation form occurs more often in /t/-deletion favoring contexts. @bybeeWordFrequencyContext2002 further argued that it is difficult to account for these results with a model that reduces words to abstract phonological representations that are context-independent because the context determines the phonetic details of the sound.

Similarly, @mcmurrayWithincategoryVOTAffects2009 demonstrated that people are sensitive to gradient changes in VOT. Specifically, in a visual-word paradigm the authors presented participants with words like *barricade*/*parakeet*. They systematically manipulated the voice-onset-timing for the initial consonant in each pair and measured the proportion of fixations to the competitor. They found that even within-category variability of VOT affected the proportion of fixations to the competitor. However if people are representing the words in terms of abstract phonetic categories, such as /b/ and /p/, then people should not be sensitive to within-category variability. Instead, humans must have gradient representations that enable this sensitivity to within-category variation.

The phonetics literature demonstrated that representations of words can not be reduced to context-independent phoneme-level representations and this led to many of the same questions being asked about higher levels of representations [e.g., words or phrases\; @bybee2003]. That is, if simple words are being represented holistically with rich phonetic detail, then perhaps it is possible that multi-morphemic words or even phrases may similarly be represented holistically.

## Evidence of Storage in Words and Phrases {#sec-evidence-of-storage-in-words-and-phrases}

There is a great deal of evidence that multi-word phrases are stored holistically. For example, high-frequency phrases such as *I don't know* undergo phonetic reduction that isn't seen in other low or mid-frequency phrases containing *don't* [@bybeeEffectUsageDegrees1999]. If the representation of *don't* is the same across different contexts, we would expect *don't* to be equally reduced in those contexts. As such, the phonetic reduction of *I don't know* suggests a holistic representation separate from that of the individual words. Following this, @bybee2003 demonstrated that there are many high-frequency phrases that undergo phonetic reduction that can't be accounted for by phonetic reduction of the words outside of those phrases (e.g., *going to*, *have to*, *want to*, etc).

Similarly, @yiEumun2002 found evidence for holistic storage of phrases in Korean as well. In Korean, certain consonants undergo tensification when they occur after the future tense marker -*l*. @yiEumun2002 demonstrated that the rate of this tensification is higher in high-frequency phrases than low-frequency phrases, suggesting that they have a separate representation. These results parallel findings at the monomorphemic word-level (which most theories posit have separate representations). For example, in Korean, epenthesis (insertion of a sound) occurs more often in high-frequency words than in low-frequency words [@leeFrequencyEffectsMorphologisation2015]. Similarly, deletion is more likely to occur in a frequent word like *most* than in an infrequent word like *mast* [@kapatsinskiHierarchicalInferenceSound2021; @bybeeWordFrequencyContext2002]. This parallelism is important because monomorphemic words must be stored. Thus the fact that the patterns of phonetic reduction in certain phrases mirrors the patterns at the monomorphemic word-level suggests that they may be stored holistically as well.

The psycholinguistics literature has also provided an abundance of evidence for multi-word holistic storage [@stembergerAreInflectedForms2004; @stembergerFrequencyLexicalStorage1986; @morganAbstractKnowledgeDirect2016; @siyanova-chanturiaSeeingPhraseTime2011; @kapatsinskiFrequencyEmergencePrefabs2009]. For example, by examining corpus data, @stembergerFrequencyLexicalStorage1986 demonstrated that errors occur less in high-frequency words than in low-frequency words. They argued that one of the consequences of high frequency is greater accuracy. They further suggested that if inflected forms are stored holistically than no-marking errors should be less common in high-frequency inflected forms than in lower frequency forms for both regular and irregular items. This is exactly what they found: they showed that fewer no-marking errors (e.g., producing *walk* instead of *walked*) are made for the past-tense forms of frequent verbs relative to infrequent verbs. They further replicated their results in spontaneous speech and found that participants produce no-marking errors less often in high-frequency regular verbs than in low-frequency regular verbs. They argued that if people are accessing each morpheme individually (e.g., accessing *walk*, then *-ed*), then errors on *-ed* should be independent of errors on the base form. That is, accessing *walk* more easily or more difficulty should not influence the error rate of the past-tense morpheme, which is constant across all verbs (if they're not stored holistically).

In addition to production, the processing literature has also found a great deal of evidence for storage. For example, @siyanova-chanturiaSeeingPhraseTime2011 investigated the reading times of binomials in their frequent ordering (e.g., *bread and butter*) and in their infrequent ordering (e.g., *butter and bread*). They found that humans read binomials faster in their frequent ordering. Further, in a follow-up study @morganAbstractKnowledgeDirect2016 examined whether this finding is due to generative constraints, such as a preference for short words before long words [@benorChickenEggProbabilistic2006] or whether it is due to the items being stored holistically. By annotating a corpus of binomials for constraints known to affect the ordering of binomials in corpus data, @morganAbstractKnowledgeDirect2016 developed a probabilistic model to predict binomial orderings. The model combines various constraints that affect binomial orderings into a single preference estimate that indicates the preferred order and the strength of that preference for a given binomial (i.e., whether *bread and butter* is preferred over *butter and bread*, and by how much). They further demonstrated that this generative preference value is a strong predictor of human ordering preferences for low-frequency items, but not for high-frequency items, suggesting that humans rely primarily on item-specific knowledge for high-frequency items. Interestingly, in a follow-up study @morgan2024 demonstrated that these generative preferences exert an effect on all items, even high-frequency items (although generative preferences exert a weaker effect on the high-frequency items than the low-frequency ones).

A related line of research examined the role of storage in the development of frequency-dependent preference extremity, which is the tendency of high-frequency items to be more polarized in their orderings than low-frequency items [@morganFrequencydependentRegularizationIterated2016a; @liuFrequencydependentRegularizationConstituent2020; @liuFrequencyDependentRegularizationSyntactic2021]. For example, @morganFrequencydependentRegularizationIterated2016a demonstrated that high-frequency binomials (e.g., *bread and butter*) tend to be more fixed in their ordering preferences than low or mid frequency binomials. Similar work has also demonstrated that more frequent verbs have more polarized preferences with respect to the dative alternation [@liuFrequencydependentRegularizationConstituent2020] and that adjectives in adjective-adjective-noun constructions (big round ball) show more polarized preferences [@liuFrequencyDependentRegularizationSyntactic2021]. @morganFrequencydependentRegularizationIterated2016a demonstrated that a model that assumes that phrases are stored holistically predicts the emergence of these preferences over generations of learners. It is harder to account for this pattern without storage at the phrase level because generative preferences do not entirely predict the ordering preferences for high-frequency items. Thus, if people are simply composing binomials on the fly from the individual words, it's unclear why high-frequency items become polarized in their orderings.

Finally, there is also evidence of holistic storage from the learning literature [@siegelmanAdvantageStartingBig2015; @bannardStoredWordSequences2008; @odonnellProductivityReuseLanguage2016]. For example, there is evidence that attending to the whole utterance as opposed to attending to each individual word facilitates learning [@siegelmanAdvantageStartingBig2015]. Specifically, @siegelmanAdvantageStartingBig2015 gave adult L2 learners of German sentences that were either segmented into individual words, or not segmented at all. They found that participants learned grammatical gender in German better when the sentences were not segmented. They argued that by presenting participants with the unsegmented segments, participants were forced to pay attention to larger chunks of the sentence, making it easier for them to learn the grammatical gender patterns. This suggests that holistic storage may actually facilitate the learning of various grammatical relationships.

Additionally, in modeling the learning of the English past tense, models that store some items holistically outperform models that don't [@odonnellProductivityReuseLanguage2016]. @odonnellProductivityReuseLanguage2016 tested 4 probabilistic models on their ability to learn the English past tense. These models differed in whether they stored items holistically or composed them using morphological knowledge. He found that the inference-based model, which stored units of varying sizes, was able to learn the English past tense much better than the other models.

However, while there is an abundance of evidence that a lot more is stored than was previously considered, what factors determine whether a multi-word phrase is stored holistically?

## Factors that Drive Storage {#sec-factors-that-drive-storage}

Despite the evidence for holistic storage, it is still unclear what drives storage. Frequency has been assumed – oftentimes implicitly – to be the driving force of storage in the literature, and for good reason: there is no shortage of evidence that frequency contributes to holistic storage [@morgan2015; @morganAbstractKnowledgeDirect2016; @stembergerAreInflectedForms2004; @stembergerFrequencyLexicalStorage1986; @bybee2003; @pierrehumbertPhonologicalRepresentationAbstract2016; @bybee2001; @bybeeEffectUsageDegrees1999; @kapatsinskiFrequencyEmergencePrefabs2009]. For example, as mentioned in the previous section phonetic reduction has been shown to occur in high-frequency multi-word phrases, but not in their medium/low-frequency counterparts [@bybee2003; @bybeeEffectUsageDegrees1999]. In addition to previous examples, there is also evidence that high-frequency phrases lose the recognizability of their component parts relative to low or mid frequency phrases [@kapatsinskiFrequencyEmergencePrefabs2009]. In other words, *up* is harder to recognize in *pick up* than in *run up* (we[^introduction-1] will revisit this example in greater detail in the next section).

[^introduction-1]: As a stylistic choice, I will default to using the 1st person plural pronoun, *we*, in this dissertation. All of the work in this dissertation came out of a great deal of collaboration and it feels deceitful to obfuscate that by using the first person singular pronoun. I will, however, occasionally use the first person pronoun when referring to my own words or phrases. Further, while the work here was the result of a great deal of collaboration, any mistakes or errors in this dissertation are mine and mine alone.

<!--# has this been examined across predictability values though? -->

While frequency clearly plays a large role in holistic storage, it's unclear if other factors also influence whether a word or phrase is stored holistically. For example, in addition to frequency, predictability also plays an important part in many linguistic theories, especially in the learning literature [@kapatsinski2018; @olejarczukDistributionalLearningErrordriven2018; @saffranStatisticalLearning8MonthOld1996; @ramscarChildrenValueInformativity2013; @rescorlaProbabilityShockPresence1968]. For example, @olejarczukDistributionalLearningErrordriven2018 examined how humans learn new phonetic categories and found that when learners experience a rare member of a phonetic category, that member of the category exerts a disproportionate influence on the learner's representation of that category. In other words, learners try to actively predict upcoming sounds when learning new phonetic categories and update their representations in proportion to how surprising the upcoming sound is [@olejarczukDistributionalLearningErrordriven2018].

Additionally, @ramscarChildrenValueInformativity2013 demonstrated that children rely on how predictive a word is of a meaning when learning the meanings of words. Specifically, they examined how children and adults learn novel words. In their artificial language paradigm, participants first saw two objects together (A and B) and heard them labeled ambiguously as a *dax*. They then heard a different two objects together (B and C) with the ambiguous label, *pid*. In testing, all three objects were presented, and participants were tasked with identifying the *dax*, the *pid*, and the *wug*, which was a novel label. They found that both children and adults learn that A refers to *dax* and C refers to *pid*, but differ in what they learn *wug* refers to. Adults use logical exclusion and learn that *wug* refers to *B*, however while children learn that B is not as predictive of *dax* or *pid* as A or C, they do not conclude that B must then refer to *wug*. Instead, since B has a higher background rate (it occurs in every context, and is thus not particularly informative), children learn to ignore it and instead learn that *wug* refers to either A or C.

Finally, learners are highly sensitive to predictability when segmenting the speech stream into words [@saffranStatisticalLearning8MonthOld1996]. In their classic paper, @saffranStatisticalLearning8MonthOld1996 demonstrated that children leverage transitional probabilities to segment the speech stream into individual words. These results, taken in conjunction with the other results, suggest that predictability may also play a role in what phrases are stored holistically.

## Representations of Stored Items {#sec-representations-of-stored-items}

If multi-morphemic words and multi-word phrases are stored holistically, then it's also important to examine how these items are represented. Specifically, do stored representations maintain internal structure with respect to their component parts? @kapatsinskiFrequencyEmergencePrefabs2009 argued that stored constructions may lose some amount of their internal structure. They presented participants with sentences containing *up* either inside of a word (e.g., *cup*) or inside of a V+*up* construction (e.g., *pick up*). Participants were tasked with pressing a button when they heard *up*. They found that in high-frequency V+*up* constructions it is harder to recognize *up* than in medium frequency phrases, even after accounting for phonetic reduction (@fig-kapatsinskiplot2). In other words, participants grow faster to recognize up as the frequency of the phrase increases, until reaching the highest frequency phrases, where their reaction time grows slower. This increase in recognition time suggests 1) that high-frequency V+*up* phrases are stored holistically (since participants should be faster to recognize words in high-frequency contexts if they are forming them compositionally) and 2) holistically stored items lose some amount of their internal structure.

```{r echo = F, fig.align = 'center', warning = F, message = F, out.width = '100%'}
#| label: fig-kapatsinskiplot2
#| fig-cap: "A plot of the results reproduced from @kapatsinskiFrequencyEmergencePrefabs2009."
#| fig-align: center
knitr::include_graphics("Figures/kapatsinskiradicke_graph.pdf")
```

A visualization of what this may look like is demonstrated in @fig-nointernalstructure2. The left tree represents the phrase *pick up* stored holistically but with intact internal structure and the right tree represents the phrase *pick up* stored holistically but without internal structure.

```{r echo = F, fig.align = 'center', warning = F, message = F, out.width = '50%'}
#| label: fig-nointernalstructure2
#| fig-cap: "A visualization of a holistically stored phrase with internal structure (left) and without internal structure (right)."
#| fig-align: center
knitr::include_graphics("Figures/storage_syntax_tree.pdf")
```

This lack of internal structure could be lost over time, or it may simply not have been learned in the first place. A great deal of children's early learning is facilitated by memorizing chunks of language [@bybee2003; @tomaselloConstructingLanguageUsagebased2005] and it is unclear how this would not lead to holistic storage of these items. For example, @tomaselloConstructingLanguageUsagebased2005 argued that young children learn verbs in fixed "islands", producing them in fixed-constructions before eventually learning to generalize them to other contexts. As a result, children may be learning holistic representations of them initially.

Additionally, if predictability drives word-segmentation, many predictable phrases may be segmented out of the speech stream as a single chunk. Following this, @bybee2003 argued that after learning these chunks, it seems unlikely that children would then flush these from their memory (which is the only plausible explanation for how one starts with holistic chunks without resulting in holistic storage later down the line). Further, many high-frequency and high-predictability phrases have semantically vague relationships (e.g., *trick or treat*). These phrases may be difficult to decompose into their component words due to a lack of semantic transparency. This may lead to the holistic storage of these phrases. However, it is possible that the representations for these items are updated to reflect knowledge of the meaning of the individual words upon learning the individual words. Thus it is not entirely clear if holistically stored items lack internal representations of the individual words.

On the other hand, internal structure could be lost over time. For example, learners are more likely to semantically extend frequent forms to novel contexts than infrequent forms [@harmonPuttingOldTools2017]. Specifically, the authors demonstrated that given a novel semantic context, learners are more likely to use a frequent suffix to describe the novel context than an infrequent suffix. They argued that this is because the frequent suffix is more accessible. It is possible that semantic extension may also lead to a loss of internal structure in the multi-morphemic word (or phrase): as the phrase is extended to new contexts, the representation of that phrase may also become more general to accommodate the new context. This may lead to the internal structure being lost over time as the contexts that the phrase is used in becomes more different from the contexts in which the individual words are used.

## Processing Consequences of Storage {#sec-processing-consequences-of-storage}

Speech is inherently temporally linear: unlike reading, when you hear a sentence, you cannot skip forward or rewind back in time. As such, how are holistically stored multi-word phrases processed? One possibility is that upon hearing part of the phrase, listeners may access the representation for the holistically stored phrase. For example, hearing *Habeas* may be enough of a cue to access the holistic representation of *Habeas Corpus*.

However, this seems not to be the case. For example, @staubTimeCoursePlausibility2007 examined the effects of plausibility on the reading times of high-frequency and low-frequency compound nouns (Noun and Noun compounds). Specifically, participants read sentences that were either locally plausible or locally implausible:

\begin{enumerate} 
    \item \textbf{Novel Compound}
    \begin{enumerate}
        \item[\textbf{1a}] The zookeeper picked up the monkey medicine that was in the enclosure.
        \item[\textbf{1b}] The zookeeper spread out the monkey medicine that was in the enclosure.
    \end{enumerate}
    \item \textbf{Familiar Compound}
    \begin{enumerate}
        \item[\textbf{2a}] Jenny looked out on the huge mountain lion pacing in its cage.
        \item[\textbf{2b}] Jenny heard the huge mountain lion pacing in its cage.
    \end{enumerate}
\end{enumerate}

Sentence 1a is locally plausible because the sentence is plausible at the first noun. That is, *picked up the monkey* is plausible. On the other hand, 1b is locally implausible because the interpretation at the first noun is implausible; *spread out the monkey* is not plausible. Sentences 2a and 2b are analogous but with a high-frequency compound noun (where frequency is the number of times the compound noun occurred, not the number of times the individual words did).

@staubTimeCoursePlausibility2007 examined readers' eye-movements as they read these sentences and found that for locally implausible sentences there was a slowdown at the first noun. Crucially, this slowdown was equal for both the high and low-frequency compound nouns. However, if high-frequency compound nouns are stored holistically, and humans are able to access the representation at the first noun, then participants should have been able to overcome at least some of the slowdown due to the implausibility effect for the high-frequency items (because the local implausibility is eliminated by the second noun in the compound). However, this is not what @staubTimeCoursePlausibility2007 found. Instead they found that implausible contexts slow down reading measures similarly regardless of the frequency of the compound noun.

Given the results of @staubTimeCoursePlausibility2007, one natural possibility is that recognition happens incrementally and the representation of the phrase becomes activated more strongly than the words after the listener has heard each of the words in the phrase. However, if this was the case then the results from @kapatsinskiFrequencyEmergencePrefabs2009 discussed earlier would complicate things. Recall that @kapatsinskiFrequencyEmergencePrefabs2009 found that participants are slower to recognize *up* in high-frequency phrases. If recognition occurs incrementally, one would expect that in high-frequency phrases, *up* would be recognized even faster. That is, a slower recognition of *up* in the context of high-frequency phrases indicates that *up* is harder to recognize depending on what the preceding verb is. It's hard to see how an incremental approach on its own can account for this context-dependent effect on recognition.

Following these results, it is possible that an incremental approach combined with competition [such as one proposed by @mcclellandTRACEModelSpeech1984] may be another possible way to account for these results. Specifically, it is possible that processing unfolds incrementally such that the representation of each individual word is activated, however upon accessing the representation of the phrase, the word-level representations may be inhibited. For example, upon hearing *pick* perhaps the word-level representation for *pick* is activated, and upon hearing *up* perhaps instead of activating the representation for *up*, the holistically stored representation for *pick up* is activated and the representations at the word-level (*pick* and *up*) are inhibited. This inhibition of *up* could be the source of the increased recognition times for high-frequency V+*up* compounds.

## Storage in Humans vs Large Language Models {#sec-storage-in-humans-vs-large-language-models}

By now it should be clear that a great deal of items are stored holistically. However it's unclear whether current learning theories necessarily predict holistic storage. In order to examine this, we turn to large language models, which have developed rapidly over the last several years.

### Transformer Model Architecture

When I started this program in 2020, the idea of a single language model that could produce fluent text that was indiscernible from text written by humans still seemed like an idea out of a Sci-Fi movie. However, with rapid advancements in transformer models, the language models of today seem to have accomplished that goal. With this impressive accomplishment, the question of whether they are accomplishing this goal in a manner similar to humans has been at the forefront of a great deal of linguistics and cognitive science research. Thus in this section, we will introduce the transformer architecture along with the current state of the literature on its ability to trade off between stored and generative knowledge.

The term large language model typically refers to a transformer model [@vaswaniAttentionAllYou2017], such as Llama [@touvronLlama2Open2023]. The heart and soul of the transformer model is a feed-forward neural network (@fig-neuralnet). These models typically take token-level embeddings as their input and predict the next token. For example, if the model is presented with the input *The boy went outside to fly his* $\underline{\hspace{1cm}}$, the large language model may assign high probabilities to the output tokens *kite* or *airplane*.[^introduction-2]

[^introduction-2]: Technically, the token-level of large language models is not analogous to words. For example, GPT-2's tokenizer tokenizes *kite* into two tokens: *k*, and *ite*. As such, GPT-2 actually predicts (i.e., assigns the greatest probability to) *k* as the upcoming token in the example context.

```{r echo = F, fig.align = 'center', warning = F, message = F, out.width = '60%'}
#| label: fig-neuralnet
#| fig-cap: "A visualization of a feed-forward neural network."

#| fig-align: center
knitr::include_graphics("Figures/feed-forward neural network visualization.pdf")
```

In addition to a feed-forward neural network, the transformer architecture also implements a self-attention mechanism (see @fig-transformermodel). The self-attention mechanism helps the model learn which words are related to each other. This attention mechanism has been the main source of the transformer model's success over its predecessors. For example, previous models such as Long-Short Term Memory (LSTM) models or Recurrent Neural Network (RNN) models struggled with long-term dependencies [@al-selwiLSTMInefficiencyLongterm2023; @bengioProblemLearningLongterm1993]. The self-attention mechanism was proposed as a solution to this limitation.

The self-attention mechanism is a way to quantify which words are most related to each other. Specifically, the self-attention mechanism computes the strength of the relationship of each pair of words in the sentence [@vaswaniAttentionAllYou2017]. Thus in the sentence, *the students are very bright*, the self-attention mechanism might assign a high value to the pair {*students, are}* because the word *students* is very relevant for predicting *are* (as opposed to *is*).[^introduction-3] This example is visualized in @fig-attentionplot using BERT. For a more in-depth explanation of attention heads, we direct readers to the original paper, @vaswaniAttentionAllYou2017.

[^introduction-3]: More accurately, Transformer models typically have many different attention heads, each of which learns different relationships between words. For example, one attention head may attend to determiners of nouns while another may attend to objects of prepositions [@clark2019whatdoesbert]. While there is no guarantee that the weights of these attention heads are human-interpretable, fortunately it seems that often times they are [@clark2019whatdoesbert].

```{r echo = F, fig.align = 'center', warning = F, message = F, out.width = '50%'}
#| label: fig-attentionplot
#| fig-cap: "A visualization of key-query attention values for BERT at layer 3, attention head 8. Notice that the strength between 'students' and 'are' is high. Brighter colors denote larger attention weights, darker colors denote smaller attention weights."

#| fig-align: center
knitr::include_graphics("Figures/attention_plot.pdf")
```

```{r echo = F, warning = F, message = F, out.width = '60%'}
#| label: fig-transformermodel
#| fig-cap: "The transformer model architecture, reproduced from @vaswaniAttentionAllYou2017."
#| fig-align: center
#| fig-pos: t
knitr::include_graphics("Figures/transformer-architecture.pdf")
```

### Lessons from Transformer Models

Neural networks have long been examined by linguists and cognitive scientists as a potential model of how humans may learn language [e.g., @rumelhart1986learningtensesenglish]. For example, @rumelhart1986learningtensesenglish demonstrated that connectionist models (also referred to as parallel distributed processing models) are able to predict the learning curve seen in children. Specifically, they used a feedforward neural network with a single hidden layer and a sigmoid activation function. They found that this model is able to not only predict the correct past tense form for English verbs, but also follows a u-shaped learning curve that children also follow. That is, the model initially learns the correct past-tense forms, then overregularizes them (e.g., producing *goed* instead of *went*), before learning which verbs the past-tense rule applies to.[^introduction-4]

[^introduction-4]: It is important to note, especially given the topic of this dissertation, that traditional feedforward networks without hidden layers implicitly reject the dichotomy between storage and abstraction. However, with the inclusion of hidden layers, models actually are able to learn abstract representations. For example, in image recognition models, hidden layers come to learn more general features such as corners or edges [@zeiler2014visualizing].

In recent years, advancements in neural networks have led to the development of what are now known as transformer models. These models have achieved state-of-the-art performance on many benchmarks and are undoubtedly able to produce fluent, human-like text. However, it remains unclear to what extent they do this in a human-like manner. Specifically, it's unclear how much these models are simply memorizing their training data as opposed to learning something more abstract about the language.

For example, there have been doubts about whether they're capable of learning anything abstract, such as meaning [e.g., @benderDangersStochasticParrots2021]. @bender2020climbingnlumeaning argued that language models cannot learn meaning because they are trained on only the form of language. They operationalized meaning as the relationship between form and communicative intent. However, as @piantadosiMeaningReferenceLarge2022 pointed out in their rebuttal, this view of meaning ignores a well-known aspect of human language: meaning is not as simple as an association between a form and a referent. For example, the meaning of the word *justice* has no real-world referent. Instead, people learn these abstract words through their relationship with other words. This type of learning is something that large language models do well [@piantadosiChapterModernLanguage].

Additionally, while there have been many arguments that large language models don't learn in a human-like manner [@bender2020climbingnlumeaning; @benderDangersStochasticParrots2021], it is rather unclear what is meant by this statement. On one hand it is the case that large language models are often trained on trillions of tokens [e.g., @groeneveldOLMoAcceleratingScience2024], which is magnitudes larger than humans who have heard an average of 350 million words by the time they enter college [@levyProcessingExtraposedStructures2012]. This is further complicated by the fact that a lot of the training data for high-end large language models is either not open-access, or so huge that it is difficult to work with. On the other hand, it is hard to compare the input that a language model receives to the input that humans receive. While it is true that the number of tokens that large language models receive is magnitudes larger than humans, humans receive a lot of contextual information when they hear a word in the form of sensory information from their environment. It could be the case that this makes learning in large language models impossible, or it could be the case that it just requires more data for them to learn. Given the performance of large language models, the latter case seems plausible.

Further, the argument that the learning mechanisms in humans and large language models are completely different also falls apart quite quickly. Large language models, as was explained in the previous section, learn from prediction error. They update their representations to maximize prediction of the upcoming token [@vaswaniAttentionAllYou2017]. Learning theories [e.g., @kapatsinski2018; @rescorlaProbabilityShockPresence1968; @kapatsinskiDefragmentingLearning2023; @rescorla1972theorypavlovianconditioning] have made the same argument for quite some time, arguing that humans and animals are sensitive to the probability of upcoming events. Further, the language processing literature has also demonstrated that humans are actively predicting upcoming linguistic information [@ramscarChildrenValueInformativity2013; @olejarczukDistributionalLearningErrordriven2018; @ferreira2018integrationpredictionlanguage; @kuperberg2016whatwemean; @bansal2018functionfailuresensory; @clark2013arewepredictive]. This isn't to say that language models are learning identically to humans, but rather that there is enough overlap between large language models and humans to warrant a close examination of them. In fact, understanding the differences between humans and large language models may be a fruitful endeavor, leading to improvements in both language models as well as theories of language.

Thus in this section we take a closer examination into what evidence there is that language models are learning something meaningful about the language as opposed to simply memorizing their training data.

First, there is evidence that large language models do copy a significant amount from their training. For example, @haleyThisBERTNow2020 demonstrated that many of the BERT models are not able to reliably determine the correct plural form for novel words. Specifically, he tested BERT on English, French, Dutch, and Spanish on a number-agreement task. He evaluated whether the model was able to predict the correct plural form of novel-words in a no-prime condition and a prime-condition, where a previous sentence reveals the correct form of the verb (as well as the appropriate gender for the languages with a gender distinction). Humans are able to reliably use the prime to form the correct plural even for non-words. Interestingly, they found that BERT was not able to reliably use the prime to improve performance on the non-words.

Similarly, @liAreNeuralNetworks2021 demonstrated that BERT relies on memorization when producing the correct tense for a word as opposed to learning a more general linguistic pattern. They examined the ability of BERT to learn the correct tense in French and Chinese and found that while BERT is able to do well in French, it does much worse in Chinese. They argued that this is because while in French the correct tense information is expressed in the verb morphology (and can therefore be predicted by surface statistics of the language), in Chinese tense is driven by a variety of different cues, including abstract, lexical, syntactic, and pragmatic information. The poor performance in Chinese thus, according to the authors, suggests that BERT relies on memorization of surface-level statistics.

There is also a substantial amount of evidence that language models are learning more general patterns of the language [@yaoBothDirectIndirect2025; @misraLanguageModelsLearn2024; @weissweilerLinguisticGeneralizationsAre2025; @liAreNeuralNetworks2021; @lasriSubjectVerbAgreement2022; @liAssessingCapacityTransformer2023; @mccoyHowMuchLanguage2023]. For example, @lasriSubjectVerbAgreement2022 examined whether BERT is able to use the correct inflection of verbs in semantically incoherent contexts (e.g., *colorless green ideas sleep furiously*). They found that while BERT does do worse when the context is semantically incoherent, the decrease in performance is comparable to the decrease we see in humans. Additionally, @liAssessingCapacityTransformer2023 examined BERT's performance on subject-verb and object-past agreements in French. They used a probing task to determine whether the model learned anything general about the language. The probing task attempted to predict the number of the verb and the object-past agreements from the representations in the model. Their probe achieved a high accuracy, suggesting that the model encoded an abstract representation for these linguistic features.

The evidence for abstractions is not limited to BERT, either. There is also evidence that other transformer models can learn more abstract knowledge as well. For example, @mccoyHowMuchLanguage2023 examined the text that GPT-2 produces in relation to its training data. They found that while GPT-2 does copy a great deal, it also produces both novel words and syntactic structures.

In addition to large language models, more recently there has been an interesting line of research examining language models trained on a more human amount of data. For example, @misraLanguageModelsLearn2024 demonstrated that a language model trained on the BabyLM-strict corpus [a corpus containing a comparable amount of data as humans receive\; @warstadtFindingsBabyLMChallenge2023] can learn article-adjective-numeral-noun constructions (AANNs). AANNs are constructions such as *a beautiful five days*. They occur relatively infrequently in English but humans still learn develop preferences for these. For example, while *a beautiful five days* is perfectly grammatical, *a blue five pencils* is not. They found that even after removing all AANN occurrences from the training data, the language model is still able to learn human-like preferences for these constructions. They further demonstrated that this is likely learned from similar constructions, such as *a few days*. The results of this shows that even when trained on a comparable amount of data as humans, language models are still able to learn general patterns in the language.

Similarly, @yaoBothDirectIndirect2025 examined how language models learn the dative alternation. The dative alternation is a common construction in English where one can say either *give the toy to the cat* or *give the cat the toy*. While these two constructions have similar meanings, humans have preferences for one over the other depending on the length or animacy of each noun phrase. In order to examine this phenomenon in language models, they trained a language model on a comparable amount of data to humans. Crucially they systematically removed the length and animacy bias from the training data. They found that while the effect weakens, there is still an effect of length. They argued that this is evidence that language models are learning general patterns of the language. These results taken together with previous results demonstrate the ability of transformer models to learn general patterns in the language.

However, there is still a lot we don't know. What factors determine whether models learn general patterns of the language as opposed to relying on item-specific preferences? For example, humans seem to be sensitive to a combination of type and token frequency when they generalize beyond a specific word [@harmonPuttingOldTools2017]. Are language models sensitive to similar factors? Further is this knowledge represented in a similar way as humans? That is, are the general preferences that large language models learn similar to those that humans learn? Understanding the answers to these questions is important in evaluating these models as theories of human language learning.

## Outline of Dissertation {#sec-outline-of-dissertation}

In the present dissertation, we provide an in depth examination of how humans trade off between storage and computation. In the next chapter, we examine whether predictability drives storage and how holistic representations are accessed. In Chapter 3, we examine how holistically stored items are represented. In Chapter 4, we examine how large language models trade off between storage and computation. In Chapter 5, we examine how stored items are represented in the embeddings of large language models. Finally, in Chapter 6, we examines whether storage accounts can explain frequency-dependent preference extremity.[^introduction-5]

[^introduction-5]: All data and code for this dissertation can be found in the following github repository: <https://github.com/znhoughton/dissertation_writeup>.
